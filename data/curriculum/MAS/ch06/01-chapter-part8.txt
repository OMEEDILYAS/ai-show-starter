(h ,a ) returns i t ij theprobabilityofplayingactiona forhistoryh . ij t AMarkovstrategyfurtherrestrictsabehavioralstrategysothat,foragiventime t,thedistributionoveractionsdependsonlyonthecurrentstate. Markovstrategy Definition6.2.3(Markovstrategy) AMarkovstrategys isabehavioralstrategy i inwhichs (h ,a ) = s (h′,a )ifq = q′,whereq andq′ arethefinalstatesof i t ij i t ij t t t t h andh′,respectively. t t Thefinalrestrictionistoremovethepossibledependenceonthetimet. stationary Definition6.2.4(Stationarystrategy) Astationary strategys is a Markovstrati strategy egyinwhichs (h ,a ) = s (h′ ,a )ifq = q′ ,whereq andq′ arethefinal i t1 ij i t2 ij t1 t2 t1 t2 statesofh andh′ ,respectively. t1 t2 Now we can consider the equilibria of stochastic games, a topic that turns out to be fraught with subtleties. The discounted-rewardcase is the less problematic one. InthiscaseitcanbeshownthataNashequilibriumexistsineverystochastic game. In fact, we can state a stronger property. A strategy profile is called a Markovperfect MarkovperfectequilibriumifitconsistsofonlyMarkovstrategies,andis aNash equilibrium equilibriumregardlessofthestartingstate. Inasense,MPEplaysaroleanalogous (MPE) tothesubgame-perfectequilibriuminperfect-informationgames. Theorem6.2.5 Everyn-player,general-sum,discounted-rewardstochasticgame hasaMarkovperfectequilibrium. The case of average rewards presents greater challenges. For one thing, the limit average may not exist (i.e., although the stage-game payoffs are bounded, theiraveragemaycycleandnotconverge). However,thereisaclassofstochastic gamesthatiswellbehavedinthisregard.Thisistheclassofirreduciblestochastic irreducible games. A stochastic game is irreducible if every strategy profile gives rise to an stochasticgame irreducibleMarkov chain overthe set ofgames, meaning thateverygame can be reachedwithpositiveprobabilityregardlessofthestrategyadopted. Insuchgames thelimitaveragesarewelldefined,andwehavethefollowingtheorem. Theorem6.2.6 Everytwo-player,general-sum,averagereward,irreduciblestochasticgamehasaNashequilibrium. Indeed, under the same condition we can state a folk theorem similar to that presented for repeated games in Section 6.1.2. That is, as long as we give each playeranexpectedpayoffthatisatleastaslargeashisminmaxvalue,anyfeasible payoffpaircanbeachievedinequilibriumthroughtheuseofthreats. Theorem6.2.7 For every two-player, general-sum, irreducible stochastic game, and every feasible outcome with a payoff vector r that provides to each player at least his minmax value, there exists a Nash equilibrium with a payoff vector r. This is true for games with averagerewards,as well as gameswith largeenough discountfactors(or,withplayersthataresufficientlypatient). Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 162 6 RicherRepresentations:BeyondtheNormalandExtensiveForms 6.2.3 Computingequilibria Thealgorithmsandresultsforstochasticgamesdependgreatlyonwhetherweuse discountedrewardoraveragerewardfortheagentutilityfunction. Wewilldiscuss bothseparately,startingwiththediscountedrewardcase. Thefirstquestiontoask about the problem of finding a Nash equilibrium is whether a polynomial procedureisavailable. Thefactthatthereexistsanlinearprogrammingformulationfor solvingMDPs(forboththediscountedrewardandaveragerewardcases)givesus areasonforoptimism,sincestochasticgamesareageneralizationofMDPs. While suchaformulationdoesnotexistforthefullclassofstochasticgames,itdoesfor severalnontrivialsubclasses. Onesuchsubclassisthesetoftwo-player,general-sum,discounted-rewardstochastic games in which the transitions are determined by a single player. The singlecontrollerconditionisformallydefinedasfollows. Definition6.2.8(Single-controllerstochasticgame) Astochasticgameissinglesingle-controller controller if there exists a player i such that q,q′ Q, a A, P(q,a,q′) = ∀ ∈ ∀ ∈ stochasticgame P(q,a′,q′)ifa = a′. i i Thesameresultsholdwhenwereplacethesingle-controllerrestrictionwiththe following pair of restrictions: that the state and action profile have independent effectsontherewardachievedbyeachagent,andthatthetransitionfunctiononly depends on the action profile. Formally, this pair is called the separable reward stateindependenttransitioncondition. Definition6.2.9(SR-SITstochasticgame) Astochasticgameisseparablereward stateindependenttransition(SR-SIT)ifthefollowingtwoconditionshold: • there exist functions α,γ such that i,q Q, a A it is the case that ∀ ∈ ∀ ∈ r (q,a) = α(q)+γ(a);and i • q,q′,q′′ Q, a AitisthecasethatP(q,a,q′′)= P(q′,a,q′′). ∀ ∈ ∀ ∈ Even when the problem does notfall into one ofthese subclasses, practicalsolutionsstillexistforthediscountedcase. Onesuchsolutionistoapplyamodified version of Newton’s method to a nonlinear program formulation of the problem. An advantageof this method is that no local minima exist. For zero-sum games, analternativeis to usean algorithmdevelopedbyShapleythatis relatedto value iteration,acommonly-usedmethodforsolvingMDPs(seeAppendixC). Moving on to the average reward case, we have to impose more restrictions in orderto use a linearprogramthan we didforthe discountedrewardcase. Specifically, for the class of two-player, general-sum, average-rewardstochastic games, thesingle-controllerassumptionnolongersuffices—wealsoneedthegameto be zerosum. Even when we cannot use a linear program, irreducibility allows us to