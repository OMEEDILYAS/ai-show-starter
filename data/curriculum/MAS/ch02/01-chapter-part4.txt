any given global state of the Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 26 2 DistributedOptimization system, theoptimalactionofeachlocalstationdependsonlyon theaction ofthe station directly “upstream” from it. Thus in our example the global Q function becomes Q(a ,a ,a ,a ) = Q (a ,a )+Q (a ,a )+Q (a ,a )+Q (a ,a ) 1 2 3 4 1 1 2 2 2 4 3 1 3 4 3 4 andwewishtocompute argmax Q (a ,a )+Q (a ,a )+Q (a ,a )+Q (a ,a ). 1 1 2 2 2 4 3 1 3 4 3 4 (a1,a2,a3,a4) Note that in the preceding expressions we omit the state argument, since that is beingheldfixed;wearelookingatoptimalactionselectionatagivenstate. variable In this case we can employ a variable elimination algorithm, which optimizes elimination thechoicefortheagentsoneatatime. Weexplaintheoperationofthealgorithm via our example. Let us begin our optimization with agent 4. To optimize a , 4 functionsQ andQ areirrelevant. Hence,weobtain 1 3 max Q (a ,a )+Q (a ,a )+max[Q (a ,a )+Q (a ,a )]. 1 1 2 3 1 3 2 2 4 4 3 4 a1,a2,a3 a4 We see that to make the optimal choice over a , the values of a and a must 4 2 3 conditional be known. Thus, what must be computed for agent 4 is a conditional strategy, strategy with a (possibly)different action choice for each action choiceof agents 2 and 3. The value that agent 4 brings to the system in the different circumstances can be summarized using a new function e (A ,A ) whose value at the point a ,a is 4 2 3 2 3 thevalueoftheinternalmaxexpression e (a ,a )= max[Q (a ,a )+Q (a ,a )]. 4 2 3 2 2 4 4 3 4 a4 Agent4hasnowbeen“eliminated,”andourproblemnowreducestocomputing max Q (a ,a )+Q (a ,a )+e (a ,a ), 1 1 2 3 1 3 4 2 3 a1,a2,a3 havingonefeweragentinvolvedinthemaximization. Next,thechoiceforagent3 ismade,giving maxQ (a ,a )+e (a ,a ). 1 1 2 3 1 2 a1,a2 wheree (a ,a ) = max [Q (a ,a )+e (a ,a )]Next,thechoiceforagent2 3 1 2 a3 3 1 3 4 2 3 ismade: e (a )= max[Q (a ,a )+e (a ,a )]. 2 1 1 1 2 3 1 2 a2 Theremainingdecisionforagent1isnowthefollowingmaximization: e = maxe (a ). 1 2 1 a1 The result e is simply a number, the required maximization over a ,...,a . 1 1 4 Note that althoughthis expressionis short, there is no free lunch; in orderto performthisoptimization,oneneedstoiteratenotonlyoverallactionsa ofthefirst 1 UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 2.2 ActionselectioninmultiagentMDPs 27 agent, but also over the action of the other agents as needed to unwind the internalmaximizations. However,ingeneralthetotalnumberofcombinationswillbe smallerthanthefullexponentialcombinationofagentactions.2 We can recoverthe maximizing set of actions by performing the process in reverse. Themaximizingchoicefore definestheactiona∗ foragent1: 1 1 a∗ = argmaxe (a ). 1 2 1 a1 Tofulfillitscommitmenttoagent1,agent2mustchoosethevaluea∗ thatyielded 2 e (a∗), 2 1 a∗ = argmax[Q (a∗,a )+e (a∗,a )]. 2 1 1 2 3 1 2 a2 This,inturn,forcesagent3andthenagent4toselecttheiractionsappropriately: a∗ = argmax[Q (a∗,a )+e (a∗,a )]; 3 3 1 3 4 2 3 a3 a∗ = argmax[Q (a∗,a )+Q (a∗,a