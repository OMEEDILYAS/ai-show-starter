h(i) max(h(i),f(i′)) ← i i′ ← Figure2.3: Thelearningreal-timeA∗ algorithm. As earlier, we assumethatthe setofnodesis finite andthatall weightsw(i,j) are positive and finite. Note that this procedure uses a given heuristic function h() that serves as the initial value for each newly encountered node. For our · purposes it is not important what the precise function is. However, to guarantee admissible certain properties of LRTA∗, we must assume that h is admissible. This means heuristic thathneveroverestimatesthedistancetothegoal,thatis,h(i) h∗(i). Because ≤ Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 22 2 DistributedOptimization weightsarenonnegativewecanensureadmissibilitybysettingh(i) = 0foralli, althoughlessconservativeadmissibleheuristicfunctions(builtusingknowledgeof theproblemdomain)canspeeduptheconvergencetotheoptimalsolution. Finally, wemustassumethatthereexistssomepathfromeverynodeinthegraphtoagoal node. Withtheseassumptions,LRTA∗ hasthefollowingproperties: • Theh-valuesneverdecrease,andremainadmissible. • LRTA∗ terminates;thecompleteexecutionfromthestartnodetoterminationat thegoalnodeiscalledatrial. • IfLRTA∗ isrepeatedwhilemaintainingtheh-valuesfromonetrialtothenext, iteventuallydiscoverstheshortestpathfromthestarttoagoalnode. • If LRTA∗ find the same path on two sequential trials, this is the shortest path. (However,this pathmay also befoundin one ormoreprevioustrials before it isfoundtwiceinarow. Doyouseewhy?) Figure 2.4 shows four trials of LRTA∗. Do you see why admissibility of the heuristicisnecessary? LRTA∗ is a centralized procedure. However, we note that rather than have a singleagentexecutethis procedure,onecanhavemultiple agentsexecuteit. The properties of the algorithm (call it LRTA∗(n), with n agents) are not altered, but the convergence to the shortest path can be sped up dramatically. First, if the agentseachbreaktiesdifferently,somewillreachthegoalmuchfasterthanothers. Furthermore,iftheyallhaveaccesstoa sharedh-valuetable, thelearningofone agentcanteachtheothers. Specifically,aftereveryroundandforeveryi,h(i) = max h (i), whereh (i)isagentj’supdatedvalueforh(i). Figure2.5showsan j j j executionofLRTA∗(2)—thatis, LRTA∗ withtwoagents—startingfromthesame initialstateasinFigure2.4. (Thehollowarrowsshowpathstraversedbyasingle agent,whilethedarkarrowsshowpathstraversedbybothagents.) 2.2 ActionselectioninmultiagentMDPs In this section we discuss the problem of optimal action selection in multiagent MDPs.1 Recallthatinasingle-agentMDPtheoptimalpolicyπ∗ ischaracterized bythemutually-recursiveBellmanequations: Qπ∗(s,a) = r(s,a)+β p(s,a,sˆ)Vπ∗(sˆ) sˆ X Vπ∗ (s) = maxQπ∗ (s,a) a Furthermore,theseequationsturnintoanalgorithm—specifically,thedynamicvalueiteration programming-stylevalueiterationalgorithm—byreplacingtheequalitysigns“=" 1. Thebasicsofsingle-agentMDPsarecoveredinAppendixC. UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 2.2 ActionselectioninmultiagentMDPs 23 0 0 3 - a c 2(cid:0) (cid:0)(cid:18) i (cid:0)(cid:0)(cid:18) 6i @ 1 0(cid:0) (cid:0) @ @R0 s 2 2 (cid:0) 3 t (cid:0) @ (cid:0)(cid:18) i (cid:0) (cid:0) i 2@ @R ?(cid:0) (cid:0) 5 - b d 3 0 0 iinitialstate i 2 1 4 1 3 - 3 - a c a c 2(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:0)(cid:0)(cid:0)(cid:0) (cid:0)(cid:18) i (cid:0)(cid:0)(cid:0)(cid:0) (cid:0)(cid:0)(cid:18) i6@@@@@ 1 2(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:0)(cid:0)(cid:0)(cid:0) (cid:0)(cid:18) i (cid:0)(cid:0)(cid:18) 6i@@@@@ 1 2(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0) @@@@@ @@@@ @R0 4(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:0) @@@@@ @@@@ @R0 s 2 2(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) 3 t s 2 2 (cid:0) 3 t (cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:0) @ (cid:0)(cid:18) @ (cid:0)(cid:18) i (cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:0) i i (cid:0) (cid:0) i 2@ @R ?(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:0) 5 2@ @R ?(cid:0) (cid:0) 5 - - b d b d 3 3 2 0 2 0 i firsttrial i isecondtrial i 4 1 4 1 3 - 3 - a c a c 2(cid:0) (cid:0)(cid:18) i (cid:0)(cid:0)(cid:18) i6@@@@@ 1 2(cid:0) (cid:0)(cid:18) i (cid:0)(cid:0)(cid:0)(cid:0) (cid:0)(cid:0)(cid:18) 6i @@@@@ 1 4(cid:0) (cid:0) @@@@@ @@@@ @R0 5(cid:0) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0) @@@@@ @@@@ @R0 s 2 2 (cid:0) 3 t s 2 2(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) 3 t (cid:0) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0) i@@@@@ (cid:0) (cid:0) (cid:0)(cid:18) i i@@@@@ (cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:0) (cid:0)(cid:18) i 2@@@@@ @@@@ @R ?(cid:0) (cid:0) 5 2@@@@@ @@@@ @R ?(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:0) 5 - - b d b d 3 3 3 4 3 4 i thirdtrial i i forthtrial i Figure2.4: FourtrialsofLRTA∗ withassignmentoperators“ "anditeratingrepeatedlythroughthoseassignments. ← However,inreal-worldapplicationsthesituationisnotthatsimple. Forexample, theMDPmaynotbeknownbytheplanningagentandthusmayhavetobelearned. This caseis discussedin Chapter7. Butmorebasically, the MDP maysimply be too large to iterate overall instances of the equations. In this case,