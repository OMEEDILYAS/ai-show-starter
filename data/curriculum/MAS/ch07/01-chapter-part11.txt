other agentscontinuetoplayastheydid. regret Definition7.5.1(Regret) Theregretanagentexperiencesattimetfornothaving playedsisRt(s) = αt αt(s). − Observethatthisisconceptuallythesameasthedefinitionofregretweoffered inSection3.4(Definition3.4.5). Alearningruleissaidtoexhibitnoregret10ifitguaranteesthatwithhighprobabilitytheagentwillexperiencenopositiveregret. no-regret Definition7.5.2(No-regretlearningrule) Alearningruleexhibitsnoregretiffor anypurestrategyoftheagentsitholdsthatPr([liminfRt(s)] 0) = 1. ≤ The quantification is over all of the agent’s pure strategies of the stage game, butnote thatit wouldmakenodifferenceifinsteadonequantifiedoverallmixed strategies of the stage game. (Do you see why?) Note also that this guarantee is only in expectation, since the agent’s strategy will in general be mixed, and thus thepayoffobtainedatanygiventime—ut—isuncertain. i It is important to realize that this “in hindsight" requirementignores the possibility that the opponents’ play might change as a result of the agent’s own play. Thisis trueforstationaryopponents,andmightbeareasonableapproximationin thecontextofalargenumberofopponents(suchasinapublicsecuritiesmarket), butless in the contextofa smallnumberofagents, ofthe sortgametheorytends to focus on. For example, in the finitely-repeated Prisoner’s Dilemma game, the only strategy exhibiting no regret is to always defect. This precludes strategies thatcapitalizeoncooperativebehaviorbytheopponent,suchasTit-for-Tat. Inthis connectionseeourearlierdiscussionoftheinseparabilityoflearningandteaching. Overthe years, a variety ofno-regretlearningtechniqueshavebeendeveloped. regretmatching Herearetwo,regretmatchingandsmoothfictitiousplay. smoothfictitious • Regretmatching. Ateachtimestepeachactionis chosenwithprobabilityproplay portionaltoitsregret. Thatis, Rt(s) σt+1(s) = , i Rt(s′) s′∈Si whereσt+1(s)istheprobabilitythata P gentiplayspurestrategysattimet+1. i • Smooth fictitious play. Instead of playing the best response to the empirical frequencyoftheopponent’splay,asfictitiousplayprescribes,oneintroducesa perturbation that gradually diminishes over time. That is, rather than adopt at timet+1apurestrategys thatmaximizesu (s ,Pt)wherePtistheempirical i i i 10. Thereareactuallyseveralversionsofregret.Theonedescribedhereiscalledexternalregretincomputer science,andunconditionalregretingametheory. Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 222 7 LearningandTeaching distributionof opponent’splayuntil time t, agenti adoptsa mixedstrategyσ i thatmaximizesu (s ,Pt)+λv (σ ). Hereλisanyconstant,andv isasmooth, i i i i i concavefunction with boundariesat the unit simplex. For example, v can be i theentropyfunction,v (σ ) = σ (s )logσ (s ). i i − Si i i i i Regret matching can be shown toPexhibit no regret, and smooth fictitious play approaches no regret as λ tends to zero. The proofs are based on Blackwell’s ApproachabilityTheorem;thenotesattheendofthechapterprovidepointersfor furtherreadingonit,aswellasonotherno-regrettechniques. 7.6 Targeted learning No-regretlearningwasoneapproachtoensuringgoodrewards,butaswediscussed thissenseof“good”hassomedrawbacks. Herewediscussanalternativesenseof “good,”whichretainstherequirementofbestresponse,butlimitsittoaparticular class of opponents. The intuition guiding this approach is that in any strategic setting,inparticularamultiagentlearningsetting,onehassomesenseoftheagents in the environment. A chess playerhas studied previousplays ofhis opponent,a skipperinasailingcompetitionknowsalotabouthiscompetitors,andsoon. And soitmakessensetotrytooptimizeagainstthissetofopponents,ratherthanagainst completelyunknownopponents. targetedlearning Technically speaking, the model of targeted learning takes as a parameter a class—the “target class"—of likely opponents and is required to perform particularly well againstthese likely opponents. At the same time one wants to ensure at least the maxmin payoffagainstopponentsoutside the targetclass. Finally, an additionaldesirablepropertyisforthealgorithmtoperformwellinself-play;the algorithmshouldbedesignedto“cooperate”withitself. For games with only two agents, these intuitions can be stated formally as follows. targeted Property7.6.1(Targetedoptimality) Against any opponent in the target class, optimality theexpectedpayoffisthebest-responsepayoff.11 safety Property7.6.2(Safety) Againstanyopponent,theexpectedpayoffisatleastthe individualsecurity(ormaxmin)valueforthegame. Property7.6.3(Autocompatibility) Self-play—inwhichbothagentsadoptthelearnautocompatibility ingprocedureinquestion—isstrictlyParetoefficient.12 11. Note:theexpectationisoverthemixed-strategyprofiles,butnotoveropponents;thisrequirementisfor anyfixedopponent. 12. RecallthatstrictParetoefficiencymeansthatoneagent’sexpectedpayoffcannotincreasewithoutthe other’sdecreasing;seeDefinition3.3.2.Alsonotethatwedonotrestrictthediscussiontosymmetricgames, andsoselfplaydoesnotingeneralmeanidenticalplaybytheagents,noridenticalpayoffs. Weabbreviate “strictlyParetoefficient”as“Paretoefficient.” UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.6 Targetedlearning 223 Weintroduceoneadditionaltwist. Sinceweareinterestedinquicklearning,not onlylearninginthelimit,weneedtoallowsomedeparturefromtheideal. Andso weamendtherequirementsasfollows. Definition7.6.4(Efficienttargetedlearning) Alearningruleexhibitsefficienttarefficienttargeted getedlearningifforeveryǫ > 0and1 > δ > 0, thereexistsanM polynomialin learning 1/ǫand1/δ suchthatafterM timesteps,withprobabilitygreaterthan1 δ,all − threepayoffrequirementslistedpreviouslyareachievedwithinǫ. Note the differencefrom no-regretlearning. Forexample, considerlearningin arepeatedPrisoner’sDilemma game. Supposethatthe targetclassconsistsofall opponents whose strategies rely on the past iteration; note this includes the Titfor-Tat strategy. In this case successful targeted learning will result in constant cooperation,whileno-regretlearningprescribesconstantdefection. How hard is it to achieve efficient targeted learning? The answer depends of courseonthetargetclass. Provablycorrect(withrespecttothiscriterion)learning proceduresexistfor the class of stationary opponents,andthe class of opponents whose memory is limited to a finite window into the past. The basic approachis to