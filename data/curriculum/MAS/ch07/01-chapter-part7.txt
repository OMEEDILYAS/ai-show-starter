isinterpretedasplayeri’s i beliefs). Letµ andµ bethedistributionsoverinfinitegamehistoriesinducedby s P thestrategyprofilesandthebelieftupleP,respectively. Ifwehavethat • ateachround,eachplayeriplaysabestresponsestrategygivenhisbeliefsP ; i • aftereachroundeachplayeriupdatesP usingBayesianupdating;and i 8. Thatis,atupleofrepeated-gamestrategies,oneforeachplayer. Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 214 7 LearningandTeaching • µ isabsolutelycontinuouswithrespecttoµ , s Pi thenforeveryǫ > 0andfor almosteveryhistoryin thesupportofµ (i.e., every s possible history given the actual strategy profile s), there is a time T such that for all t T, the play µ predicted by the player i’s beliefs is ǫ-close to the ≥ Pi distributionofplayµ predictedbytheactualstrategies. s Thusaplayer’sbeliefswilleventuallyconvergetothetruthifheisusingBayesian updating, is playing a best response strategy, and the play predicted by the other players’ real strategies is absolutely continuous with respect to that predicted by his beliefs. In other words, he will correctly predict the on-path portions of the otherplayers’strategies. Notethatthisresultdoesnotstatethatplayerswilllearnthetruestrategybeing playedbytheiropponents.Asstatedearlier,thereareaninfinitenumberofpossible strategiesthattheiropponentcouldbeplaying,andeachplayerbeginswithaprior distribution that assigns positive probability to only some subset of the possible strategies. Instead, players’ beliefs will accurately predict the play of the game, andnoclaimismadeabouttheiraccuracyinpredictingtheoff-pathportionsofthe opponents’strategies. ConsideragainthetwoplayersplayingtheinfinitelyrepeatedPrisoner’sDilemma game,as describedin the previousexample. Let usverify that, as Theorem7.3.3 dictates,thefutureplayofthisgamewillbecorrectlypredictedbytheplayers. If T < T then from time T +1 on, player2’s posterior beliefs will assign prob1 2 1 ability 1 to player 1’s strategy, g . On the other hand, player 1 will never fully T1 knowplayer2’s strategy, butwill knowthatT > T . However,this is sufficient 2 1 informationtopredictthatplayer2willalwayschoosetodefectinthefuture. A player’s beliefs must converge to the truth even when his strategy space is incorrect(doesnotincludetheopponent’sactualstrategy),as longas theysatisfy theabsolutecontinuityassumption. Suppose,forinstance,thatplayer1isplaying thetriggerstrategyg ,andplayer2isplayingtit-for-tat,butthatplayer1believes ∞ that player 2 is also playing the trigger strategy. Thus player 1’s beliefs about player2’sstrategyareincorrect. Nevertheless,hisbeliefswillcorrectlypredictthe futureplayofthegame. We have so far spoken about the accuracy of beliefs in rational learning. The followingtheoremaddressesconvergencetoequilibrium. Notethattheconditions of this theorem are identical to those of Theorem 7.3.3, and that the definition refers to the concept of an ǫ-Nash equilibrium from Section 3.4.7, as well as to ǫ-closenessasdefinedearlier. Theorem7.3.4(RationalLearningandNash) Let s be a repeated-game strategy profile for a given n-player game, and let P = P ,...,P be a a tuple of 1 n probability distributions over such strategy profiles. Let µ and µ be the distris P butionsoverinfinitegamehistoriesinducedbythestrategyprofilesandthebelief tupleP,respectively. Ifwehavethat • ateachround,eachplayeriplaysabestresponsestrategygivenhisbeliefsP ; i UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.4 Reinforcementlearning 215 • aftereachroundeachplayeriupdatesP usingBayesianupdating;and i • µ isabsolutelycontinuouswithrespecttoµ , s Pi then for every ǫ > 0 and for almost every history in the support of µ there is a s time T suchthatfor everyt T thereexists anǫ-equilibriums∗ oftherepeated ≥ gameinwhichtheplayµ predictedbyplayeri’sbeliefsisǫ-closetotheplayµ Pi s∗ oftheequilibrium. Inotherwords,ifutility-maximizingplayersstartwithindividualsubjectivebeliefswithrespecttowhichthetruestrategiesareabsolutelycontinuous,theninthe longrun,theirbehaviormustbeessentiallythesameasabehaviordescribedbyan ǫ-Nashequilibrium. Ofcourse,thespaceofrepeated-gameequilibriaishuge,whichleavesopenthe questionofwhichequilibriumwillbereached. Herenoticeacertainself-fulfilling property: players’optimismcanleadtohighrewards,andlikewisepessimismcan leadtolowrewards. Forexample,inarepeatedPrisoner’sDilemmagame,ifboth players begin believing that their opponentwill likely play the TfT strategy, they each will tend to cooperate, leading to mutualcooperation. If, on the other hand, theyeachassignhighpriorprobabilitytoconstantdefection,ortothegrim-trigger strategy,theywilleachtendtodefect. 7.4 Reinforcement learning In this section we look at multiagent extensions of learning in MDPs, that is, in single-agent stochastic games (see Appendix C for a review of MDP essentials). Unlike the first two learning techniques discussed, and with one exception disreinforcement cussedinsection7.4.4,reinforcementlearningdoesnotexplicitlymodeltheoppolearning nent’sstrategy. Thespecificfamily oftechniqueswe lookatarederivedfromthe Q-learning algorithm for learning in unknown (single-agent) MDPs. Q-learning is described in the nextsection, after which we presentits extensionto zero-sum stochasticgames. We then briefly discuss the difficulty in