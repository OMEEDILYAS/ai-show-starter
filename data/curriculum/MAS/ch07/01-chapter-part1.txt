7 Learning and Teaching Thecapacitytolearnisakeyfacetofintelligentbehavior,anditisnosurprisethat muchattentionhasbeendevotedtothesubjectinthevariousdisciplinesthatstudy intelligence and rationality. We will concentrate on techniques drawn primarily fromtwosuchdisciplines—artificialintelligenceandgametheory—althoughthose in turn borrow from a variety of disciplines, including control theory, statistics, psychology and biology, to name a few. We start with an informal discussion of the various subtle aspects of learning in multiagent systems and then discuss representativetheoriesinthisarea. 7.1 Whythe subject of“learning”is complex Thesubjectmatterofthischapterisfraughtwithsubtleties,andsowebeginwith an informal discussion of the area. We address three issues—the interaction between learning and teaching, the settings in which learning takes place and what constituteslearningin thosesettings, andtheyardsticksbywhichtomeasurethis orthattheoryoflearninginmultiagentsystems. 7.1.1 Theinteraction betweenlearningandteaching Most work in artificial intelligence concerns the learning performed by an individual agent. In that setting the goal is to design an agent that learns to function successfullyinanenvironmentthatisunknownandpotentiallyalsochangesasthe agentislearning. Abroadrangeoftechniqueshavebeendeveloped,andlearning ruleshavebecomequitesophisticated. Inamultiagentsetting,however,anadditionalcomplicationarises,sincetheenvironmentcontains(or perhapsconsists entirelyof) otheragents. The problemis notonlythattheotheragents’learningwillchangetheenvironmentforourprotagonistagent—dynamicenvironmentsfeaturealreadyin the single-agentcase—but thatthesechangeswilldependinpartontheactionsoftheprotagonistagent. That is, thelearningoftheotheragentswillbeimpactedbythe learningperformedby ourprotagonist. 200 7 LearningandTeaching Thesimultaneouslearningoftheagentsmeansthateverylearningruleleadsto a dynamical system, and sometimes even very simple learning rules can lead to complexglobalbehaviorsofthesystem. Beyondthismathematicalfact,however, liesaconceptualone. Inthecontextofmultiagentsystemsonecannotseparatethe learningand phenomenonoflearningfromthatofteaching;whenchoosingacourseofaction, teaching an agent must take into account not only what he has learned from other agents’ pastbehavior,butalsohowhewishestoinfluencetheirfuturebehavior. The following example illustrates this point. Consider the infinitely repeated gamewithaveragereward(i.e.,wherethepayofftoagivenagentisthelimitaverageof his payoffsin the individualstage games, as in Definition 6.1.1), in which Stackelberg thestagegameisthenormal-formgameshowninFigure7.1. game L R T 1,0 3,2 B 2,1 4,0 Figure7.1: Stackelberggame:player1mustteachplayer2. First note that player 1 (the row player) has a dominant strategy, namely B. Also note that (B,L) is the unique Nash equilibrium of the game. Indeed, if player1 wereto playB repeatedly,it is reasonableto expectthatplayer2 would always respond with L. Of course, if player 1 were to choose T instead, then player 2’s best response would be R, yielding player 1 a payoff of 3 which is greater than player1’s Nash equilibrium payoff. In a single-stage game it would behardforplayer1toconvinceplayer2thathe(player1)willplayT,sinceitis astrictlydominatedstrategy.1 However,inarepeated-gamesettingagent1hasan opportunity to put his payoffwhere his mouth is, and adopt the role of a teacher. Thatis,player1couldrepeatedlyplayT;presumably,afterawhileplayer2,ifhe hasanysenseatall,wouldgetthemessageandstartrespondingwithR. Intheprecedingexampleitisprettyclearwhothenaturalcandidateforadopting the teacherrole is. Butconsidernow the repetition of the Coordinationgame, reproducedinFigure7.2. Inthiscase,eitherplayercouldplaytheteacherwithequal success. However, if both decide to play teacher and happen to select uncoordinated actions (Left,Right) or (Right,Left) then the players will receive a payoff ofzeroforever.2 Istherealearningrulethatwillenablethemtocoordinatewithout anexternaldesignationofateacher? 1. SeerelateddiscussiononsignalingandcheaptalkinChapter8. 2. Thisisreminiscentofthe“sidewalkshuffle,”thatawkwardprocessoftryingtogetbythepersonwalking towardyouwhileheisdoingthesamething,theresultbeingthatyoukeepblockingeachother. UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.1 Whythesubjectof“learning”iscomplex 201 Left Right Left 1,1 0,0 Right 0,0 1,1 Figure7.2: Who’stheteacherhere? 7.1.2 Whatconstituteslearning? In the preceding examples the setting was a repeated game. We consider this a “learning”settingbecauseofthetemporalnatureofthedomain,andtheregularity across time (at each time the same players are involved, and they play the same game as before). This allows us to consider strategies in which future action is selected based on the experiencegainedso far. When discussingrepeated games in Chapter 6 we mentioned a few simple strategies. For example, in the context of repeated Prisoner’s Dilemma, we mentioned the Tit-for-Tat (TfT) and trigger strategies. These, in particular TfT, can be viewed as very rudimentary forms of learningstrategies. Butonecanimaginemuchmorecomplexstrategies,inwhich anagent’snextchoicedependsonthe historyofplayin more sophisticatedways. For example, the agent could guess that the frequency of actions played by his opponentinthepastmightbehiscurrentmixedstrategy,andplayabestresponse to that mixed strategy. As we shall see in Section 7.2, this basic learning