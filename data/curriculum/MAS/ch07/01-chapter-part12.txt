construct a number of building blocks and then specialize and combine them differently depending on the precise setting. The details of the algorithms can getinvolved,especiallyintheinterestingcaseofnonstationaryopponents,butthe essentialflowisasfollows. 1. Startbyassumingthattheopponentisinthetargetsetandlearnabestresponse totheparticularagentunderthisassumption. Ifthepayoffsyouobtainstraytoo muchfromyourexpectation,moveon. 2. Signal to the opponentto find out whether he is employing the same learning strategy. Ifheis,coordinatetoaPareto-efficientoutcome. Ifyourpayoffsstray toofaroff,moveon. 3. Playyoursecurity-levelstrategy. Note that so far we have restricted the discussion to two-player games. Can wegeneralizethecriteria—andthealgorithms—togameswithmoreplayers? The answeris yes, butvariousnew subtleties creepin. Forexample, in the two-agent caseweneededtoworryaboutthreecases,correspondingtowhethertheopponent is in thetargetset, is aself-playagent, oris neither. We mustnowconsiderthree setsofagents—selfplayagents(i.e.,agentsusingthealgorithminquestion),agents inthe targetset, andunconstrainedagents, andaskhowagentsinthe firstsetcan jointly achieve a Pareto-efficient outcome against the second set and yet protect themselvesfromexploitationbyagentsinthethirdset. Thisraisesquestionsabout possiblecoordinationamongtheagents: • Canself-playagentscoordinateotherthanimplicitlythroughtheiractions? Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 224 7 LearningandTeaching • Can opponents—whether in the target set or outside—coordinate other than throughtheactions? Thesectionattheendofthechapterpointstofurtherreadingonthistopic. 7.7 Evolutionary learningand other large-populationmodels Inthissectionweshiftourfocusfrommodelsofthelearningofindividualagents to models of the learning of populationsof agents (although, as we shall see, we will not abandon the single-agent perspective altogether). When we speak about learning in a population of agents, we mean the change in the constitution and behavior of that population over time. These models were originally developed by population biologists to model the process of biological evolution, and later adoptedandadaptedbyotherfields. Inthefirstsubsectionwepresentthemodelofthereplicatordynamic,asimple modelinspired by evolutionary biology. In the second subsection we presentthe conceptofevolutionarilystablestrategies,astabilityconceptthatisrelatedtothe replicatordynamic. Weconcludewithasomewhatdifferentmodelofagent-based simulationandtheconceptofemergentconventions. 7.7.1 Thereplicator dynamic replicator Thereplicatordynamicmodelsapopulationundergoingfrequentinteractions. We dynamic willconcentrateonthesymmetric,two-playercase,inwhichtheagentsrepeatedly playatwo-playersymmetricnormal-formstagegame13againsteachother. Definition7.7.1(Symmetric2 2game) Letatwo-playertwo-actionnormal-form × symmetricgame gamebecalledasymmetricgameifithasthefollowingform: A B A x,x u,v B v,u y,y Intuitively, this requirement says that the agents do not have distinct roles in the game, and the payofffor agents doesnot dependon theiridentities. We have alreadyseenseveralinstancesofsuchgames,includingthePrisoner’sDilemma.14 13. There exist much more general notions of symmetric normal-form games with multiple actions and players,butthefollowingissufficientforourpurposes. 14. Thisrestrictiontosymmetricgamesisveryconvenient,simplifyingboththesubstanceandnotationof whatfollows. However, thereexistmorecomplicated evolutionary models, includingonesallowingboth differentstrategyspacesfordifferentagentsandnonsymmetricpayoffs. Attheendofthechapterwepoint thereadertofurtherreadingonthesemodels. UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.7 Evolutionarylearningandotherlarge-populationmodels 225 The replicator dynamic describes a population of agents playing such a game in an ongoing fashion. At each point in time, each agent only plays a pure strategy. Informallyspeaking,the modelthenpairsall agentsandhas themplayeach fitness other,eachobtainingsomepayoff. Thispayoffiscalledtheagent’sfitness. Atthis point the biologicalinspiration kicks in—each agentnow “reproduces"in a manner proportional to this fitness, and the process repeats. The question is whether theprocessconvergestoafixedproportionofthevariouspurestrategieswithinthe population,andifsotowhichfixedproportions. The verbal description aboveis only meant to be suggestive. The actual mathematical model is a little different. First, we never explicitly model the play of thegamebetweenparticularsetsofplayers;weonlymodeltheproportionsofthe populationsassociatedwith agivenstrategy. Second,the modelis notoneofdiscreterepetitionsofplay,butratheroneofcontinuousevolution. Third,beyondthe fitness-basedreproduction,thereisalsoarandomelementthatimpactstheproportionsinthepopulation. (Again,becauseofthebiologicalinspiration,thisrandom mutation elementiscalledmutation.) Theformalmodelisasfollows. Givenanormal-formgameG = ( 1,2 ,A,u), { } letϕ (a)denotethenumberofplayersplayingactionaattimet. Also,let t ϕ (a) θ (a) = t t ϕ (a′) a′∈A t betheproportionofplayersplayingactPionaattimet. Wedenotewithϕ thevector t of measures of players playing each action, and with θ the vector of population t sharesforeachaction. Theexpectedpayofftoanyindividualplayerforplayingactionaattimetis u (a) = θ (a′)u(a,a′). t t a′ X The change in the number of agents playing action a at time t is defined to be proportionaltohisfitness,thatis,hisaveragepayoffatthecurrenttime, ϕ˙ (a) = ϕ (a)u (a). t t t The absolute numbers of agents of each type are not important; only the relative ratiosare. Definingtheaverageexpectedpayoffofthewholepopulationas u∗ = θ (a)u (a), t t t a X wehavethatthechangeinthefractionofagentsplayingactionaattimetis ϕ˙ (a) ϕ (a′) ϕ (a) ϕ˙ (a′)