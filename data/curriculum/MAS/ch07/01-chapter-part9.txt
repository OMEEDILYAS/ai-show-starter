to updatethe strategyΠ. Therearestillsomefreeparameters,suchashowtoupdatethelearning parameter, α. One way of doingso is to simply use a decayrate, so that α is set to α decay after each Q-valueupdate, forsome value of decay < 1. Another ∗ possibilityfromtheQ-learningliteratureistokeepseparateα’sforeachstateand action profile pair. In this case, a common method is to use α = 1/k, where k equalsthenumberoftimesthatparticularQ-valuehasbeenupdatedincludingthe current one. So, when first encountering a reward for a state s where an action profile a was played, the Q-value is set entirely to the observed reward plus the discountedvalueofthesuccessorstate(α = 1). Onthenexttimethatstate–action profilepairisencountered,itwillbesettobehalfoftheoldQ-valueplushalfof thenewrewardanddiscountedsuccessorstatevalue. Wenowlookatanexampledemonstratingtheoperationofminimax-Qlearning in a simple repeated game: repeated Matching Pennies (see Figure 7.4) against an unknown opponent. Note that the convergence results for Q-learning impose onlyweakconstraintsonhowtoselectactionsandvisitstates. Inthisexample,we followthegivenalgorithmandassumethattheagentchoosesanactionrandomly some fraction of the time (denoted explor), and plays according to his current Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 218 7 LearningandTeaching //Initialize: foralls S,a A,ando Odo ∈ ∈ ∈ Q(s,a,o) 1 ← forallsinSdo V(s) 1 ← foralls S anda Ado ∈ ∈ Π(s,a) 1/A ← | | α 1.0 ← //Takeanaction: wheninstates,withprobabilityexplor chooseanactionuniformlyatrandom, andwithprobability(1 explor)chooseactionawithprobabilityΠ(s,a) − //Learn: afterreceivingrewardrewformovingfromstatestos′ viaactionaand opponent’sactiono Q(s,a,o) (1 α) Q(s,a,o)+α (rew+γ V(s′)) ← − ∗ ∗ ∗ Π(s, ) argmax (min (Π(s,a′) Q(s,a′,o′))) · ← Π′(s,·) o′ a′ ∗ //Theabovecanbedone,forexample,bylinearprogramming V(s) min ( (Π(s,a′) Q P (s,a′,o′))) ← o′ a′ ∗ Updateα P Figure7.8: Theminimax-Qalgorithm. beststrategyotherwise. Forupdatingthelearningrate,wehavechosenthesecond methoddiscussedearlier,withα = 1/k,wherek isthenumberoftimesthestate andactionprofilepairhasbeenobserved.AssumethattheQ-valuesareinitialized to1andthatthediscountfactorofthegameis0.9. Table7.4showsthevaluesofplayer1’sQ-functioninthefirstfewiterationsof this game as well as his best strategy at each step. We see that the value of the game,0,isbeingapproached,albeitslowly. Thisisnotanaccident. Theorem7.4.3 UnderthesameconditionsthatassureconvergenceofQ-learning to the optimal policy in MDPs, in zero-sum games Minimax-Q converges to the valueofthegameinselfplay. Here again, no guarantee is made about the rate of convergence or about the accumulation of optimal rewards. We can achieve more rapid convergenceif we arewillingtosacrificetheguaranteeoffindingaperfectlyoptimalmaxminstrategy. In particular, we can consider the framework of probably approximately correct probably (PAC) learning. In this setting, choose some ǫ > 0 and 1 > δ > 0, and seek approximately analgorithmthatcanguarantee—regardlessoftheopponent—apayoffofatleast correct(PAC) thatofthemaxminstrategyminusǫ,withprobability(1 δ). Ifwearewillingto learning − settleforthisweakerguarantee,wegainthepropertythatitwillalwaysholdafter apolynomially-boundednumberoftimesteps. UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.4 Reinforcementlearning 219 t Actions Reward1 Qt(H,H) Qt(H,T) Qt(T,H) Qt(T,T) V(s) π1(H) 0 1 1 1 1 1 0.5 1 (H*,H) 1 1.9 1 1 1 1 0.5 2 (T,H) -1 1.9 1 -0.1 1 1 0.55 3 (T,T) 1 1.9 1 -0.1 1.9 1.279 0.690 4 (H*,T) -1 1.9 0.151 -0.1 1.9 0.967 0.534 5 (T,H) -1 1.9 0.151 -0.115 1.9 0.964 0.535 6 (T,T) 1 1.9 0.151 -0.115 1.884 0.960 0.533 7 (T,H) -1 1.9 0.151 -0.122 1.884 0.958 0.534 8 (H,T) -1 1.9 0.007 -0.122 1.884 0.918 0.514 . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 (H,H) 1 1.716 -0.269 -0.277 1.730 0.725 0.503 . . . . . . . . . . . . . . . . . . . . . . . . . . . 1000 (T,T) 1 1.564 -0.426 -0.415 1.564 0.574 0.500 . . . . . . . . . . .