treated in a special manner, but we will not discuss this interesting aspect here. Finally, the notion of“personalhistory” canbe furtherhoned. We will assume that the agent has access to the action he has taken and the reward he received at each instance. One could assume further that the agent observes the choices of others in the games in which he participated, and perhaps also their payoffs. But we will look specifically at an action-selection rule that does not make this highest assumption. This rule, called the highest cumulative reward (HCR) rule, is the cumulative followinglearningprocedure: reward(HCR) UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.8 Historyandreferences 233 1. Initializethecumulativerewardforeachaction(e.g.,tozero). 2. Pickaninitialaction. 3. Playaccordingtothecurrentactionandupdateitscumulativereward. 4. Switchtoanewactioniffthetotalpayoffobtainedfromthatactioninthelatest miterationsisgreaterthanthepayoffobtainedfromthecurrentlychosenaction inthesametimeperiod. 5. Gotostep3. The parameter m in the procedure denotes a finite bound, but the bound may vary. HCR is a simple and natural procedure, but it admits many variants. One canconsiderrulesthatuseaweightedaccumulationoffeedbackratherthansimple accumulation, or ones that normalize the reward somehow rather than looking at absolute numbers. However even this basic rule gives rise to interesting properties. In particular,undercertainconditionsitguaranteesconvergenceto a“good" convention. Theorem7.7.16 Let g be a symmetric game as defined earlier, with x > 0 or y > 0 or x = y > 0, and either u < 0 or v < 0 or x < 0 or y < 0. Then if all agents employ the HCR rule, it is the case that for everyǫ > 0 there exists anintegerδ suchthatafterδ iterationsoftheprocesstheprobabilitythatasocial convention is reached is greater than 1 ǫ. Once a convention is reached, it is − neverleft. Furthermore,thisconventionguaranteestotheagentapayoffwhichis nolessthanthemaxminvalueofg. Therearemanymorequestionstoaskabouttheevolutionofconventions: How quickly does a convention evolve? How does this time depend on the various parameters,forexamplem, thehistory remembered? How doesit dependonthe initialchoicesofaction? Howdoestheparticularconventionreached—sincethere are many—dependonthese variables? The discussionbelowpoints the readerto furtherreadingonthistopic. 7.8 Historyandreferences Therearequiteafewbroadintroductionsto,andtextbookson,single-agentlearning. Incontrast,therearefewgeneralintroductionstotheareaofmultiagentlearning. Fudenberg and Levine [1998] provide a comprehensive survey of the area from a game-theoreticperspective, as does Young [2004]. A special issue of the JournalofArtificialIntelligence[VohraandWellman,2007]lookedatthefoundationsofthearea. PartsofthischapterarebasedonShohametal. [2007]fromthat specialissue. Someofthespecificreferencesareasfollows. FictitiousplaywasintroducedbyBrown[1951]andRobinson[1951]. Theconvergence results for fictitious play in Theorem 7.2.5 are taken respectively from Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 234 7 LearningandTeaching Robinson [1951], Nachbar [1990], Monderer and Shapley [1996b] and Berger [2005]. Thenon-convergenceexampleappearedinShapley[1964]. Rational learning was introducedand analyzedby Kalai and Lehrer [1993]. A richliteraturefollowed,butthisremainstheseminalpaperonthetopic. Single-agentreinforcementlearningissurveyedinKaelblingetal. [1996].Some key publications in the literature include Bellman [1957] on value iteration in knownMDPs, andWatkins[1989]andWatkinsandDayan[1992]onQ-learning in unknown MDPs. The literature on multiagent reinforcement learning begins withLittman[1994]. Someothermilestonesinthislineofresearchareasfollows. Littman and Szepesvari [1996] completed the story regarding zero-sum games, ClausandBoutilier[1998]definedbelief-basedreinforcementlearningandshowed experimentalresultsinthecaseofpurecoordination(orteam)games,andHuand Wellman [1998], Bowling and Veloso [2001], and Littman [2001] attempted to generalize the approach to general-sum games. The R-max algorithm was introducedbyBrafmanandTennenholtz[2002],anditspredecessor,theE3algorithm, byKearnsandSingh[1998]. Thenotionofno-regretlearningcanbetracedtoBlackwell’sapproachabilitytheorem [Blackwell, 1956] and Hannan’s notion of Universal Consistency [Hannan, 1957].AgoodreviewofthehistoryofthislineofthoughtisprovidedinFosterand Vohra[1999]. The regret-matchingalgorithm and the analysis ofits convergence to correlated equilibria appears in Hart and Mas-Colell [2000]. Modifications of fictitiousplaythatexhibitnoregretarediscussedinFudenbergandLevine[1995] andFudenbergandLevine[1999]. TargetedlearningwasintroducedinPowersandShoham[2005b],andfurtherrefinedandextendedinPowersandShoham[2005a]andVuetal. [2006]. (However, thetermtargetedlearningwasinventedlatertoapplytothisapproachtolearning.) The replicator dynamic is borrowed from biology. While the concept can be traced back at least to Darwin, work that had the most influence on game theory is perhaps Taylor and Jonker [1978]. The specific model of replicator dynamics discussed here appears in Schuster and