to convergetoamixed-strategyequilibrium,wehavenotmadeanyclaimsaboutthe distributionoftheparticularoutcomesplayed. To better understand this point, consider the following example. Consider the Anti- Anti-CoordinationgameshowninFigure7.5. Coordination ClearlytherearetwopureNashequilibriaofthisgame,(A,B)and(B,A),and game onemixedNashequilibrium,inwhicheachagentmixesAandB withprobability 0.5. Eitherofthetwopure-strategyequilibriaearnseachplayerapayoffof1,and themixed-strategyequilibriumearnseachplayerapayoffof0.5. UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.2 Fictitiousplay 209 A B A 0,0 1,1 B 1,1 0,0 Figure7.5: TheAnti-Coordinationgame. Now let us see what happens when we have agents play the repeated AntiCoordination game using fictitious play. Let us assume that the weight function foreachplayerisinitializedto(1,0.5). Theplayofthefirstfewroundsisshown inTable7.2. Round 1’saction 2’saction 1’sbeliefs 2’sbeliefs 0 (1,0.5) (1,0.5) 1 B B (1,1.5) (1,1.5) 2 A A (2,1.5) (2,1.5) 3 B B (2,2.5) (2,2.5) 4 A A (3,2.5) (3,2.5) . . . . . . . . . . . . . . . Table7.2: FictitiousplayofarepeatedAnti-Coordinationgame. Asyoucansee,theplayofeachplayerconvergestothemixedstrategy(0.5,0.5), which is the mixed strategy Nash equilibrium. However, the payoff received by each player is 0, since the players never hit the outcomes with positive payoff. Thus, althoughthe empiricaldistribution of the strategies convergesto the mixed strategyNashequilibrium,theplayersmaynotreceivetheexpectedpayoffofthe Nashequilibrium,becausetheiractionsaremiscorrelated. Finally, the empirical distributions of players’ actions need not convergeat all. Consider the game in Figure 7.6. Note that this example, due to Shapley, is a modificationoftherock-paper-scissorsgame;thisgameisnotconstantsum. The uniqueNashequilibrium ofthis gameis foreachplayerto playthe mixed strategy(1/3,1/3,1/3). However,considerthe fictitious playofthe gamewhen player1’sweightfunctionhasbeeninitializedto(0,0,0.5) andplayer2’sweight function has been initialized to (0,0.5,0). The play of this game is shown in Table7.3. Althoughitisnotobviousfromthesefirstfewrounds,itcanbeshown thattheempiricalplayofthisgameneverconvergestoanyfixeddistribution. Forcertainrestrictedclassesofgamesweareguaranteedtoreachconvergence. Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 210 7 LearningandTeaching Rock Paper Scissors Rock 0,0 0,1 1,0 Paper 1,0 0,0 0,1 Scissors 0,1 1,0 0,0 Figure7.6: Shapley’sAlmost-Rock-Paper-Scissorsgame. Round 1’saction 2’saction 1’sbeliefs 2’sbeliefs 0 (0,0,0.5) (0,0.5,0) 1 Rock Scissors (0,0,1.5) (1,0.5,0) 2 Rock Paper (0,1,1.5) (2,0.5,0) 3 Rock Paper (0,2,1.5) (3,0.5,0) 4 Scissors Paper (0,3,1.5) (3,0.5,1) 5 Scissors Paper (0,1.5,0) (1,0,0.5) . . . . . . . . . . . . . . . Table 7.3: Fictitious play of a repeated game of the Almost-Rock-Paper-Scissors game. Theorem7.2.5 Each of the following is a sufficient condition for the empirical frequenciesofplaytoconvergeinfictitiousplay: • Thegameiszerosum; • Thegameissolvablebyiteratedeliminationofstrictlydominatedstrategies; • Thegameisapotentialgame;5 • Thegameis2 nandhasgenericpayoffs.6 × Overall,fictitiousplayisaninterestingmodeloflearninginmultiagentsystems not because it is realistic or because it provides strong guarantees, but because it 5. Actuallyanevenmoremoregeneralconditionapplieshere,thattheplayershave“identicalinterests,"but wewillnotdiscussthisfurtherhere. 6. Fulldiscussionofgenericityingamesliesoutsidethescopeofthisbook,buthereistheessentialidea,at leastforgamesinnormalform.Roughlyspeaking,agameinnormalformisgenericifitdoesnothaveany interestingpropertythatdoesnotalsoholdwithprobability1whenthepayoffsareselectedindependently fromasufficientlyrichdistribution(e.g.,theuniformdistributionoverafixedinterval). Ofcourse,tomake thisprecisewewouldneedtodefine“interesting”and“sufficiently.” Intuitively,though,thismeansthatthe payoffsdonothaveaccidentalproperties.Agamewhosepayoffsarealldistinctisnecessarilygeneric. UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.3 Rationallearning 211 isverysimpletostateandgivesrisetonontrivialproperties. Butitisverylimited; itsmodelofbeliefsandbeliefupdateismathematicallyconstraining,andisclearly implausibleasamodelofhumanlearning. Thereexistvariousvariantsoffictitious play that score somewhat better on both fronts. We will mention one of them— calledsmoothfictitiousplay—whenwediscussno-regretlearningmethods. 7.3 Rationallearning rationallearning Rationallearning(alsosometimescalledBayesianlearning)adoptsthesamegeneral model-based scheme as fictitious play. Unlike fictitious play, however, it alBayesian lowsplayerstohaveamuchrichersetofbeliefsaboutopponents’strategies. First, learning the setofstrategies ofthe opponentcanincluderepeated-gamestrategiessuchas TfTinthePrisoner’sDilemmagame,notonlyrepeatedstage-gamestrategies. Second,thebeliefsofeachplayerabouthisopponent’sstrategiesmaybeexpressedby anyprobabilitydistributionoverthesetofallpossiblestrategies. Asinfictitiousplay,eachplayerbeginsthegamewithsomepriorbeliefs. After Bayesian eachround,theplayerusesBayesianupdatingtoupdatethesebeliefs. LetSi be −i updating thesetofthe opponent’sstrategiesconsideredpossiblebyplayeri, andH bethe setofpossiblehistories ofthe game. Then we canuse Bayes’rule to expressthe probability assigned by player i to the event in which the opponent is playing a particularstrategys Si giventheobservationofhistoryh H,as −i ∈ −i ∈ P (hs )P (s ) P (s h) = i | −i i −i . i −i | P (hs′ )P (s′ ) s′ −i ∈S − i i i | −i i −i P For example, consider two players playing the infinitely repeated Prisoner’s Dilemmagame,reproducedinFigure7.7. C D C 3,3 0,4 D 4,0 1,1 Figure7.7: Prisoner’sDilemmagame Supposethatthesupportofthepriorbeliefofeachplayer(i.e.,thestrategiesof theopponenttowhichtheplayerascribesnonzeroprobability;seeDefinition3.2.6) consistsofthestrategiesg ,g ,...g ,definedasfollows. g isthetriggerstrat1 2