∞ ∞ triggerstrategy egythatwaspresentedinSection6.1.2. Aplayerusingthetriggerstrategybegins the repeated game by cooperating, and if his opponent defects in any round, he defects in everysubsequentround. For T < , g coincideswith g at all hisT ∞ ∞ Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 212 7 LearningandTeaching toriesshorterthanT butprescribesunprovokeddefectionstartingfromtimeT on. Followingthisconvention,strategyg isthestrategyofconstantdefection. 0 Suppose furthermore that each player happensindeed to select a best response from among g ,g ,...,g . (There are of course infinitely many additional best 0 1 ∞ responsesoutsidethisset.) Thuseachroundofthegamewillbeplayedaccording tosomestrategyprofile(g ,g ). T1 T2 Afterplayingeachroundofthe repeatedgame,eachplayerperformsBayesian updating.Forexample,ifplayerihasobservedthatplayerjhasalwayscooperated, theBayesianupdatingafterhistoryh H oflengthtreducesto t ∈ 0 ifT t; P i (g T | h t ) = ( ∞ k= P t i + ( 1 gT P ) i(gk) ifT ≤ > t. P Rational learning is a very intuitive model of learning, but its analysis is quite involved. The formal analysis focuses on self-play, that is, on properties of the repeatedgameinwhichallagentsemployrationallearning(thoughtheymaystart withdifferentpriors). Broadly,thehighlightsofthismodelareasfollows. • Under some conditions, in self-play rational learning results in agents having closetocorrectbeliefsabouttheobservableportionoftheiropponent’sstrategy. • Undersomeconditions,in self-playrationallearningcausesthe agentsto convergetowardaNashequilibriumwithhighprobability. • Chiefamongthese“conditions”isabsolutecontinuity,astrongassumption. In the remainderofthis sectionwe discussthese pointsin moredetail, starting withthenotionofabsolutecontinuity. Definition7.3.1(Absolutecontinuity) Let X be a set and let µ,µ′ Π(X) be ∈ probability distributions over X. Then the distribution µ is said to be absolutely absolute continuouswithrespecttothedistributionµ′ iffforx X thatismeasurable7 it ⊂ continuity isthecasethatifµ(x) > 0thenµ′(x) > 0. Note that the players’ beliefs and the actual strategies each induce probability distributionsoverthesetofhistoriesH. Lets = (s ,...,s )beastrategyprofile. 1 n Ifweassumethatthesestrategiesareusedbytheplayers,wecancalculatetheprobabilityofeachhistoryofthegameoccurring,thusinducingadistributionoverH. Wecanalsoinducesuchadistributionwithaplayer’sbeliefsaboutplayers’strategies. Let Si be a set of strategies that i believespossible for j, and Pi Π(Si) j j ∈ j be the distribution over Si believed by player i. Let P = (Pi,...,Pi) be the j i 1 n tuple of beliefs about the possible strategies of every player. Now, if player i assumesthatallplayers(includinghimself)willplayaccordingtohisbeliefs,hecan 7. RecallthataprobabilitydistributionoveradomainXdoesnotnecessarilygiveavalueforallsubsetsof X,butonlyoversomeσ-algebraofX,thecollectionofmeasurablesets. UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.3 Rationallearning 213 alsocalculatetheprobabilityofeachhistoryofthegameoccurring,thusinducinga distributionoverH. Theresultsthatfollowallrequirethatthedistributionoverhistoriesinducedbytheactualstrategiesisabsolutelycontinuouswith respecttothe distributioninducedbyaplayer’sbeliefs;inotherwords,ifthereisapositiveprobabilityofsomehistorygiventheactualstrategies,thentheplayer’sbeliefsshould alsoassignthehistorypositiveprobability. (Colloquially,itissometimessaidthat grainoftruth thebeliefsoftheplayersmustcontainagrainoftruth.) Althoughthe resultsthat followareveryelegant,itmustbesaidthattheabsolutecontinuityassumptionisa significantlimitationofthetheoreticalresultsassociatedwithrationallearning. In the Prisoner’s Dilemma example discussed earlier, it is easy to see that the distribution of histories induced by the actual strategies is absolutely continuous with respect to the distribution predicted by the prior beliefs of the players. All positive probability histories in the game are assigned positive probability by the original beliefs of both players: if the true strategies are g ,g , players assign T1 T2 positive probability to the history with cooperation up to time t < min(T ,T ) 1 2 anddefectioninalltimesexceedingthemin(T ,T ). 1 2 The rational learning model is interesting because it has some very desirable properties. Roughly speaking, players satisfying the assumptions of the rational learningmodelwillhavebeliefs aboutthe playofthe otherplayersthatconverge to the truth, and furthermore, players will in finite time converge to play that is arbitrarilyclosetotheNashequilibrium. Beforewecanstatetheseresultsweneed todefineameasureofthesimilarityoftwoprobabilitymeasures. Definition7.3.2(ǫ-closeness) Given an ǫ > 0 and two probability measures µ andµ′onthesamespace,wesaythatµisǫ-closetoµ′ifthereisameasurableset Qsatisfying: • µ(Q)andµ′(Q)areeachgreaterthan1 ǫ;and − • ForeverymeasurablesetA Q,wehavethat ⊆ (1+ǫ)µ′(A) µ(A) (1 ǫ)µ′(A). ≥ ≥ − Now we can state a result about the accuracy of the beliefs of a player using rationallearning. Theorem7.3.3(Rationallearningandbeliefaccuracy) Letsbearepeated-game strategyprofileforagivenn-playergame8,andletP = P ,...,P beatupleof 1 n probabilitydistributionsoversuchstrategyprofiles(P