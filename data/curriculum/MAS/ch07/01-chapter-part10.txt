. . . . . . . . . . . . . . . . Table7.4: Minimax-QlearninginarepeatedMatchingPenniesgame. R-max Oneexampleofsuchanalgorithmisthemodel-basedlearningalgorithmR-max. algorithm It first initializes its estimate of the value of each state to be the highest reward that can be returned in the game (hence the name). This philosophy has been referredtoasoptimisminthefaceofuncertaintyandhelpsguaranteethattheagent will explore its environment to the best of its ability. The agent then uses these optimistic values to calculate a maxmin strategy for the game. Unlike normal Qlearning, the algorithm does not update its values for any state and action profile pairuntilithasvisitedthem“enough”timestohaveagoodestimateofthereward Chernoffbounds andtransitionprobabilities. UsingatheoreticalmethodcalledChernoffbounds,it is possible to polynomially bound the number of samples necessary to guarantee that the accuracy of the average over the samples deviates from the true average by at most ǫ with probability (1 δ) for any selected value of ǫ and δ. The − polynomialisinΣ,k,T,1/ǫ,and1/δ,whereΣisthenumberofstates(orgames) inthestochasticgame,kisthenumberofactionsavailabletoeachagentinagame (withoutlossofgenerallywecanassumethatthisisthesameforallagentsandall mixingtime games),andT istheǫ-returnmixingtimeoftheoptimalpolicy,thatis,thesmallest length of time after which the optimal policy is guaranteed to yield an expected payoffat most ǫ away from optimal. The notes at the end ofthe chapterpointto E3algorithm furtherreadingonR-max,andapredecessoralgorithmcalledE3(pronounced“E cubed”). 7.4.3 Beyondzero-sum stochasticgames Sofarwehaveshownresultsfortheclassofzero-sumstochasticgames. Although the algorithms discussed, in particular minimax-Q, are still well defined in the Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 220 7 LearningandTeaching general-sum case, the guarantee of achieving the maxmin strategy payoff is less compelling. Anothersubclassofstochasticgamesthathas beenaddressedis that ofcommon-payoff(purecoordination)games,inwhichallagentsreceivethesame reward for an outcome. This class has the advantage of reducing the problem to identifying an optimal action profile and coordinating with the other agents to play it. In many ways this problem can really be seen as a single-agentproblem of distributed control. This is a relatively well-understood problem, and various algorithmsexistforit,dependingonpreciselyhowtheproblemisdefined. Expanding reinforcement learning algorithms to the general-sum case is quite problematic,ontheotherhand. TherehavebeenattemptstogeneralizeQ-learning to general-sum games, but they have not yet been truly successful. As was discussed at the beginning of this chapter, the question of what it means to learn in general-sum games is subtle. One yardstick we have discussed is convergence to Nash equilibrium of the stage game during self play. No generalization of Qlearninghasbeenputforwardthathasthisproperty. 7.4.4 Belief-basedreinforcement learning Thereisalsoaversionofreinforcementlearningthatincludesexplicitmodelingof theotheragent(s),givenbythefollowingequations. Q (s ,a ) (1 α)Q (s ,a )+α (r(s ,a )+βV (s )) t+1 t t t t t t t t t t+1 ← − V (s) max Q (s,(a ,a ))Pr (a ) t t i −i i −i ← ai a−Xi⊂A−i In this version, the agent updates the value of the game using the probability he assigns to the opponent(s) playing each action profile. Of course, the belief functionmustbeupdatedaftereachplay. How itis updateddependsonwhatthe functionis. Indeed, belief-basedreinforcementlearningis nota singleprocedure but a family, each member characterized by how beliefs are formed and updated. Forexample,inoneversionthebeliefsareofthekindconsideredinfictitiousplay, andin anothertheyareBayesianin thestyleofrationallearning. Therearesome experimental results that show convergence to equilibrium in self-play for some versionsofbelief-basedreinforcementlearningandsomeclassesofgames,butno theoreticalresults. 7.5 No-regret learning anduniversal consistency As discussed above, a learning rule is universally consistent or (equivalently)exhibitsnoregretif,looselyspeaking,againstanysetofopponentsityieldsapayoff thatisnolessthanthepayofftheagentcouldhaveobtainedbyplayinganyoneof hispurestrategiesthroughout. More precisely, let αt be the average per-period reward the agent received up untiltimet,andletαt(s )betheaverageper-periodrewardtheagentwouldhave i UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.5 No-regretlearninganduniversalconsistency 221 receivedup until time t had he playedpure strategys instead, assumingall