a repeated game of Matching Pennies. If both agentsrepeatedlyplayed(H,H)and(T,T),thefrequencyofboththeirplayswould 3. Theliterary-mindedreadermayberemindedofthequotefromOscarWilde’sAWomanofNoImportance: “[...] theworsttyrannytheworldhaseverknown; thetyrannyoftheweakoverthestrong. Itistheonly tyrannythateverlasts.”Excepthereitisthetyrannyofthesimpletonoverthesophisticated. 4. However, recent theoretical progress on the complexity of computing a Nash equilibrium (see Section4.2.1)raisesdoubtsaboutwhetheranysuchprocedurecouldbeguaranteedtoconvergetoanequilibrium, atleastwithinpolynomialtime. Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 204 7 LearningandTeaching convergeto(.5,.5), thestrategyinthe uniqueNashequilibrium,eventhoughthe payoffsobtainedwouldbeverydifferentfromtheequilibriumpayoffs. Thirdandyetmoreradically,wecangiveupentirelyonNashequilibriumasthe relevant solution concept. One alternative is to seek convergenceto a correlated equilibriumofthestagegame. Thisisinterestinginanumberofways. No-regret learning,whichwediscusslater,canbeshowntoconvergetocorrelatedequilibria in certain cases. Indeed, convergenceto a correlatedequilibriumprovidesa justification forthe no-regretlearningconcept; the “correlatingdevice"in this case is notanabstractnotion,butthepriorhistoryofplay. Finally, we can give up on convergence to stationary policies, but require that thenon-stationarypoliciesconvergeto aninterestingstate. Inparticular,learning strategiesthatincludebuildinganexplicitmodeloftheopponents’strategies(aswe shallsee,thesearecalledmodel-basedlearningrules)canberequiredtoconverge tocorrectmodelsoftheopponents’strategies. Prescriptivetheories Incontrastwithdescriptivetheories,prescriptivetheoriesaskhowagents—people, programs, or otherwise—should learn. A such they are not required to show a match with real-world phenomena. By the same token, their main focus is not onbehavioralproperties, thoughthey mayinvestigateconvergenceissuesas well. For the most part, we will concentrate on strategic normative theories, in which individualagentsareself-motivated. In zero-sum games, and even in repeated or stochastic zero sum games, it is meaningfultoaskwhetheranagentis learningin anoptimalfashion. Butin general this question is not meaningful, since the answer depends not only on the learningbeingdonebutalsoonthebehaviorofotheragentsinthesystem. When all agentsadoptthe samestrategy (e.g., they alladoptTfT, or alladoptreinforceself-play mentlearning, tobe discussedshortly), this is called self-play. Onewayto judge learningproceduresisbasedontheirperformanceinself-play. However,learning agentscan be judgedalso by how theydo in the contextofothertypes ofagents; aTfTagentmayperformwellagainstanotherTfT agent,butless wellagainstan agentusingreinforcementlearning. Nolearningprocedureisoptimalagainstallpossibleopponentbehaviors. This observationissimplyaninstanceofthegeneralmoveingametheoryawayfromthe notionof“optimalstrategy”andtoward“bestresponse”andequilibrium. Indeed, in the broad sense in which we use the term, a “learning strategy” is simply a strategyinagamethathasaparticularstructure(namely,thestructureofarepeated orstochastic game)that happensto havea componentthat is naturally viewedas adaptive. So how do we evaluate a prescriptive learning strategy? There are several answers. Thefirstistoadoptthestandardgame-theoreticstance: giveuponjudging astrategyinisolation,andinsteadaskwhichlearningrulesareinequilibriumwith eachother. Notethatrequiringthatrepeated-gamelearningstrategiesbeinequilibUncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.1 Whythesubjectof“learning”iscomplex 205 riumwitheachotherisverydifferentfromtheconvergencerequirementsdiscussed above;thosespeakaboutequilibriuminthestagegame,notintherepeatedgame. For example, TfT is in equilibrium with itself in an infinitely repeated Prisoner’s Dilemmagame,butdoesnotleadtotherepeatedDefectplay,theonlyNashequilibrium of the stage game. This “equilibrium of learning strategies” approach is notcommon,butweshallseeoneexampleofitlateron. Amoremodest,butbyfarmorecommonandperhapsmorepracticalapproach istoaskwhetheralearningstrategyachievespayoffsthatare“highenough.”This approachisbothstrongerandweakerthantherequirementof“bestresponse.”Best responserequiresthatthestrategyyieldthehighestpossiblepayoffagainstaparticularstrategyoftheopponent(s). Afocuson“highenough”payoffscanconsidera broaderclassofopponents,butmakesweakerrequirementsregardingthepayoffs, whichareallowedtofallshortofbestresponse. Thereareseveraldifferentversionsofsuchhigh-payoffrequirements,eachadoptingand/orcombiningdifferentbasicproperties. safetyofa Property7.1.3(Safety) A learningrule is safe if it guaranteesthe agentatleast learningrule its maxmin payoff, or “security value.” (Recall that this is the payoff the agent canguaranteetohimselfregardlessofthestrategiesadoptedbytheopponents;see Definition3.4.1.) rationalityofa Property7.1.4(Rationality) Alearningruleisrationalifwhenevertheopponent learningrule settles on a stationary strategy of the stage game (i.e., the opponent adopts the samemixedstrategyeachtime, regardlessofthepast), theagentsettles ona best responsetothatstrategy. universal Property7.1.5(No-regret,informal) Alearningruleisuniversallyconsistent,or consistency Hannanconsistent,orexhibitsnoregret(theseareallsynonymousterms),if,loosely speaking, against any set of opponents it yields a payoff that is no less than the Hannan payoff the agent could have obtained by playing any one of his pure strategies consistency throughout. Wegiveamoreformaldefinitionofthisconditionlaterinthechapter. no-regret Some of these basic requirements are quite strong, and can be weakened in a varietyofways. Onewayistoallowslightdeviations,eitherintermsofthemagnitudeofthepayoffobtained,ortheprobabilityofobtainingit,orboth. Forexample, rather than require optimality, one can require ǫ,δ-optimality, meaning that with probability of at least 1 δ the agent’s payoff comes within ǫ of the payoff ob- − tainedbythebestresponse.Anotherwayofweakeningtherequirementsistolimit theclassofopponentsagainstwhichtherequirementholds.Forexample,attention canberestricted tothecaseofselfplay,in whichtheagentplaysacopyofitself. (Note that while the learning strategies are identical, the game being played may notbesymmetric.)Forexample,onemightrequirethatthelearningruleguarantee convergenceinselfplay. Morebroadly,asinthecaseoftargetedoptimality,which we discuss later, onemight requirea bestresponseonly againsta particularclass ofopponents. Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 206 7 LearningandTeaching In the next sections, as we discuss several learning rules, we will encounter various versionsof these requirements and their combinations. For the most part wewillconcentrateonrepeated,two-playergames,thoughinsomecaseswewill broaden the discussion and discuss stochastic games and games with