extendingthe methods togeneral-sumstochasticgames. 7.4.1 LearninginunknownMDPs First, consider(single-agent)MDPs. Value iteration, asdescribedinAppendixC, assumesthattheMDPisknown. Whatifwedonotknowtherewardsortransition probabilitiesoftheMDP? Itturnsoutthat, ifwe alwaysknowwhatstate9 we are in and the reward received in each iteration, we can still converge to the correct Q-values. 9. Forconsistencywiththeliteratureonreinforcementlearning, inthissectionweusethenotationsand S forastateandsetofstatesrespectively, ratherthanforastrategyprofileandsetofstrategyprofilesas elsewhereinthebook. Freeforon-screenuse;pleasedonotdistribute.Youcangetanotherfreecopy ofthisPDFororderthebookathttp://www.masfoundations.org. 216 7 LearningandTeaching Definition7.4.1(Q-learning) Q-learningisthefollowingprocedure: InitializetheQ-functionandV values(arbitrarily,forexample) repeatuntilconvergence Observethecurrentstates . t Selectactiona andandtakeit. t Observetherewardr(s ,a ) t t Performthefollowingupdates(anddonotupdateanyotherQ-values): Q (s ,a ) (1 α)Q (s ,a )+α (r(s ,a )+βV (s )) t+1 t t t t t t t t t t+1 ← − V (s) max Q (s,a) t+1 a t ← Theorem7.4.2 Q-learningguaranteesthattheQandV valuesconvergetothose of the optimal policy, provided that each state-action pair is sampled an infinite numberoftimes,andthatthetime-dependentlearningrateα obeys0 α < 1, t t ∞α = and ∞α2 < . ≤ 0 t ∞ 0 t ∞ P P Theintuitionbehindthisapproachisthatweapproximatetheunknowntransition probability by using the actual distribution of states reached in the game itself. Notice that this still leaves us a lot of room in designing the order in which the algorithmselectsactions. Notethatthistheoremsaysnothingabouttherateofconvergence.Furthermore, it gives no assuranceregardingthe accumulationof optimal future discounted rewardsbytheagent;itcouldwellbe,dependingonthediscountfactor,thatbythe time the agent convergesto the optimal policy it has paid too high a cost, which cannotberecoupedbyexploitingthepolicygoingforward. Thisisnotaconcernif thelearningtakesplaceduringtrainingsessions,andonlywhenlearninghasconvergedsufficientlyistheagentunleashedontheworld(e.g.,thinkofafighterpilot beingtrainedonasimulatorbeforegoingintocombat). ButingeneralQ-learning shouldbethoughtofasguaranteeinggoodlearning,butneitherquicklearningnor highfuturediscountedrewards. 7.4.2 Reinforcementlearninginzero-sum stochasticgames In order to adapt the method presented from the setting of MDPs to stochastic games,wemustmakea few modifications. The simplestpossiblemodificationis to have each agent ignore the existence of the other agent (recall that zero-sum games involve only two agents). We then define Qπ : S A R to be the i × i 7→ valueforplayeriifthetwoplayersfollowstrategyprofileπ afterstartinginstate s andplayerichoosesthe actiona. We can nowapplythe Q-learningalgorithm. Asmentionedearlierinthechapter,themultiagentsettingforcesusto foregoour search for an “optimal” policy, and instead to focus on one that performs well against its opponent. For example, we might require than it satisfy Hannan consistency (Property 7.1.5). Indeed, the Q-learning procedure can be shown to be UncorrectedmanuscriptofMultiagentSystems,publishedbyCambridgeUniversityPress Revision1.1©Shoham&Leyton-Brown,2009,2010. 7.4 Reinforcementlearning 217 Hannan-consistentforanagentinastochasticgameagainstopponentsplayingstationarypolicies. However,againstopponentsusingmorecomplexstrategies,such asQ-learningitself,wedonotobtainsuchaguarantee. Theaboveapproach,assumingawaytheopponent,seemsunmotivated. Instead, iftheagentisawareofwhatactionsitsopponentselectedateachpointinitshistory, wecanuseamodifiedQ-function,Qπ : S A R,definedoverstatesandaction i × 7→ profiles,whereA = A A . TheformulatoupdateQ is simpleto modifyand 1 2 × wouldbethefollowingforatwo-playergame. Q (s ,a ,o )= (1 α )Q (s ,a ,o )+α (r (s ,a ,o )+βV (s )) i,t+1 t t t t i,t t t t t i t t t t t+1 − Nowthattheactionsrangeoverbothouragent’sactionsandthatofitscompetitor, how can we calculate the value of a state? Recall that for (two-player) zero-sum games,thepolicyprofilewhereeachagentplaysitsmaxminstrategyformsaNash equilibrium. Thepayofftothefirstagent(andthusthenegativeofthepayofftothe valueofa secondagent)iscalledthevalueofthegame,anditformsthebasisforourrevised zero-sumgame valuefunctionforQ-learning, V (s)= maxminQ (s,Π (s),o). t i,t i Πi o minimax-Q Like the basic Q-learning algorithm, the above minimax-Q learning algorithm is guaranteed to converge in the limit of infinite samples of each state and action profile pair. While this will guarantee the agent a payoff at least equal to that of its maxmin strategy, it no longer satisfies Hannan consistency. If the opponent is playing a suboptimal strategy, minimax-Q will be unable to exploit it in most games. The minimax-Q algorithm is described in Figure 7.8. Note that this algorithm specifies not only how to updatethe Q and V values, but also how