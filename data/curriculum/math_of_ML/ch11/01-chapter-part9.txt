to the parameters estimation problem via EMalgorithm maximum likelihood. The expectation maximization algorithm (EM algoDraft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 11.3 EMAlgorithm 361 rithm) was proposed by Dempster et al. (1977) and is a general iterative schemeforlearningparameters(maximumlikelihoodorMAP)inmixture modelsand,moregenerally,latent-variablemodels. InourexampleoftheGaussianmixturemodel,wechooseinitialvalues forµ ,Σ ,π andalternateuntilconvergencebetween k k k E-step: Evaluate the responsibilities r (posterior probability of data nk pointnbelongingtomixturecomponentk). M-step: Use the updated responsibilities to reestimate the parameters µ ,Σ ,π . k k k EverystepintheEMalgorithmincreasesthelog-likelihoodfunction(Neal and Hinton, 1999). For convergence, we can check the log-likelihood or the parameters directly. A concrete instantiation of the EM algorithm for estimatingtheparametersofaGMMisasfollows: 1. Initializeµ ,Σ ,π . k k k 2. E-step: Evaluate responsibilities r for every data point x using curnk n rentparametersπ ,µ ,Σ : k k k (cid:0) (cid:1) π x µ , Σ r = k N n | k k . (11.53) nk (cid:80) (cid:0) (cid:1) π x µ , Σ j j N n | j j 3. M-step: Reestimate parameters π ,µ ,Σ using the current responsik k k bilitiesr (fromE-step): nk N 1 (cid:88) µ = r x , (11.54) k N nk n k n=1 N 1 (cid:88) Σ = r (x µ )(x µ )(cid:62), (11.55) k N nk n − k n − k k n=1 N π = k . (11.56) k N Example 11.6 (GMM Fit) Figure11.8 EM algorithmappliedto 0.30 theGMMfrom 0.25 Figure11.2.(a) 0.20 FinalGMMfit; 0.15 (b)negative 0.10 log-likelihoodasa functionoftheEM 0.05 iteration. 0.00 5 0 5 10 15 − x )x(p π1N (x | µ1,σ12) 28 π2N (x | µ2,σ22) 26 π3N (x | µ3,σ32) GMMdensity 24 22 20 18 16 14 0 1 2 3 4 5 Iteration (a)FinalGMMfit.Afterfiveiterations,theEM algorithmconvergesandreturnsthisGMM. doohilekil-golevitageN (b)Negativelog-likelihoodasafunctionofthe EMiterations. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 362 DensityEstimationwithGaussianMixtureModels Figure11.9 10 Illustrationofthe EMalgorithmfor 5 fittingaGaussian mixturemodelwith 0 threecomponentsto atwo-dimensional 5 − dataset.(a)Dataset; (b)negative 10 log-likelihood − − 10 − 5 x 0 1 5 10 (lowerisbetter)as afunctionoftheEM iterations.Thered dotsindicatethe iterationsforwhich themixture componentsofthe correspondingGMM fitsareshownin(c) through(f).The yellowdiscsindicate themeansofthe Gaussianmixture components. Figure11.10(a) showsthefinal GMMfit. 2x 104 6 103 × 4 103 × 0 20 40 60 EMiteration (a)Dataset. doohilekil-golevitageN (b)Negativelog-likelihood. 10 5 0 5 − 10 − 10 5 0 5 10 − − x1 2x 10 5 0 5 − 10 − 10 5 0 5 10 − − x1 (c)EMinitialization. 2x (d)EMafteroneiteration. 10 5 0 5 − 10 − 10 5 0 5 10 − − x1 2x 10 5 0 5 − 10 − 10 5 0 5 10 − − x1 (e)EMafter10iterations. 2x (f)EMafter62iterations. WhenwerunEMonourexamplefromFigure11.3,weobtainthefinal result shown in Figure 11.8(a) after five iterations, and Figure 11.8(b) shows how the negative log-likelihood evolves as a function of the EM iterations.ThefinalGMMisgivenas (cid:0) (cid:1) (cid:0) (cid:1) p(x) = 0.29 x 2.75, 0.06 +0.28 x 0.50, 0.25 N | − N | − (11.57) (cid:0) (cid:1) +0.43 x 3.64, 1.63 . N | We applied the EM algorithm to the two-dimensional dataset shown in Figure 11.1 with K = 3 mixture components. Figure 11.9 illustrates some steps of the EM algorithm and shows the negative log-likelihood as a function of the