assumed that the number of components K isknown.Inpractice,thisisoftennotthecase.However,wecoulduse nestedcross-validation,asdiscussedinSection8.6.1,tofindgoodmodels. GaussianmixturemodelsarecloselyrelatedtotheK-meansclustering algorithm. K-means also uses the EM algorithm to assign data points to clusters. If we treat the means in the GMM as cluster centers and ignore the covariances (or set them to I), we arrive at K-means. As also nicely describedbyMacKay(2003),K-meansmakesa“hard”assignmentofdata points to cluster centers µ , whereas a GMM makes a “soft” assignment k viatheresponsibilities. Weonlytoucheduponthelatent-variableperspectiveofGMMsandthe EMalgorithm.NotethatEMcanbeusedforparameterlearningingeneral latent-variable models, e.g., nonlinear state-space models (Ghahramani andRoweis,1999;RoweisandGhahramani,1999)andforreinforcement learningasdiscussedbyBarber(2012).Therefore,thelatent-variableperspective of a GMM is useful to derive the corresponding EM algorithm in aprincipledway(Bishop,2006;Barber,2012;Murphy,2012). We only discussed maximum likelihood estimation (via the EM algorithm)forfindingGMMparameters.Thestandardcriticismsofmaximum likelihoodalsoapplyhere: As in linear regression, maximum likelihood can suffer from severe overfitting. In the GMM case, this happens when the mean of a mixturecomponentisidenticaltoadatapointandthecovariancetendsto 0. Then, the likelihood approaches infinity. Bishop (2006) and Barber (2012)discussthisissueindetail. We only obtain a point estimate of the parameters π ,µ ,Σ for k = k k k 1,...,K, which does not give any indication of uncertainty in the parametervalues.ABayesianapproachwouldplaceapriorontheparameters,whichcanbeusedtoobtainaposteriordistributionontheparameters.Thisposteriorallowsustocomputethemodelevidence(marginal likelihood),whichcanbeusedformodelcomparison,whichgivesusa principled way to determine the number of mixture components. Unfortunately,closed-forminferenceisnotpossibleinthissettingbecause there is no conjugate prior for this model. However, approximations, such as variational inference, can be used to obtain an approximate posterior(Bishop,2006). Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 11.5 FurtherReading 369 0.30 Figure11.13 Histogram(orange 0.25 bars)andkernel densityestimation 0.20 (blueline).The kerneldensity 0.15 estimatorproduces 0.10 asmoothestimate oftheunderlying 0.05 density,whereasthe histogramisan 0.00 unsmoothedcount 4 2 0 2 4 6 8 − − x measureofhow manydatapoints (black)fallintoa singlebin. )x(p Data KDE Histogram In this chapter, we discussed mixture models for density estimation. Thereisaplethoraofdensityestimationtechniquesavailable.Inpractice, weoftenusehistogramsandkerneldensityestimation. histogram Histograms provide a nonparametric way to represent continuous densities and have been proposed by Pearson (1895). A histogram is constructedby“binning”thedataspaceandcount,howmanydatapointsfall intoeachbin.Thenabarisdrawnatthecenterofeachbin,andtheheight ofthebarisproportionaltothenumberofdatapointswithinthatbin.The bin size is a critical hyperparameter, and a bad choice can lead to overfitting and underfitting. Cross-validation, as discussed in Section 8.2.4, can beusedtodetermineagoodbinsize. kerneldensity Kerneldensityestimation,independentlyproposedbyRosenblatt(1956) estimation and Parzen (1962), is a nonparametric way for density estimation. Given N i.i.d. samples, the kernel density estimator represents the underlying distributionas N (cid:18) (cid:19) p(x) = 1 (cid:88) k x − x n , (11.74) Nh h n=1 wherekisakernelfunction,i.e.,anonnegativefunctionthatintegratesto 1 and h > 0 is a smoothing/bandwidth parameter, which plays a similar role as the bin size in histograms. Note that we place a kernel on every single data point x in the dataset. Commonly used kernel functions are n theuniformdistributionandtheGaussiandistribution.Kerneldensityestimatesarecloselyrelatedtohistograms,butbychoosingasuitablekernel, we can guarantee smoothness of the density estimate. Figure 11.13 illustrates the difference between a histogram and a kernel density estimator (withaGaussian-shapedkernel)foragivendatasetof250datapoints. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress.