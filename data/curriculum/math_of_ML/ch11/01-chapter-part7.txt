k n=1 wherer istheprobabilityvectordefinedin(11.25).Thisgivesusasimk pleupdateruleforΣ fork = 1,...,K andprovesTheorem11.2. k Similar to the update of µ in (11.20), we can interpret the update of k the covariance in (11.30) as an importance-weighted expected value of thesquareofthecentereddata ˜ := x µ ,...,x µ . X k { 1 − k N − k} Example 11.4 (Variance Updates) InourexamplefromFigure11.3,thevariancesareupdatedasfollows: σ2 : 1 0.14 (11.39) 1 → σ2 : 0.2 0.44 (11.40) 2 → σ2 : 3 1.53 (11.41) 3 → (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 358 DensityEstimationwithGaussianMixtureModels Here we see that the variances of the first and third component shrink significantly, whereas the variance of the second component increases slightly. Figure 11.6 illustrates this setting. Figure 11.6(a) is identical (but zoomed in) to Figure 11.5(b) and shows the GMM density and its individual components prior to updating the variances. Figure 11.6(b) shows theGMMdensityafterupdatingthevariances. Figure11.6 Effect ofupdatingthe 0.30 variancesinaGMM. 0.25 (a)GMMbefore updatingthe 0.20 variances;(b)GMM 0.15 afterupdatingthe 0.10 varianceswhile 0.05 retainingthemeans 0.00 andmixture 4 2 0 2 4 6 8 weights. − − x )x(p π1N (x | µ1,σ12) 0.35 π π 2 3 N N ( ( x x | | µ µ 2 3 , , σ σ 2 3 2 2 ) ) 0.30 GMMdensity 0.25 0.20 0.15 0.10 0.05 0.00 4 2 0 2 4 6 8 − − x (a) GMM density and individual components priortoupdatingthevariances. )x(p π1N (x | µ1,σ12) π2N (x | µ2,σ22) π3N (x | µ3,σ32) GMMdensity (b) GMM density and individual components afterupdatingthevariances. Similartotheupdateofthemeanparameters,wecaninterpret(11.30) as a Monte Carlo estimate of the weighted covariance of data points x n associated with the kth mixture component, where the weights are the responsibilities r . As with the updates of the mean parameters, this upnk datedependsonallπ ,µ ,Σ , j = 1,...,K,throughtheresponsibilities j j j r ,whichprohibitsaclosed-formsolution. nk 11.2.4 Updating the Mixture Weights Theorem11.3(UpdateoftheGMMMixtureWeights). Themixtureweights oftheGMMareupdatedas N πnew = k , k = 1,...,K, (11.42) k N whereN isthenumberofdatapointsandN isdefinedin(11.24). k Proof To find the partial derivative of the log-likelihood with respect to the weight parameters π , k = 1,...,K, we account for the conk (cid:80) straint π = 1 by using Lagrange multipliers (see Section 7.2). The k k Lagrangianis (cid:32) (cid:33) K (cid:88) L = +λ π 1 (11.43a) k L − k=1 Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 11.2 ParameterLearningviaMaximumLikelihood 359 (cid:32) (cid:33) N K K (cid:88) (cid:88) (cid:0) (cid:1) (cid:88) = log π x µ , Σ +λ π 1 , (11.43b) k N n | k k k − n=1 k=1 k=1 where is the log-likelihood from (11.10) and the second term encodes L for the equality constraint that all the mixture weights need to sum up to 1.Weobtainthepartialderivativewithrespecttoπ as k ∂L = (cid:88) N N (cid:0) x n | µ k , Σ k (cid:1) +λ (11.44a) ∂π k n=1 (cid:80)K j=1 π j N (cid:0) x n | µ j , Σ j (cid:1) = 1 (cid:88) N π k N (cid:0) x n | µ k , Σ k (cid:1) +λ = N k +λ, (11.44b) π k