1). An illustration is given in Figure 11.2, displaying the weighted (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 350 DensityEstimationwithGaussianMixtureModels Figure11.2 0.30 Gaussianmixture model.The 0.25 Gaussianmixture distribution(black) 0.20 iscomposedofa 0.15 convexcombination ofGaussian 0.10 distributionsandis moreexpressive 0.05 thananyindividual component.Dashed 0.00 linesrepresentthe 4 2 0 2 4 6 8 weightedGaussian − − x components. )x(p Component1 Component2 Component3 GMMdensity componentsandthemixturedensity,whichisgivenas p(x θ) = 0.5 (cid:0) x 2, 1(cid:1) +0.2 (cid:0) x 1, 2 (cid:1) +0.3 (cid:0) x 4, 1 (cid:1) . (11.5) | N | − 2 N | N | 11.2 Parameter Learning via Maximum Likelihood Assume we are given a dataset = x ,...,x , where x , n = 1 N n X { } 1,...,N, are drawn i.i.d. from an unknown distribution p(x). Our objective is to find a good approximation/representation of this unknown distribution p(x) by means of a GMM with K mixture components. The parameters of the GMM are the K means µ , the covariances Σ , and k k mixture weights π . We summarize all these free parameters in θ := k π ,µ ,Σ : k = 1,...,K . { k k k } Example 11.1 (Initial Setting) Figure11.3 Initial 0.30 setting:GMM (black)with 0.25 mixturethree 0.20 mixturecomponents (dashed)andseven 0.15 datapoints(discs). 0.10 0.05 0.00 5 0 5 10 15 − x )x(p π1N (x | µ1,σ1 2) π2N (x | µ2,σ2 2) π3N (x | µ3,σ3 2) GMMdensity Throughout this chapter, we will have a simple running example that helpsusillustrateandvisualizeimportantconcepts. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 11.2 ParameterLearningviaMaximumLikelihood 351 We consider a one-dimensional dataset = 3, 2.5, 1,0,2,4,5 X {− − − } consisting of seven data points and wish to find a GMM with K = 3 componentsthatmodelsthedensityofthedata.Weinitializethemixture componentsas (cid:0) (cid:1) p (x) = x 4, 1 (11.6) 1 N | − (cid:0) (cid:1) p (x) = x 0, 0.2 (11.7) 2 N | (cid:0) (cid:1) p (x) = x 8, 3 (11.8) 3 N | and assign them equal weights π = π = π = 1. The corresponding 1 2 3 3 model(andthedatapoints)areshowninFigure11.3. In the following, we detail how to obtain a maximum likelihood estimate θ of the model parameters θ. We start by writing down the likeML lihood, i.e., the predictive distribution of the training data given the parameters. We exploit our i.i.d. assumption, which leads to the factorized likelihood N K (cid:89) (cid:88) (cid:0) (cid:1) p( θ) = p(x θ), p(x θ) = π x µ , Σ , (11.9) X | n | n | k N n | k k n=1 k=1 where every individual likelihood term p(x θ) is a Gaussian mixture n | density.Thenweobtainthelog-likelihoodas N N K (cid:88) (cid:88) (cid:88) (cid:0) (cid:1) logp( θ) = logp(x θ) = log π x µ , Σ . (11.10) X | n | k N n | k k n=1 n=1 k=1 (cid:124) (cid:123)(cid:122) (cid:125) =:L Weaimtofindparametersθ∗ thatmaximizethelog-likelihood defined ML L in (11.10). Our “normal” procedure would be to compute the gradient d /dθ of the log-likelihood with respect to the model parameters θ, set L it to