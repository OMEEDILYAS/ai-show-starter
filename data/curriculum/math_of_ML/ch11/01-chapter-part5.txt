N r x , n=1 nk n n=1 nk k ⇐⇒ k (cid:80)N n=1 r nk N k n=1 nk n (11.23) wherewedefined N (cid:88) N := r (11.24) k nk n=1 as the total responsibility of the kth mixture component for the entire dataset.ThisconcludestheproofofTheorem11.1. Intuitively,(11.20)canbeinterpretedasanimportance-weightedMonte Carlo estimate of the mean, where the importance weights of data point x are the responsibilities r of the kth cluster for x , k = 1,...,K. n nk n Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 11.2 ParameterLearningviaMaximumLikelihood 355 Therefore, the mean µ k is pulled toward a data point x n with strength Figure11.4 Update givenbyr .Themeansarepulledstrongertowarddatapointsforwhich ofthemean nk parameterof thecorrespondingmixturecomponenthasahighresponsibility,i.e.,ahigh mixturecomponent likelihood.Figure11.4illustratesthis.WecanalsointerpretthemeanupinaGMM.The date in (11.20) as the expected value of all data points under the distri- meanµisbeing butiongivenby pulledtoward individualdata r k := [r 1k ,...,r Nk ](cid:62)/N k , (11.25) pointswiththe weightsgivenbythe whichisanormalizedprobabilityvector,i.e., corresponding µ E [ ]. (11.26) responsibilities. k ← rk X x2 x3 r2 Example 11.3 (Mean Updates) x1 r1 r3 µ Figure11.5 Effect 0.30 ofupdatingthe 0.25 meanvaluesina GMM.(a)GMM 0.20 beforeupdatingthe 0.15 meanvalues; 0.10 (b)GMMafter updatingthemean 0.05 valuesµ kwhile 0.00 retainingthe 5 0 5 10 15 − x variancesand mixtureweights. )x(p π1N (x | µ1,σ12) 0.30 π2N (x | µ2,σ22) π3N (x | µ3,σ32) 0.25 GMMdensity 0.20 0.15 0.10 0.05 0.00 5 0 5 10 15 − x (a) GMM density and individual components priortoupdatingthemeanvalues. )x(p π1N (x | µ1,σ12) π2N (x | µ2,σ22) π3N (x | µ3,σ32) GMMdensity (b) GMM density and individual components afterupdatingthemeanvalues. In our example from Figure 11.3, the mean values are updated as follows: µ : 4 2.7 (11.27) 1 − → − µ : 0 0.4 (11.28) 2 → − µ : 8 3.7 (11.29) 3 → Here we see that the means of the first and third mixture component move toward the regime of the data, whereas the mean of the second component does not change so dramatically. Figure 11.5 illustrates this change, where Figure 11.5(a) shows the GMM density prior to updating themeansandFigure11.5(b)showstheGMMdensityafterupdatingthe meanvaluesµ . k The update of the mean parameters in (11.20) look fairly straightforward. However, note that the responsibilities r are a function of nk π ,µ ,Σ for all j = 1,...,K, such that the updates in (11.20) depend j j j on all parameters of the GMM, and a closed-form solution, which we obtained for linear regression in Section 9.2 or PCA in Chapter 10, cannot beobtained. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 356 DensityEstimationwithGaussianMixtureModels 11.2.3 Updating the Covariances Theorem 11.2 (Updates of the GMM Covariances). The update of the covarianceparametersΣ ,k = 1,...,K oftheGMMisgivenby k N 1 (cid:88) Σnew = r (x µ )(x µ )(cid:62), (11.30) k N nk n − k n − k k n=1 wherer andN aredefinedin(11.17)and(11.24),respectively. nk k Proof To prove Theorem 11.2, our approach is to compute the partial derivativesofthelog-likelihood withrespecttothecovariancesΣ ,set k L themto0,andsolveforΣ .Westartwithourgeneralapproach k ∂ L = (cid:88) N ∂logp(x n | θ) = (cid:88) N 1 ∂p(x n | θ) . (11.31) ∂Σ ∂Σ p(x θ) ∂Σ k n=1 k n=1 n | k