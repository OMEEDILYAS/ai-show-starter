11 Density Estimation with Gaussian Mixture Models In earlier chapters, we covered already two fundamental problems in machine learning: regression (Chapter 9) and dimensionality reduction (Chapter 10). In this chapter, we will have a look at a third pillar of machine learning: density estimation. On our journey, we introduce important concepts, such as the expectation maximization (EM) algorithm and alatentvariableperspectiveofdensityestimationwithmixturemodels. When we apply machine learning to data we often aim to represent data in some way. A straightforward way is to take the data points themselves as the representation of the data; see Figure 11.1 for an example. However, this approach may be unhelpful if the dataset is huge or if we are interested in representing characteristics of the data. In density estimation,werepresentthedatacompactlyusingadensityfromaparametric family, e.g., a Gaussian or Beta distribution. For example, we may be looking for the mean and variance of a dataset in order to represent the datacompactlyusingaGaussiandistribution.Themeanandvariancecan be found using tools we discussed in Section 8.3: maximum likelihood or maximumaposterioriestimation.Wecanthenusethemeanandvariance ofthisGaussiantorepresentthedistributionunderlyingthedata,i.e.,we think of the dataset to be a typical realization from this distribution if we weretosamplefromit. Figure11.1 Two-dimensional 4 datasetthatcannot bemeaningfully representedbya 2 Gaussian. 0 2 − 4 − 5 0 5 − x 1 x 2 348 ThismaterialwillbepublishedbyCambridgeUniversityPressasMathematicsforMachineLearningbyMarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng.Thispre-publicationversionis freetoviewanddownloadforpersonaluseonly.Notforre-distribution,re-saleoruseinderivativeworks.(cid:13)cbyM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2019.https://mml-book.com. 11.1 GaussianMixtureModel 349 Inpractice,theGaussian(orsimilarlyallotherdistributionsweencountered so far) have limited modeling capabilities. For example, a Gaussian approximationofthedensitythatgeneratedthedatainFigure11.1would be a poor approximation. In the following, we will look at a more expressive family of distributions, which we can use for density estimation: mixturemodels. mixturemodel Mixturemodelscanbeusedtodescribeadistributionp(x)byaconvex combinationofK simple(base)distributions K (cid:88) p(x) = π p (x) (11.1) k k k=1 K (cid:88) 0 (cid:54) π (cid:54) 1, π = 1, (11.2) k k k=1 where the components p are members of a family of basic distributions, k e.g., Gaussians, Bernoullis, or Gammas, and the π k are mixture weights. mixtureweight Mixture models are more expressive than the corresponding base distributionsbecausetheyallowformultimodaldatarepresentations,i.e.,they candescribedatasetswithmultiple“clusters”,suchastheexampleinFigure11.1. We will focus on Gaussian mixture models (GMMs), where the basic distributions are Gaussians. For a given dataset, we aim to maximize the likelihood of the model parameters to train the GMM. For this purpose, we will use results from Chapter 5, Chapter 6, and Section 7.2. However, unlike other applications we discussed earlier (linear regression or PCA), we will not find a closed-form maximum likelihood solution. Instead, we will arrive at a set of dependent simultaneous equations, which we can onlysolveiteratively. 11.1 Gaussian Mixture Model A Gaussian mixture model is a density model where we combine a finite Gaussianmixture (cid:0) (cid:1) numberofK Gaussiandistributions x µ , Σ sothat model N | k k K (cid:88) (cid:0) (cid:1) p(x θ) = π x µ , Σ (11.3) | k N | k k k=1 K (cid:88) 0 (cid:54) π (cid:54) 1, π = 1, (11.4) k k k=1 where we defined θ := µ ,Σ ,π : k = 1,...,K as the collection of { k k k } all parameters of the model. This convex combination of Gaussian distribution gives us significantly more flexibility for modeling complex densitiesthanasimpleGaussiandistribution(whichwerecoverfrom(11.3)for K =