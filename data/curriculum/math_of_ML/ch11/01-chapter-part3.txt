0, and solve for θ. However, unlike our previous examples for maximum likelihood estimation (e.g., when we discussed linear regression in Section 9.2), we cannot obtain a closed-form solution. However, we can exploitaniterativeschemetofindgoodmodelparametersθ ,whichwill ML turnouttobetheEMalgorithmforGMMs.Thekeyideaistoupdateone modelparameteratatimewhilekeepingtheothersfixed. Remark. If we were to consider a single Gaussian as the desired density, thesumoverk in(11.10)vanishes,andthelog canbeapplieddirectlyto theGaussiancomponent,suchthatweget log (cid:0) x µ, Σ (cid:1) = D log(2π) 1 logdet(Σ) 1(x µ)(cid:62)Σ−1(x µ). N | −2 − 2 − 2 − − (11.11) This simple form allows us to find closed-form maximum likelihood estimatesofµandΣ,asdiscussedinChapter8.In(11.10),wecannotmove (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 352 DensityEstimationwithGaussianMixtureModels thelogintothesumoverk sothatwecannotobtainasimpleclosed-form maximumlikelihoodsolution. ♦ Any local optimum of a function exhibits the property that its gradientwithrespecttotheparametersmustvanish(necessarycondition);see Chapter7.Inourcase,weobtainthefollowingnecessaryconditionswhen weoptimizethelog-likelihoodin(11.10)withrespecttotheGMMparametersµ ,Σ ,π : k k k ∂ L = 0(cid:62) (cid:88) N ∂logp(x n | θ) = 0(cid:62), (11.12) ∂µ ⇐⇒ ∂µ k n=1 k ∂ L = 0 (cid:88) N ∂logp(x n | θ) = 0, (11.13) ∂Σ ⇐⇒ ∂Σ k n=1 k ∂ L = 0 (cid:88) N ∂logp(x n | θ) = 0. (11.14) ∂π ⇐⇒ ∂π k n=1 k For all three necessary conditions, by applying the chain rule (see Section5.2.2),werequirepartialderivativesoftheform ∂logp(x θ) 1 ∂p(x θ) n | = n | , (11.15) ∂θ p(x θ) ∂θ n | whereθ = µ ,Σ ,π ,k = 1,...,K arethemodelparametersand { k k k } 1 1 = . (11.16) p(x n | θ) (cid:80)K j=1 π j N (cid:0) x n | µ j , Σ j (cid:1) In the following, we will compute the partial derivatives (11.12) through (11.14). But before we do this, we introduce a quantity that will play a centralroleintheremainderofthischapter:responsibilities. 11.2.1 Responsibilities Wedefinethequantity (cid:0) (cid:1) π x µ , Σ r := k N n | k k (11.17) nk (cid:80)K π (cid:0) x µ , Σ (cid:1) j=1 j N n | j j responsibility as the responsibility of the kth mixture component for the nth data point. The responsibility r of the kth mixture component for data point x is nk n proportionaltothelikelihood (cid:0) (cid:1) p(x π ,µ ,Σ ) = π x µ , Σ (11.18) n | k k k k N n | k k rnfollowsa of the mixture component given the data point. Therefore, mixture comBoltzmann/Gibbs ponents have a high responsibility for a data point when the data point distribution. could be a plausible sample from that mixture component. Note that r := [r ,...,r ](cid:62) RK is a (normalized) probability vector, i.e., n n1 nK ∈ Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 11.2 ParameterLearningviaMaximumLikelihood 353 (cid:80) r = 1 with r (cid:62) 0. This probability vector distributes probabilk nk nk ity mass among the K mixture components, and we can think of r as a n “soft assignment” of x n to the K mixture components. Therefore, the re- Theresponsibility sponsibility r nk from (11.17) represents the probability that x n has been r nkisthe generatedbythekthmixturecomponent. probabilitythatthe kthmixture component Example 11.2 (Responsibilities) generatedthenth datapoint. ForourexamplefromFigure11.3,wecomputetheresponsibilitiesr nk  1.0 0.0 0.0   1.0 0.0 0.0    0.057 0.943 0.0