kind of sampling, where samples of random variables depend on samples from the variable’s parents in the ancestralsampling graphicalmodel,iscalledancestralsampling. ♦ Generally, a probabilistic model is defined by the joint distribution of the data and the latent variables (see Section 8.4). With the prior p(z) definedin(11.59)and(11.60)andtheconditionalp(x z)from(11.58), | weobtainallK componentsofthisjointdistributionvia (cid:0) (cid:1) p(x,z = 1) = p(x z = 1)p(z = 1) = π x µ , Σ (11.61) k | k k k N | k k Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 11.4 Latent-VariablePerspective 365 fork = 1,...,K,sothat    (cid:0) (cid:1)  p(x,z = 1) π x µ , Σ 1 1 N | 1 1 . . p(x,z) =   . .   =   . .   , (11.62) (cid:0) (cid:1) p(x,z = 1) π x µ , Σ K K N | K K whichfullyspecifiestheprobabilisticmodel. 11.4.2 Likelihood To obtain the likelihood p(x θ) in a latent-variable model, we need to | marginalize out the latent variables (see Section 8.4.3). In our case, this can be done by summing out all latent variables from the joint p(x,z) in(11.62)sothat (cid:88) p(x θ) = p(x θ,z)p(z θ), θ := µ ,Σ ,π : k = 1,...,K . | | | { k k k } z (11.63) Wenowexplicitlyconditionontheparametersθoftheprobabilisticmodel, whichwepreviouslyomitted.In(11.63),wesumoverallK possibleone- (cid:80) hot encodings of z, which is denoted by . Since there is only a single z nonzero single entry in each z there are only K possible configurations/ settingsofz.Forexample,ifK = 3,thenz canhavetheconfigurations       1 0 0 0, 1, 0 . (11.64) 0 0 1 Summing over all possible configurations of z in (11.63) is equivalent to lookingatthenonzeroentryofthez-vectorandwriting (cid:88) p(x θ) = p(x θ,z)p(z θ) (11.65a) | | | z K (cid:88) = p(x θ,z = 1)p(z = 1 θ) (11.65b) k k | | k=1 sothatthedesiredmarginaldistributionisgivenas K p(x θ) (11 = .65b) (cid:88) p(x θ,z = 1)p(z = 1 θ) (11.66a) k k | | | k=1 K (cid:88) (cid:0) (cid:1) = π x µ , Σ , (11.66b) k N | k k k=1 whichweidentifyastheGMMmodelfrom(11.3).Givenadataset ,we X immediatelyobtainthelikelihood N N K p( θ) = (cid:89) p(x θ) (11 = .66b) (cid:89)(cid:88) π (cid:0) x µ , Σ (cid:1) , (11.67) X | n | k N n | k k n=1 n=1k=1 (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 366 DensityEstimationwithGaussianMixtureModels π Figure11.12 Graphicalmodelfor aGMMwithN data points. z n µ k Σ k x n k=1,...,K n=1,...,N which is exactly the GMM likelihood from (11.9). Therefore, the latentvariable model with latent indicators z is an equivalent way of thinking k aboutaGaussianmixturemodel. 11.4.3 Posterior Distribution Letushaveabrieflookattheposteriordistributiononthelatentvariable z.AccordingtoBayes’theorem,theposteriorofthekthcomponenthaving generateddatapointx p(z = 1)p(x z = 1) p(z = 1 x) = k | k , (11.68) k | p(x) where the marginal p(x) is given in (11.66b). This yields the posterior distributionforthekthindicatorvariablez k (cid:0) (cid:1) p(z = 1)p(x z = 1) π x µ , Σ p(z = 1 x) = k | k = k N | k k , k | (cid:80)K p(z = 1)p(x z = 1)