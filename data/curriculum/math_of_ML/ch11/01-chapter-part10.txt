EM iteration (Figure 11.9(b)). Figure 11.10(a) shows Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 11.4 Latent-VariablePerspective 363 6 Figure11.10 GMM fitand 4 responsibilities 2 whenEMconverges. 0 (a)GMMfitwhen EMconverges; 2 − (b)eachdatapoint 4 iscoloredaccording − tothe 6 − − 5 x 0 1 5 responsibilitiesof themixture components. 2x 6 4 2 0 2 − 4 − 6 − 5 0 5 − x1 (a)GMMfitafter62iterations. 2x (b)Datasetcoloredaccordingtotheresponsibilitiesofthemixturecomponents. the corresponding final GMM fit. Figure 11.10(b) visualizes the final responsibilitiesofthemixturecomponentsforthedatapoints.Thedatasetis coloredaccordingtotheresponsibilitiesofthemixturecomponentswhen EM converges. While a single mixture component is clearly responsible for the data on the left, the overlap of the two data clusters on the right could have been generated by two mixture components. It becomes clear that there are data points that cannot be uniquely assigned to a single component (either blue or yellow), such that the responsibilities of these twoclustersforthosepointsarearound0.5. 11.4 Latent-Variable Perspective WecanlookattheGMMfromtheperspectiveofadiscretelatent-variable model, i.e., where the latent variable z can attain only a finite set of values.ThisisincontrasttoPCA,wherethelatentvariableswerecontinuousvaluednumbersinRM. The advantages of the probabilistic perspective are that (i) it will justifysomeadhocdecisionswemadeintheprevioussections,(ii)itallows for a concrete interpretation of the responsibilities as posterior probabilities, and (iii) the iterative algorithm for updating the model parameters can be derived in a principled manner as the EM algorithm for maximum likelihoodparameterestimationinlatent-variablemodels. 11.4.1 Generative Process and Probabilistic Model ToderivetheprobabilisticmodelforGMMs,itisusefultothinkaboutthe generativeprocess,i.e.,theprocessthatallowsustogeneratedata,using aprobabilisticmodel. We assume a mixture model with K components and that a data point x can be generated by exactly one mixture component. We introduce a binaryindicatorvariablez 0,1 withtwostates(seeSection6.2)that k ∈ { } indicates whether the kth mixture component generated that data point (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 364 DensityEstimationwithGaussianMixtureModels sothat (cid:0) (cid:1) p(x z = 1) = x µ , Σ . (11.58) | k N | k k We define z := [z ,...,z ](cid:62) RK as a probability vector consisting of 1 K ∈ K 1many0sandexactlyone1.Forexample,forK = 3,avalidzwould − be z = [z ,z ,z ](cid:62) = [0,1,0](cid:62), which would select the second mixture 1 2 3 componentsincez = 1. 2 Remark. Sometimes this kind of probability distribution is called “multinoulli”, a generalization of the Bernoulli distribution to more than two values(Murphy,2012). ♦ one-hotencoding The properties of z imply that (cid:80)K k=1 z k = 1. Therefore, z is a one-hot 1-of-K encoding(also:1-of-K representation). representation Thus far, we assumed that the indicator variables z are known. Howk ever,inpractice,thisisnotthecase,andweplaceapriordistribution K (cid:88) p(z) = π = [π ,...,π ](cid:62), π = 1, (11.59) 1 K k k=1 onthelatentvariablez.Thenthekthentry π = p(z = 1) (11.60) k k of this probability vector describes the probability that the kth mixture Figure11.11 componentgenerateddatapointx. Graphicalmodelfor Remark(SamplingfromaGMM). Theconstructionofthislatent-variable aGMMwithasingle model (see the corresponding graphical model in Figure 11.11) lends itdatapoint. selftoaverysimplesamplingprocedure(generativeprocess)togenerate π data: z 1. Samplez(i) p(z). ∼ 2. Samplex(i) p(x z(i) = 1). ∼ | µ k In the first step, we select a mixture component i (via the one-hot encodΣ k x ing z) at random according to p(z) = π; in the second step we draw a k=1,...,K samplefromthecorrespondingmixturecomponent.Whenwediscardthe samples of the latent variable so that we are left with the x(i), we have valid samples from the GMM. This