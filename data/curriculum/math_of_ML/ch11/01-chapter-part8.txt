n=1 (cid:80)K j=1 π j N (cid:0) x n | µ j , Σ j (cid:1) π k (cid:124) (cid:123)(cid:122) (cid:125) =Nk andthepartialderivativewithrespecttotheLagrangemultiplierλas ∂L (cid:88) K = π 1. (11.45) ∂λ k − k=1 Setting both partial derivatives to 0 (necessary condition for optimum) yieldsthesystemofequations N π = k , (11.46) k − λ K (cid:88) 1 = π . (11.47) k k=1 Using(11.46)in(11.47)andsolvingforπ ,weobtain k (cid:88) K π = 1 (cid:88) K N k = 1 N = 1 λ = N . k ⇐⇒ − λ ⇐⇒ −λ ⇐⇒ − k=1 k=1 (11.48) Thisallowsustosubstitute N forλin(11.46)toobtain − N πnew = k , (11.49) k N whichgivesustheupdatefortheweightparametersπ andprovesTheok rem11.3. We can identify the mixture weight in (11.42) as the ratio of the total responsibility of the kth cluster and the number of data points. Since (cid:80) N = N , the number of data points can also be interpreted as the k k totalresponsibilityofallmixturecomponentstogether,suchthatπ isthe k relativeimportanceofthekthmixturecomponentforthedataset. Remark. Since N = (cid:80)N r , the update equation (11.42) for the mixk i=1 nk ture weights π also depends on all π ,µ ,Σ ,j = 1,...,K via the rek j j j sponsibilitiesr . nk ♦ (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 360 DensityEstimationwithGaussianMixtureModels Example 11.5 (Weight Parameter Updates) Figure11.7 Effect ofupdatingthe 0.35 mixtureweightsina 0.30 GMM.(a)GMM 0.25 beforeupdatingthe 0.20 mixtureweights; 0.15 (b)GMMafter 0.10 updatingthe 0.05 mixtureweights 0.00 whileretainingthe − 4 − 2 0 x 2 4 6 8 meansand variances.Notethe differentscalesof theverticalaxes. )x(p π π π 1 2 3 N N N ( ( ( x x x | | | µ µ µ 1 2 3 , , , σ σ σ 1 2 3 2 2 2 ) ) ) 0 0 . . 2 3 5 0 GMMdensity 0.20 0.15 0.10 0.05 0.00 4 2 0 2 4 6 8 − − x (a)GMMdensityandindividualcomponents priortoupdatingthemixtureweights. )x(p π π π 1 2 3 N N N ( ( ( x x x | | | µ µ µ 1 2 3 , , , σ σ σ 1 2 3 2 2 2 ) ) ) GMMdensity (b)GMMdensityandindividualcomponents afterupdatingthemixtureweights. In our running example from Figure 11.3, the mixture weights are updatedasfollows: π : 1 0.29 (11.50) 1 3 → π : 1 0.29 (11.51) 2 3 → π : 1 0.42 (11.52) 3 3 → Here we see that the third component gets more weight/importance, while the other components become slightly less important. Figure 11.7 illustrates the effect of updating the mixture weights. Figure 11.7(a) is identicaltoFigure11.6(b)andshowstheGMMdensityanditsindividual components prior to updating the mixture weights. Figure 11.7(b) shows theGMMdensityafterupdatingthemixtureweights. Overall, having updated the means, the variances, and the weights once, we obtain the GMM shown in Figure 11.7(b). Compared with the initializationshowninFigure11.3,wecanseethattheparameterupdates causedtheGMMdensitytoshiftsomeofitsmasstowardthedatapoints. After updating the means, variances, and weights once, the GMM fit in Figure 11.7(b) is already remarkably better than its initialization from Figure11.3.Thisisalsoevidencedbythelog-likelihoodvalues,whichincreasedfrom28.3(initialization)to14.4afteronecompleteupdatecycle. 11.3 EM Algorithm Unfortunately,theupdatesin(11.20),(11.30),and(11.42)donotconstitute a closed-form solution for the updates of the parameters µ ,Σ ,π k k k ofthemixturemodelbecausetheresponsibilitiesr dependonthosepank rametersinacomplexway.However,theresultssuggestasimpleiterative scheme for finding a solution