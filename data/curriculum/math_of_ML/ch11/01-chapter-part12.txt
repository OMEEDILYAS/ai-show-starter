(cid:80)K π (cid:0) x µ , Σ (cid:1) j=1 j | j j=1 j N | j j (11.69) which we identify as the responsibility of the kth mixture component for datapointx.NotethatweomittedtheexplicitconditioningontheGMM parametersπ ,µ ,Σ wherek = 1,...,K. k k k 11.4.4 Extension to a Full Dataset Thus far, we have only discussed the case where the dataset consists only of a single data point x. However, the concepts of the prior and posterior canbedirectlyextendedtothecaseofN datapoints := x ,...,x . 1 N X { } IntheprobabilisticinterpretationoftheGMM,everydatapointx posn sessesitsownlatentvariable z = [z ,...,z ](cid:62) RK. (11.70) n n1 nK ∈ Previously (when we only considered a single data point x), we omitted theindexn,butnowthisbecomesimportant. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 11.4 Latent-VariablePerspective 367 We share the same prior distribution π across all latent variables z . n The corresponding graphical model is shown in Figure 11.12, where we usetheplatenotation. The conditional distribution p(x ,...,x z ,...,z ) factorizes over 1 N 1 N | thedatapointsandisgivenas N (cid:89) p(x ,...,x z ,...,z ) = p(x z ). (11.71) 1 N 1 N n n | | n=1 To obtain the posterior distribution p(z = 1 x ), we follow the same nk n | reasoningasinSection11.4.3andapplyBayes’theoremtoobtain p(x z = 1)p(z = 1) p(z = 1 x ) = n | nk nk (11.72a) nk | n (cid:80)K p(x z = 1)p(z = 1) j=1 n | nj nj (cid:0) (cid:1) π x µ , Σ = k N n | k k = r . (11.72b) (cid:80)K π (cid:0) x µ , Σ (cid:1) nk j=1 j N n | j j This means that p(z = 1 x ) is the (posterior) probability that the kth k n | mixture component generated data point x and corresponds to the ren sponsibility r we introduced in (11.17). Now the responsibilities also nk have not only an intuitive but also a mathematically justified interpretationasposteriorprobabilities. 11.4.5 EM Algorithm Revisited TheEMalgorithmthatweintroducedasaniterativeschemeformaximum likelihood estimation can be derived in a principled way from the latentvariableperspective.Givenacurrentsettingθ(t) ofmodelparameters,the E-stepcalculatestheexpectedlog-likelihood Q(θ θ(t)) = E [logp(x,z θ)] (11.73a) | z|x,θ(t) | (cid:90) = logp(x,z θ)p(z x,θ(t))dz, (11.73b) | | wheretheexpectationoflogp(x,z θ)istakenwithrespecttotheposte- | riorp(z x,θ(t))ofthelatentvariables.TheM-stepselectsanupdatedset | ofmodelparametersθ(t+1) bymaximizing(11.73b). Although an EM iteration does increase the log-likelihood, there are no guarantees that EM converges to the maximum likelihood solution. It is possible that the EM algorithm converges to a local maximum of the log-likelihood. Different initializations of the parameters θ could be used in multiple EM runs to reduce the risk of ending up in a bad local optimum.Wedonotgointofurtherdetailshere,butrefertotheexcellent expositionsbyRogersandGirolami(2016)andBishop(2006). (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 368 DensityEstimationwithGaussianMixtureModels 11.5 Further Reading The GMM can be considered a generative model in the sense that it is straightforward to generate new data using ancestral sampling (Bishop, 2006). For given GMM parameters π ,µ ,Σ , k = 1,...,K, we sample k k k an index k from the probability vector [π ,...,π ](cid:62) and then sample a 1 K (cid:0) (cid:1) datapointx µ , Σ .IfwerepeatthisN times,weobtainadataset ∼ N k k thathasbeengeneratedbyaGMM.Figure11.1wasgeneratedusingthis procedure. Throughout this chapter, we