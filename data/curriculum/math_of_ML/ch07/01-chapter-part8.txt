− − wheretheright-handsidecanberearrangedto θ(f (x)+f (x))+(1 θ)(f (y)+f (y)), (7.37) 1 2 1 2 − completingtheproofthatthesumofconvexfunctionsisconvex. Combining the preceding two facts, we see that αf (x) + βf (x) is 1 2 convex for α,β (cid:62) 0. This closure property can be extended using a similar argument for nonnegative weighted sums of more than two convex functions. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 7.3 ConvexOptimization 239 Remark. The inequality in (7.30) is sometimes called Jensen’s inequality. Jensen’sinequality Infact,awholeclassofinequalitiesfortakingnonnegativeweightedsums ofconvexfunctionsareallcalledJensen’sinequality. ♦ Insummary,aconstrainedoptimizationproblemiscalledaconvexopti- convexoptimization mizationproblemif problem minf(x) x subjecttog (x) (cid:54) 0 forall i = 1,...,m (7.38) i h (x) = 0 forall j = 1,...,n, j whereallfunctionsf(x)andg (x)areconvexfunctions,andallh (x) = i j 0areconvexsets.Inthefollowing,wewilldescribetwoclassesofconvex optimizationproblemsthatarewidelyusedandwellunderstood. 7.3.1 Linear Programming Considerthespecialcasewhenalltheprecedingfunctionsarelinear,i.e., min c(cid:62)x (7.39) x∈Rd subjectto Ax (cid:54) b, whereA Rm×d andb Rm.Thisisknownasalinearprogram.Ithasd linearprogram ∈ ∈ variablesandmlinearconstraints.TheLagrangianisgivenby Linearprogramsare oneofthemost L(x,λ) = c(cid:62)x+λ(cid:62)(Ax b), (7.40) widelyused − approachesin where λ Rm is the vector of non-negative Lagrange multipliers. Rear- industry. ∈ rangingthetermscorrespondingtoxyields L(x,λ) = (c+A(cid:62)λ)(cid:62)x λ(cid:62)b. (7.41) − Taking the derivative of L(x,λ) with respect to x and setting it to zero givesus c+A(cid:62)λ = 0. (7.42) Therefore, the dual Lagrangian is D(λ) = λ(cid:62)b. Recall we would like − to maximize D(λ). In addition to the constraint due to the derivative of L(x,λ) being zero, we also have the fact that λ (cid:62) 0, resulting in the followingdualoptimizationproblem Itisconventionto minimizetheprimal max b(cid:62)λ (7.43) andmaximizethe λ∈Rm − dual. subjectto c+A(cid:62)λ = 0 λ (cid:62) 0. This is also a linear program, but with m variables. We have the choice of solving the primal (7.39) or the dual (7.43) program depending on (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 240 ContinuousOptimization whethermordislarger.Recallthatdisthenumberofvariablesandmis thenumberofconstraintsintheprimallinearprogram. Example 7.5 (Linear Program) Considerthelinearprogram (cid:20) (cid:21)(cid:62)(cid:20) (cid:21) 5 x min 1 x∈R2 − 3 x 2     2 2 33 (7.44)  2 4(cid:20) (cid:21)  8  subjectto   2 − 1   x 1 (cid:54)   5     − 0 1   x 2   1   − − 0 1 8 withtwovariables.ThisprogramisalsoshowninFigure7.9.Theobjective function is linear, resulting in linear contour lines. The constraint set in standardformistranslatedintothelegend.Theoptimalvaluemustliein theshaded(feasible)region,andisindicatedbythestar. Figure7.9 Illustrationofa 10 linearprogram.The unconstrained problem(indicated bythecontour 8 lines)hasa minimumonthe rightside.The 6 optimalvaluegiven theconstraintsare shownbythestar. 4 2 0 0 2 4 6 8 10 12 14 16 x1 2x 2x2≤ 33 − 2x1 4x2≥ 2x1− 8 x2≤ 2x1− 5 x2≥ 1 x2≤ 8 Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 7.3 ConvexOptimization 241 7.3.2 Quadratic Programming Considerthecaseofaconvexquadraticobjectivefunction,wheretheconstraintsareaffine,i.e., 1 min x(cid:62)Qx+c(cid:62)x (7.45) x∈Rd 2 subjectto Ax (cid:54) b, whereA Rm×d,b Rm,andc Rd.ThesquaresymmetricmatrixQ Rd×d is p ∈ ositive defi ∈ nite, and the ∈ refore the objective function is convex ∈ . Thisis knownasaquadraticprogram.Observe thatithasdvariablesand mlinearconstraints. Example 7.6 (Quadratic Program) Considerthequadraticprogram (cid:20) (cid:21)(cid:62)(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)(cid:62)(cid:20) (cid:21) 1 x 2 1 x 5 x min 1 1 + 1 (7.46) x∈R2 2 x 2 1 4 x 2 3 x 2     1 0 1 (cid:20) (cid:21) subjectto  − 1 0   x 1 (cid:54)   1  (7.47)  0 1 