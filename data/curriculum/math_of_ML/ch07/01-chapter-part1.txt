7 Continuous Optimization Since machine learning algorithms are implemented on a computer, the mathematicalformulationsareexpressedasnumericaloptimizationmethods. This chapter describes the basic numerical methods for training machine learning models. Training a machine learning model often boils down to finding a good set of parameters. The notion of “good” is determined by the objective function or the probabilistic model, which we will see examples of in the second part of this book. Given an objective function,findingthebestvalueisdoneusingoptimizationalgorithms. Sinceweconsider Thischaptercoverstwomainbranchesofcontinuousoptimization(Fig- dataandmodelsin RD,the ure 7.1): unconstrained and constrained optimization. We will assume in optimization this chapter that our objective function is differentiable (see Chapter 5), problemsweface hencewehaveaccesstoagradientateachlocationinthespacetohelpus arecontinuous find the optimum value. By convention, most objective functions in ma- optimization chine learning are intended to be minimized, that is, the best value is the problems,as opposedto minimum value. Intuitively finding the best value is like finding the valcombinatorial leysoftheobjectivefunction,andthegradientspointusuphill.Theideais optimization tomovedownhill(oppositetothegradient)andhopetofindthedeepest problemsfor point. For unconstrained optimization, this is the only concept we need, discretevariables. but there are several design choices, which we discuss in Section 7.1. For constrained optimization, we need to introduce other concepts to manage the constraints (Section 7.2). We will also introduce a special class ofproblems(convexoptimizationproblemsinSection7.3)wherewecan makestatementsaboutreachingtheglobaloptimum. ConsiderthefunctioninFigure7.2.Thefunctionhasaglobalminimum globalminimum around x = 4.5, with a function value of approximately 47. Since − − the function is “smooth,” the gradients can be used to help find the minimum by indicating whether we should take a step to the right or left. Thisassumesthatweareinthecorrectbowl,asthereexistsanotherlocal localminimum minimum around x = 0.7. Recall that we can solve for all the stationary pointsofafunctionbycalculatingitsderivativeandsettingittozero.For Stationarypoints aretherealrootsof (cid:96)(x) = x4+7x3+5x2 17x+3, (7.1) thederivative,that − is,pointsthathave weobtainthecorrespondinggradientas zerogradient. d(cid:96)(x) = 4x3+21x2+10x 17. (7.2) dx − 225 ThismaterialwillbepublishedbyCambridgeUniversityPressasMathematicsforMachineLearningbyMarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng.Thispre-publicationversionis freetoviewanddownloadforpersonaluseonly.Notforre-distribution,re-saleoruseinderivativeworks.(cid:13)cbyM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2019.https://mml-book.com. 226 ContinuousOptimization Figure7.1 Amind Continuous mapoftheconcepts optimization Stepsize relatedto optimization,as presentedinthis chapter.Thereare Unconstrained optimization Gradientdescent Momentum twomainideas: gradientdescent andconvex optimization. Stochastic gradient Constrained Chapter10 descent optimization Dimensionreduc. Lagrange Chapter11 multipliers Densityestimation Convex Convexoptimization Linear &duality programming Convexconjugate Quadratic Chapter12 programming Classification Sincethisisacubicequation,ithasingeneralthreesolutionswhensetto zero. In the example, two of them are minimums and one is a maximum (around x = 1.4). To check whether a stationary point is a minimum − or maximum, we need to take the derivative a second time and check whether the second derivative is positive or negative at the stationary point.Inourcase,thesecondderivativeis d2(cid:96)(x) = 12x2+42x+10. (7.3) dx2 By substituting our visually estimated values of x = 4.5, 1.4,0.7, we − −(cid:16) (cid:17) willobservethatasexpectedthemiddlepointisamaximum d2(cid:96)(x) < 0 dx2 andtheothertwostationarypointsareminimums. Note that we have avoided analytically solving for values of x in the previous discussion, although for low-order polynomials such as the preceding we could do so. In general, we are unable to find analytic solutions,andhenceweneedtostartatsomevalue,sayx = 6,andfollow 0 − the negative gradient. The negative gradient indicates that we should go Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 7.1 OptimizationUsingGradientDescent 227 Figure7.2 Example objectivefunction. 60 Negativegradients areindicatedby arrows,andthe 40 globalminimumis indicatedbythe 20 dashedblueline. 0 20 − 40 − 60 − 6 5 4 3 2 1 0 1 2 − − − − − − Valueofparameter evitcejbO x4+7x3+5x2 17x+3 − right, but not how far (this is called the step-size).