and Borwein and Lewis(2006).Legendre–Fencheltransformsarealsocoveredintheaforementioned books on convex analysis, but a more beginner-friendly presentation is available at Zia et al. (2009). The role of Legendre–Fenchel transforms in the analysis of convex optimization algorithms is surveyed inPolyak(2016). Exercises 7.1 Considertheunivariatefunction f(x)=x3+6x2−3x−5. Find its stationary points and indicate whether they are maximum, minimum,orsaddlepoints. 7.2 Considertheupdateequationforstochasticgradientdescent(Equation(7.15)). Writedowntheupdatewhenweuseamini-batchsizeofone. 7.3 Considerwhetherthefollowingstatementsaretrueorfalse: a. Theintersectionofanytwoconvexsetsisconvex. b. Theunionofanytwoconvexsetsisconvex. c. ThedifferenceofaconvexsetAfromanotherconvexsetB isconvex. 7.4 Considerwhetherthefollowingstatementsaretrueorfalse: a. Thesumofanytwoconvexfunctionsisconvex. b. Thedifferenceofanytwoconvexfunctionsisconvex. c. Theproductofanytwoconvexfunctionsisconvex. d. Themaximumofanytwoconvexfunctionsisconvex. 7.5 Expressthefollowingoptimizationproblemasastandardlinearprogramin matrixnotation max p(cid:62)x+ξ x∈R2,ξ∈R subjecttotheconstraintsthatξ(cid:62)0,x (cid:54)0andx (cid:54)3. 0 1 7.6 ConsiderthelinearprogramillustratedinFigure7.9, (cid:20) (cid:21)(cid:62)(cid:20) (cid:21) 5 x min − 1 x∈R2 3 x 2     2 2 33  2 −4(cid:20) (cid:21)  8  subjectto  −2 1   x 1 (cid:54)  5     0 −1   x 2  −1   0 1 8 DerivetheduallinearprogramusingLagrangeduality. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 248 ContinuousOptimization 7.7 ConsiderthequadraticprogramillustratedinFigure7.4, (cid:20) (cid:21)(cid:62)(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)(cid:62)(cid:20) (cid:21) min 1 x 1 2 1 x 1 + 5 x 1 x∈R22 x 2 1 4 x 2 3 x 2     1 0 1 (cid:20) (cid:21) subjectto   −1 0   x 1 (cid:54)  1   0 1  x 2 1 0 −1 1 DerivethedualquadraticprogramusingLagrangeduality. 7.8 Considerthefollowingconvexoptimizationproblem 1 min w(cid:62)w w∈RD 2 subjectto w(cid:62)x(cid:62)1. DerivetheLagrangiandualbyintroducingtheLagrangemultiplierλ. 7.9 Considerthenegativeentropyofx∈RD, D (cid:88) f(x)= x logx . d d d=1 Derive the convex conjugate function f∗(s), by assuming the standard dot product. Hint:Takethegradientofanappropriatefunctionandsetthegradienttozero. 7.10 Considerthefunction 1 f(x)= x(cid:62)Ax+b(cid:62)x+c, 2 whereAisstrictlypositivedefinite,whichmeansthatitisinvertible.Derive theconvexconjugateoff(x). Hint:Takethegradientofanappropriatefunctionandsetthegradienttozero. 7.11 The hinge loss (which is the loss used by the support vector machine) is givenby L(α)=max{0,1−α}, If we are interested in applying gradient methods such as L-BFGS, and do not want to resort to subgradient methods, we need to smooth the kink in thehingeloss.ComputetheconvexconjugateofthehingelossL∗(β)where β isthedualvariable.Adda(cid:96) proximalterm,andcomputetheconjugate 2 oftheresultingfunction γ L∗(β)+ β2, 2 whereγ isagivenhyperparameter. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. Part II Central Machine Learning Problems 249 ThismaterialwillbepublishedbyCambridgeUniversityPressasMathematicsforMachineLearningbyMarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng.Thispre-publicationversionis freetoviewanddownloadforpersonaluseonly.Notforre-distribution,re-saleoruseinderivativeworks.(cid:13)cbyM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2019.https://mml-book.com.