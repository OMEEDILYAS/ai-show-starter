walls of the valley and approaches the optimum in small steps. The proposed tweak to improve convergence is Goh(2017)wrote togivegradientdescentsomememory. anintuitiveblog Gradientdescentwithmomentum(Rumelhartetal.,1986)isamethod postongradient that introduces an additional term to remember what happened in the descentwith previous iteration. This memory dampens oscillations and smoothes out momentum. the gradient updates. Continuing the ball analogy, the momentum term emulates the phenomenon of a heavy ball that is reluctant to change directions.Theideaistohaveagradientupdatewithmemorytoimplement Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 7.1 OptimizationUsingGradientDescent 231 a moving average. The momentum-based method remembers the update ∆x ateachiterationianddeterminesthenextupdateasalinearcombii nationofthecurrentandpreviousgradients x = x γ (( f)(x ))(cid:62)+α∆x (7.11) i+1 i i i i − ∇ ∆x = x x = α∆x γ (( f)(x ))(cid:62), (7.12) i i i−1 i−1 i−1 i−1 − − ∇ where α [0,1]. Sometimes we will only know the gradient approxi- ∈ mately. In such cases, the momentum term is useful since it averages out different noisy estimates of the gradient. One particularly useful way to obtain an approximate gradient is by using a stochastic approximation, whichwediscussnext. 7.1.3 Stochastic Gradient Descent Computingthegradientcanbeverytimeconsuming.However,oftenitis possible to find a “cheap” approximation of the gradient. Approximating thegradientisstillusefulaslongasitpointsinroughlythesamedirection asthetruegradient. stochasticgradient Stochastic gradient descent (often shortened as SGD) is a stochastic ap- descent proximation of the gradient descent method for minimizing an objective function that is written as a sum of differentiable functions. The word stochastic here refers to the fact that we acknowledge that we do not know the gradient precisely, but instead only know a noisy approximation to it. By constraining the probability distribution of the approximate gradients,wecanstilltheoreticallyguaranteethatSGDwillconverge. Inmachinelearning,givenn = 1,...,N datapoints,weoftenconsider objective functions that are the sum of the losses L incurred by each n examplen.Inmathematicalnotation,wehavetheform N (cid:88) L(θ) = L (θ), (7.13) n n=1 whereθisthevectorofparametersofinterest,i.e.,wewanttofindθthat minimizesL.Anexamplefromregression(Chapter9)isthenegativeloglikelihood, which is expressed as a sum over log-likelihoods of individual examplessothat N (cid:88) L(θ) = logp(y x ,θ), (7.14) n n − | n=1 wherex RD arethetraininginputs,y arethetrainingtargets,andθ n n ∈ aretheparametersoftheregressionmodel. Standard gradient descent, as introduced previously, is a “batch” optimizationmethod,i.e.,optimizationisperformedusingthefulltrainingset (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 232 ContinuousOptimization byupdatingthevectorofparametersaccordingto N (cid:88) θ = θ γ ( L(θ ))(cid:62) = θ γ ( L (θ ))(cid:62) (7.15) i+1 i i i i i n i − ∇ − ∇ n=1 forasuitablestep-sizeparameterγ .Evaluatingthesumgradientmayrei quire expensive evaluations of the gradients from all individual functions L . When the training set is enormous and/or no simple formulas exist, n evaluatingthesumsofgradientsbecomesveryexpensive. Considertheterm (cid:80)N ( L (θ ))in(7.15),wecanreducetheamount n=1 ∇ n i of computation by taking a sum over a smaller set of L . In contrast to n batchgradientdescent,whichusesallL forn = 1,...,N,werandomly n choose a subset of L for mini-batch gradient descent. In the extreme n case, we randomly select only a single L to estimate the gradient. The n key insight about why taking a subset of data is sensible is to realize that for gradient descent to converge, we only require that the gradient is an unbiased estimate of the true gradient. In fact the term (cid:80)N ( L (θ )) n=1 ∇ n i in(7.15)isanempiricalestimateoftheexpectedvalue(Section6.4.1)of the gradient. Therefore, any other unbiased empirical