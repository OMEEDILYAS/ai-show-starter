of c for which 0 0 − y = sx+cintersectswiththegraphoff istherefore inf sx +f(x ). (7.55) 0 0 x0 − The preceding convex conjugate is by convention defined to be the negative of this. The reasoning in this paragraph did not rely on the fact that wechoseaone-dimensionalconvexanddifferentiablefunction,andholds forf : RD R,whicharenonconvexandnon-differentiable. → Theclassical Remark. Convexdifferentiablefunctionssuchastheexamplef(x) = x2is Legendretransform anicespecialcase,wherethereisnoneedforthesupremum,andthereis isdefinedonconvex differentiable a one-to-one correspondence between a function and its Legendre transfunctionsinRD. form. Let us derive this from first principles. For a convex differentiable function,weknowthatatx thetangenttouchesf(x )sothat 0 0 f(x ) = sx +c. (7.56) 0 0 Recall that we want to describe the convex function f(x) in terms of its gradient f(x), and that s = f(x ). We rearrange to get an expresx x 0 ∇ ∇ sionfor ctoobtain − c = sx f(x ). (7.57) 0 0 − − Note that c changes with x and therefore with s, which is why we can 0 − thinkofitasafunctionofs,whichwecall f∗(s) := sx f(x ). (7.58) 0 0 − Comparing(7.58)withDefinition7.4,weseethat(7.58)isaspecialcase (withoutthesupremum). ♦ The conjugate function has nice properties; for example, for convex functions,applyingtheLegendretransformagaingetsusbacktotheoriginalfunction.Inthesamewaythattheslopeoff(x)iss,theslopeoff∗(s) (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 244 ContinuousOptimization isx.Thefollowingtwoexamplesshowcommonusesofconvexconjugates inmachinelearning. Example 7.7 (Convex Conjugates) To illustrate the application of convex conjugates, consider the quadratic function λ f(y) = y(cid:62)K−1y (7.59) 2 based on a positive definite matrix K Rn×n. We denote the primal variabletobey Rn andthedualvariab ∈ letobeα Rn. ∈ ∈ ApplyingDefinition7.4,weobtainthefunction λ f∗(α) = sup y,α y(cid:62)K−1y. (7.60) y∈Rn (cid:104) (cid:105)− 2 Since the function is differentiable, we can find the maximum by taking thederivativeandwithrespecttoy settingittozero. ∂ (cid:2) y,α λy(cid:62)K−1y (cid:3) (cid:104) (cid:105)− 2 = (α λK−1y)(cid:62) (7.61) ∂y − and hence when the gradient is zero we have y = 1Kα. Substituting λ into(7.60)yields 1 λ (cid:18) 1 (cid:19)(cid:62) (cid:18) 1 (cid:19) 1 f∗(α) = α(cid:62)Kα Kα K−1 Kα = α(cid:62)Kα. λ − 2 λ λ 2λ (7.62) Example 7.8 Inmachinelearning,weoftenusesumsoffunctions;forexample,theobjectivefunctionofthetrainingsetincludesasumofthelossesforeachexampleinthetrainingset.Inthefollowing,wederivetheconvexconjugate of a sum of losses (cid:96)(t), where (cid:96) : R R. This also illustrates the application of the convex conjugate to the → vector case. Let (t) = (cid:80)n (cid:96) (t ). L i=1 i i Then, n (cid:88) ∗(z) = sup z,t (cid:96) (t ) (7.63a) i i L t∈Rn (cid:104) (cid:105)− i=1 n (cid:88) = sup z t (cid:96) (t ) definitionof dotproduct (7.63b) i i i i t∈Rn − i=1 n (cid:88) = sup z t (cid:96) (t ) (7.63c) i i i i t∈Rn − i=1 Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 7.3 ConvexOptimization 245 n (cid:88) = (cid:96)∗(z ). definitionof conjugate (7.63d) i i i=1 RecallthatinSection7.2wederivedadualoptimizationproblemusing Lagrange multipliers. Furthermore, for convex optimization problems we have strong duality, that is the solutions of the primal and dual problem match. The Legendre–Fenchel transform described here also can be used to derive a dual optimization problem. Furthermore, when the function is convex and differentiable, the supremum is unique. To further investigate the relation between these two approaches, let us consider a linear equalityconstrainedconvexoptimizationproblem. Example 7.9 Letf(y)andg(x)beconvexfunctions,andAarealmatrixofappropriate dimensionssuchthatAx = y.Then minf(Ax)+g(x) = min f(y)+g(x). (7.64) x Ax=y ByintroducingtheLagrangemultiplierufortheconstraintsAx = y, min f(y)+g(x) = minmaxf(y)+g(x)+(Ax