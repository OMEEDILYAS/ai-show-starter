y)(cid:62)u (7.65a) Ax=y x,y u − = maxminf(y)+g(x)+(Ax y)(cid:62)u, (7.65b) u x,y − where the last step of swapping max and min is due to the fact that f(y) and g(x) are convex functions. By splitting up the dot product term and collectingxandy, maxminf(y)+g(x)+(Ax y)(cid:62)u (7.66a) u x,y − (cid:20) (cid:21) (cid:104) (cid:105) = max min y(cid:62)u+f(y) + min(Ax)(cid:62)u+g(x) (7.66b) u y − x (cid:20) (cid:21) (cid:104) (cid:105) = max min y(cid:62)u+f(y) + minx(cid:62)A(cid:62)u+g(x) (7.66c) u y − x Recall the convex conjugate (Definition 7.4) and the fact that dot prod- Forgeneralinner uctsaresymmetric, products,A(cid:62)is replacedbythe (cid:20) (cid:21) (cid:104) (cid:105) adjointA∗. max min y(cid:62)u+f(y) + minx(cid:62)A(cid:62)u+g(x) (7.67a) u y − x = max f∗(u) g∗( A(cid:62)u). (7.67b) u − − − Therefore,wehaveshownthat minf(Ax)+g(x) = max f∗(u) g∗( A(cid:62)u). (7.68) x u − − − (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 246 ContinuousOptimization The Legendre–Fenchel conjugate turns out to be quite useful for machine learning problems that can be expressed as convex optimization problems.Inparticular,forconvexlossfunctionsthatapplyindependently to each example, the conjugate loss is a convenient way to derive a dual problem. 7.4 Further Reading Continuous optimization is an active area of research, and we do not try toprovideacomprehensiveaccountofrecentadvances. From a gradient descent perspective, there are two major weaknesses which each have their own set of literature. The first challenge is the fact that gradient descent is a first-order algorithm, and does not use information about the curvature of the surface. When there are long valleys, the gradient points perpendicularly to the direction of interest. The idea of momentum can be generalized to a general class of acceleration methods (Nesterov,2018). Conjugategradient methodsavoid theissues faced bygradientdescentbytakingpreviousdirectionsintoaccount(Shewchuk, 1994).Second-ordermethodssuchasNewtonmethodsusetheHessianto provide information about the curvature. Many of the choices for choosingstep-sizesandideaslikemomentumarisebyconsideringthecurvature of the objective function (Goh, 2017; Bottou et al., 2018). Quasi-Newton methodssuchasL-BFGStrytousecheapercomputationalmethodstoapproximate the Hessian (Nocedal and Wright, 2006). Recently there has been interest in other metrics for computing descent directions, resulting in approaches such as mirror descent (Beck and Teboulle, 2003) and naturalgradient(Toussaint,2012). The second challenge is to handle non-differentiable functions. Gradient methods are not well defined when there are kinks in the function. In these cases, subgradient methods can be used (Shor, 1985). For further information and algorithms for optimizing non-differentiable functions, we refer to the book by Bertsekas (1999). There is a vast amount of literature on different approaches for numerically solving continuous optimizationproblems,includingalgorithmsforconstrainedoptimization problems. Good starting points to appreciate this literature are the books byLuenberger(1969)andBonnansetal.(2006).ArecentsurveyofconHugoGon¸calves’ tinuousoptimizationisprovidedbyBubeck(2015). blogisalsoagood Modern applications of machine learning often mean that the size of resourceforan datasets prohibit the use of batch gradient descent, and hence stochastic easierintroduction gradientdescentisthecurrentworkhorseoflarge-scalemachinelearning toLegendre–Fenchel transforms: methods. Recent surveys of the literature include Hazan (2015) and Bothttps://tinyurl. touetal.(2018). com/ydaal7hj For duality and convex optimization, the book by Boyd and Vandenberghe (2004) includes lectures and slides online. A more mathematical treatment is provided by Bertsekas (2009), and recent book by one of Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. Exercises 247 the key researchers in the area of optimization is Nesterov (2018). Convexoptimizationisbaseduponconvexanalysis,andthereaderinterested in more foundational results about convex functions is referred to Rockafellar (1970), Hiriart-Urruty and Lemar´echal (2001),