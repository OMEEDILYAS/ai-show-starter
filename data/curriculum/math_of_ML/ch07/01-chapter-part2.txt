Furthermore, if we Accordingtothe had started at the right side (e.g., x = 0) the negative gradient would Abel–Ruffini 0 theorem,thereisin haveledustothewrongminimum.Figure7.2illustratesthefactthatfor generalnoalgebraic x > 1,thenegativegradientpointstowardtheminimumontherightof − solutionfor thefigure,whichhasalargerobjectivevalue. polynomialsof In Section 7.3, we will learn about a class of functions, called convex degree5ormore (Abel,1826). functions,thatdonotexhibitthistrickydependencyonthestartingpoint of the optimization algorithm. For convex functions, all local minimums are global minimum. It turns out that many machine learning objective Forconvexfunctions functions are designed such that they are convex, and we will see an ex- alllocalminimaare globalminimum. ampleinChapter12. Thediscussioninthischaptersofarwasaboutaone-dimensionalfunction, where we are able to visualize the ideas of gradients, descent directions,andoptimalvalues.Intherestofthischapterwedevelopthesame ideas in high dimensions. Unfortunately, we can only visualize the concepts in one dimension, but some concepts do not generalize directly to higherdimensions,thereforesomecareneedstobetakenwhenreading. 7.1 Optimization Using Gradient Descent Wenowconsidertheproblemofsolvingfortheminimumofareal-valued function minf(x), (7.4) x (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 228 ContinuousOptimization where f : Rd R is an objective function that captures the machine → learningproblemathand.Weassumethatourfunctionf isdifferentiable, andweareunabletoanalyticallyfindasolutioninclosedform. Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point. Weusethe Recall from Section 5.1 that the gradient points in the direction of the conventionofrow steepest ascent. Another useful intuition is to consider the set of lines vectorsfor wherethefunctionisatacertainvalue(f(x) = cforsomevaluec R), gradients. ∈ which are known as the contour lines. The gradient points in a direction thatisorthogonaltothecontourlinesofthefunctionwewishtooptimize. Let us consider multivariate functions. Imagine a surface (described by the function f(x)) with a ball starting at a particular location x . When 0 the ball is released, it will move downhill in the direction of steepest descent.Gradientdescentexploitsthefactthatf(x )decreasesfastestifone 0 movesfromx inthedirectionofthenegativegradient (( f)(x ))(cid:62) of 0 0 − ∇ f at x . We assume in this book that the functions are differentiable, and 0 referthereadertomoregeneralsettingsinSection7.4.Then,if x = x γ(( f)(x ))(cid:62) (7.5) 1 0 0 − ∇ for a small step-size γ (cid:62) 0, then f(x ) (cid:54) f(x ). Note that we use the 1 0 transpose for the gradient since otherwise the dimensions will not work out. This observation allows us to define a simple gradient descent algorithm: If we want to find a local optimum f(x ) of a function f : Rn ∗ R, x f(x),westartwithaninitialguessx oftheparameterswewi → sh 0 (cid:55)→ tooptimizeandtheniterateaccordingto x = x γ (( f)(x ))(cid:62). (7.6) i+1 i i i − ∇ For suitable step-size γ , the sequence f(x ) (cid:62) f(x ) (cid:62) ... converges to i 0 1 alocalminimum. Example 7.1 Consideraquadraticfunctionintwodimensions (cid:18)(cid:20) (cid:21)(cid:19) (cid:20) (cid:21)(cid:62)(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)(cid:62)(cid:20) (cid:21) x 1 x 2 1 x 5 x f 1 = 1 1 1 (7.7) x 2 2 x 2 1 20 x 2 − 3 x 2 withgradient (cid:18)(cid:20) (cid:21)(cid:19) (cid:20) (cid:21)(cid:62)(cid:20) (cid:21) (cid:20) (cid:21)(cid:62) x x 2 1 5 f 1 = 1 . (7.8) ∇ x 2 x 2 1 20 − 3 Starting at the initial location x = [ 3, 1](cid:62), we iteratively apply (7.6) 0