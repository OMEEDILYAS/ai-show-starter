estimate of the expectedvalue,forexampleusinganysubsampleofthedata,wouldsuffice forconvergenceofgradientdescent. Remark. Whenthelearningratedecreasesatanappropriaterate,andsubject to relatively mild assumptions, stochastic gradient descent converges almostsurelytolocalminimum(Bottou,1998). ♦ Why should oneconsider using an approximate gradient?A major reason is practical implementation constraints, such as the size of central processing unit (CPU)/graphics processing unit (GPU) memory or limits oncomputationaltime.Wecanthinkofthesizeofthesubsetusedtoestimatethegradientinthesamewaythatwethoughtofthesizeofasample when estimating empirical means (Section 6.4.1). Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in theparameterupdate.Furthermore,largemini-batchestakeadvantageof highly optimized matrix operations in vectorized implementations of the cost and gradient. The reduction in variance leads to more stable convergence,buteachgradientcalculationwillbemoreexpensive. In contrast, small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our gradient estimate will allow us to get out of some bad local optima, which we may otherwise get stuck in. In machine learning, optimization methods are used for training by minimizing an objective function on the training data, but the overall goal is to improve generalization performance (Chapter 8). Since the goal in machinelearningdoesnotnecessarilyneedapreciseestimateoftheminimum of the objective function, approximate gradients using mini-batch approaches have been widely used. Stochastic gradient descent is very effective in large-scale machine learning problems (Bottou et al., 2018), Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 7.2 ConstrainedOptimizationandLagrangeMultipliers 233 3 Figure7.4 Illustrationof constrained optimization.The 2 unconstrained problem(indicated bythecontour 1 lines)hasa minimumonthe rightside(indicated 0 bythecircle).The boxconstraints (−1(cid:54)x(cid:54)1and −1(cid:54)y(cid:54)1)require 1 − thattheoptimal solutioniswithin thebox,resultingin 2 anoptimalvalue − indicatedbythe star. 3 − 3 2 1 0 1 2 3 − − − x1 2x such as training deep neural networks on millions ofimages (Dean et al., 2012),topicmodels(Hoffmanetal.,2013),reinforcementlearning(Mnih etal.,2015),ortrainingoflarge-scaleGaussianprocessmodels(Hensman etal.,2013;Galetal.,2014). 7.2 Constrained Optimization and Lagrange Multipliers Intheprevioussection,weconsideredtheproblemofsolvingfortheminimumofafunction minf(x), (7.16) x wheref : RD R. → In this section, we have additional constraints. That is, for real-valued functions g : RD R for i = 1,...,m, we consider the constrained i → optimizationproblem(seeFigure7.4foranillustration) min f(x) (7.17) x subjectto g (x) (cid:54) 0 forall i = 1,...,m. i It is worth pointing out that the functions f and g could be non-convex i ingeneral,andwewillconsidertheconvexcaseinthenextsection. One obvious, but not very practical, way of converting the constrained problem(7.17)intoanunconstrainedoneistouseanindicatorfunction m (cid:88) J(x) = f(x)+ 1(g (x)), (7.18) i i=1 (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 234 ContinuousOptimization where1(z)isaninfinitestepfunction (cid:40) 0 ifz (cid:54) 0 1(z) = . (7.19) otherwise ∞ This gives infinite penalty if the constraint is not satisfied, and hence would provide the same solution. However, this infinite step function is equally difficult to optimize. We can overcome this difficulty by introducLagrangemultiplier ingLagrangemultipliers.TheideaofLagrangemultipliersistoreplacethe stepfunctionwithalinearfunction. Lagrangian We associate to problem (7.17) the Lagrangian by introducing the Lagrange multipliers λ (cid:62) 0 corresponding to each inequality constraint rei spectively(BoydandVandenberghe,2004,chapter4)sothat m (cid:88) L(x,λ) = f(x)+ λ g (x) (7.20a) i i i=1 = f(x)+λ(cid:62)g(x), (7.20b) where in the last line we have concatenated all constraints g (x) into a i vectorg(x),andalltheLagrangemultipliersintoavectorλ Rm. ∈ We now introduce the idea of Lagrangian duality. In general, duality in optimization is the idea of converting an optimization problem in one set of variables x (called the primal variables), into another optimization problem in a different set of variables λ (called the dual variables). We introduce two