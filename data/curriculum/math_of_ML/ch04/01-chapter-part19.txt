a principled manneropensupmachinelearningapplicationsrangingfromdimensionalityreductionandtopicmodelingtodatacompressionandclustering. 4.6 Matrix Approximation We considered the SVD as a way to factorize A = UΣV(cid:62) Rm×n into the product of three matrices, where U Rm×m and V ∈Rn×n are or- ∈ ∈ thogonalandΣcontainsthesingularvaluesonitsmaindiagonal.Instead of doing the full SVD factorization, we will now investigate how the SVD allowsustorepresentamatrixAasasumofsimpler(low-rank)matrices A , which lends itself to a matrix approximation scheme that is cheaper i tocomputethanthefullSVD. Weconstructarank-1matrixA Rm×n as i ∈ A := u v(cid:62), (4.90) i i i whichisformedbytheouterproductoftheithorthogonalcolumnvector (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 130 MatrixDecompositions Figure4.11 Image processingwiththe SVD.(a)The originalgrayscale imageisa 1,432×1,910 matrixofvalues between0(black) (a)OriginalimageA. (b)A1, σ1≈228,052. (c)A2, σ2≈40,647. and1(white). (b)–(f)Rank-1 matrices A1,...,A5and theircorresponding singularvalues σ1,...,σ5.The grid-likestructureof eachrank-1matrix (d)A3, σ3≈26,125. (e)A4, σ4≈20,232. (f)A5, σ5≈15,436. isimposedbythe outer-productofthe leftand right-singular of U and V. Figure 4.11 shows an image of Stonehenge, which can be vectors. represented by a matrix A R1432×1910, and some outer products A , as i ∈ definedin(4.90). AmatrixA Rm×nofrankrcanbewrittenasasumofrank-1matrices ∈ A sothat i r r (cid:88) (cid:88) A = σ u v(cid:62) = σ A , (4.91) i i i i i i=1 i=1 where the outer-product matrices A are weighted by the ith singular i value σ . We can see why (4.91) holds: The diagonal structure of the i singular value matrix Σ multiplies only matching left- and right-singular vectors u v(cid:62) and scales them by the corresponding singular value σ . All i i i termsΣ u v(cid:62) vanishfori = j becauseΣisadiagonalmatrix.Anyterms ij i j (cid:54) i > r vanishbecausethecorrespondingsingularvaluesare0. In (4.90), we introduced rank-1 matrices A . We summed up the r ini dividual rank-1 matrices to obtain a rank-r matrix A; see (4.91). If the sum does not run over all matrices A , i = 1,...,r, but only up to an i rank-k intermediatevaluek < r,weobtainarank-k approximation approximation k k (cid:88) (cid:88) A(cid:98)(k) := σ u v(cid:62) = σ A (4.92) i i i i i i=1 i=1 of A with rk(A(cid:98)(k)) = k. Figure 4.12 shows low-rank approximations A(cid:98)(k) of an original image A of Stonehenge. The shape of the rocks becomes increasingly visible and clearly recognizable in the rank-5 approximation. While the original image requires 1,432 1,910 = 2,735,120 · numbers, the rank-5 approximation requires us only to store the five singularvaluesandthefiveleft-andright-singularvectors(1,432and1,910Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 4.6 MatrixApproximation 131 Figure4.12 Image reconstructionwith theSVD.(a) Originalimage. (b)–(f)Image reconstructionusing thelow-rank (a)OriginalimageA. (b)Rank-1approximationA(cid:98)(1).(c)Rank-2approximationA(cid:98)(2). approximationof theSVD,wherethe rank-k approximationis givenbyA(cid:98)(k)= (cid:80)k i=1 σiAi. (d)Rank-3approximationA(cid:98)(3).(e)Rank-4approximationA(cid:98)(4).(f)Rank-5approximationA(cid:98)(5). dimensionaleach)foratotalof5 (1,432+1,910+1) = 16,715numbers · –justabove0.6%oftheoriginal. Tomeasurethedifference(error)betweenAanditsrank-kapproximationA(cid:98)(k),weneedthenotionofanorm.InSection3.1,wealreadyused norms on vectors that measure the length of a vector. By analogy we can alsodefinenormsonmatrices. Definition4.23(SpectralNormofaMatrix). Forx Rn 0 ,thespectral spectralnorm normofamatrixA Rm×n isdefinedas ∈ \{ } ∈ Ax A := max (cid:107) (cid:107)2 . (4.93) (cid:107) (cid:107)2 x x (cid:107) (cid:107)2 We introduce the notation of a subscript in the matrix norm (left-hand side), similar to the Euclidean norm for vectors (right-hand side), which hassubscript2.Thespectralnorm(4.93)determineshowlonganyvector xcanatmostbecomewhenmultipliedbyA. Theorem 4.24. ThespectralnormofAisitslargestsingularvalueσ . 1 Weleavetheproofofthistheoremasanexercise. Eckart-Young Theorem 4.25 (Eckart-Young Theorem (Eckart and Young, 1936)). Con- theorem sidera matrixA Rm×n ofrankr andletB Rm×n bea matrixofrank k.Foranyk (cid:54) r w ∈ ithA(cid:98)(k) = (cid:80)k σ u