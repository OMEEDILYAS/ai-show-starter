The determinant of a matrix A Rn×n is the product of ∈ itseigenvalues,i.e., n (cid:89) det(A) = λ , (4.42) i i=1 whereλ are(possiblyrepeated)eigenvaluesofA. i Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 4.2 EigenvaluesandEigenvectors 113 Figure4.6 Geometric x A interpretationof 2 eigenvalues.The v 2 eigenvectorsofA x v getstretchedbythe 1 1 corresponding eigenvalues.The Theorem 4.17. The trace of a matrix A Rn×n is the sum of its eigenval- areaoftheunit ∈ squarechangesby ues,i.e., |λ1λ2|,the n circumference (cid:88) tr(A) = λ , (4.43) changesbyafactor i i=1 2(|λ1|+|λ2|). whereλ are(possiblyrepeated)eigenvaluesofA. i Let us provide a geometric intuition of these two theorems. Consider a matrix A R2×2 that possesses two linearly independent eigenvectors x ,x .Forth ∈ isexample,weassume(x ,x )areanONBofR2sothatthey 1 2 1 2 are orthogonal and the area of the square they span is 1; see Figure 4.6. FromSection4.1, weknowthatthedeterminantcomputes thechangeof area of unit square under the transformation A. In this example, we can compute the change of area explicitly: Mapping the eigenvectors using A gives us vectors v = Ax = λ x and v = Ax = λ x , i.e., the 1 1 1 1 2 2 2 2 new vectors v are scaled versions of the eigenvectors x , and the scaling i i factors are the corresponding eigenvalues λ . v ,v are still orthogonal, i 1 2 andtheareaoftherectangletheyspanis λ λ . 1 2 | | Given that x ,x (in our example) are orthonormal, we can directly 1 2 compute the circumference of the unit square as 2(1 + 1). Mapping the eigenvectorsusingAcreatesarectanglewhosecircumferenceis2( λ + 1 | | λ ).Therefore,thesumoftheabsolutevaluesoftheeigenvaluestellsus 2 | | how the circumference of the unit square changes under the transformationmatrixA. Example 4.9 (Google’s PageRank – Webpages as Eigenvectors) Google uses the eigenvector corresponding to the maximal eigenvalue of a matrix A to determine the rank of a page for search. The idea for the PageRank algorithm, developed at Stanford University by Larry Page and SergeyBrinin1996,wasthattheimportanceofanywebpagecanbeapproximated by the importance of pages that link to it. For this, they write down all web sites as a huge directed graph that shows which page links to which. PageRank computes the weight (importance) x (cid:62) 0 of a web i site a by counting the number of pages pointing to a . Moreover, PageRi i anktakesintoaccounttheimportanceofthewebsitesthatlinktoa .The i navigationbehaviorofauseristhenmodeledbyatransitionmatrixAof thisgraphthattellsuswithwhat(click)probabilitysomebodywillendup (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 114 MatrixDecompositions on a different web site. The matrix A has the property that for any initialrank/importancevectorxofawebsitethesequencex,Ax,A2x,... PageRank converges to a vector x∗. This vector is called the PageRank and satisfies Ax∗ = x∗, i.e., it is an eigenvector (with corresponding eigenvalue 1) of A.Afternormalizingx∗,suchthat x∗ = 1,wecaninterprettheentries (cid:107) (cid:107) as probabilities. More details and different perspectives on PageRank can befoundintheoriginaltechnicalreport(Pageetal.,1999). 4.3 Cholesky Decomposition There are many ways to factorize special types of matrices that we encounter often in machine learning. In the positive real numbers, we have the square-root operation that gives us a decomposition of the number into identical components, e.g., 9 = 3 3. For matrices, we need to be · careful that we compute a square-root-like operation on positive