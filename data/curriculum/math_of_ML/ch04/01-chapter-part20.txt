v(cid:62) it ∈ holdsthat i=1 i i i A(cid:98)(k) = argmin A B , (4.94) rk(B)=k(cid:107) − (cid:107)2 (cid:13) (cid:13) (cid:13)A A(cid:98)(k)(cid:13) = σ . (4.95) (cid:13) (cid:13) k+1 − 2 The Eckart-Young theorem states explicitly how much error we introduce by approximating A using a rank-k approximation. We can interpret the rank-k approximation obtained with the SVD as a projection of (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 132 MatrixDecompositions the full-rank matrix A onto a lower-dimensional space of rank-at-most-k matrices. Of all possible projections, the SVD minimizes the error (with respecttothespectralnorm)betweenAandanyrank-k approximation. Wecanretracesomeofthestepstounderstandwhy(4.95)shouldhold. We observe that the difference between A A(cid:98)(k) is a matrix containing − thesumoftheremainingrank-1matrices r (cid:88) A A(cid:98)(k) = σ u v(cid:62). (4.96) − i i i i=k+1 ByTheorem4.24,weimmediatelyobtainσ asthespectralnormofthe k+1 difference matrix. Let us have a closer look at (4.94). If we assume that thereisanothermatrixB withrk(B) (cid:54) k,suchthat (cid:13) (cid:13) A B < (cid:13)A A(cid:98)(k)(cid:13) , (4.97) (cid:107) − (cid:107)2 (cid:13) − (cid:13) 2 thenthereexistsanatleast(n k)-dimensionalnullspaceZ Rn,such − ⊆ thatx Z impliesthatBx = 0.Thenitfollowsthat ∈ Ax = (A B)x , (4.98) (cid:107) (cid:107)2 (cid:107) − (cid:107)2 and by using a version of the Cauchy-Schwartz inequality (3.17) that encompassesnormsofmatrices,weobtain Ax (cid:54) A B x < σ x . (4.99) (cid:107) (cid:107)2 (cid:107) − (cid:107)2(cid:107) (cid:107)2 k+1 (cid:107) (cid:107)2 However, there exists a (k + 1)-dimensional subspace where Ax (cid:62) σ x ,whichisspannedbytheright-singularvectorsv ,j (cid:54)(cid:107) k+ (cid:107)2 1of k+1 (cid:107) (cid:107)2 j A.Addingupdimensionsofthesetwospacesyieldsanumbergreaterthan n,astheremustbeanonzerovectorinbothspaces.Thisisacontradiction oftherank-nullitytheorem(Theorem2.24)inSection2.7.3. The Eckart-Young theorem implies that we can use SVD to reduce a rank-r matrix A to a rank-k matrix A(cid:98) in a principled, optimal (in the spectralnormsense)manner.WecaninterprettheapproximationofAby a rank-k matrix as a form of lossy compression. Therefore, the low-rank approximationofamatrixappearsinmanymachinelearningapplications, e.g.,imageprocessing,noisefiltering,andregularizationofill-posedproblems. Furthermore, it plays a key role in dimensionality reduction and principalcomponentanalysis,aswewillseeinChapter10. Example 4.15 (Finding Structure in Movie Ratings and Consumers (continued)) Coming back to our movie-rating example, we can now apply the conceptoflow-rankapproximationstoapproximatetheoriginaldatamatrix. Recall that our first singular value captures the notion of science fiction theme in movies and science fiction lovers. Thus, by using only the first Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 4.6 MatrixApproximation 133 singularvalueterminarank-1decompositionofthemovie-ratingmatrix, weobtainthepredictedratings   0.6710 − A 1 = u 1 v(cid:62) 1 =    − 0 0 . . 7 0 1 9 9 3 7 9    (cid:2) − 0.7367 − 0.6515 − 0.1811 (cid:3) (4.100a) − 0.1515 −   0.4943 0.4372 0.1215 0.5302 0.4689 0.1303 =   . (4.100b) 0.0692 0.0612 0.0170 0.1116 0.0987 0.0274 This first rank-1 approximation A is insightful: it tells us that Ali and 1 Beatrix like science fiction movies, such as Star Wars and Bladerunner (entries have values > 0.4), but fails to capture the ratings of the other movies by Chandra. This is not surprising, as Chandra’s type of movies is not captured by the first singular value. The second singular value gives usabetterrank-1approximationforthosemovie-themelovers:   0.0236 A 2 = u 2 v(cid:62) 2 =    0 0 .2 .7 0 7 5 0 4 5    (cid:2) 0.0852 0.1762 − 0.9807 (cid:3) (4.101a) −