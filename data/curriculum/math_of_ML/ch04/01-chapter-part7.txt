, i.e., it shears the 2 0 1 points along the horizontal axis to the right if they are on the positive Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 4.2 EigenvaluesandEigenvectors 109 Figure4.4 Determinantsand eigenspaces. Overviewoffive λ1=2.0 linearmappingsand λ2=0.5 theirassociated det(A)=1.0 transformation matrices Ai∈R2×2 projecting400 color-codedpoints x∈R2(left λ1=1.0 column)ontotarget λ2=1.0 det(A)=1.0 pointsAix(right column).The centralcolumn depictsthefirst eigenvector, stretchedbyits λ1=(0.87-0.5j) associated λ2=(0.87+0.5j) eigenvalueλ1,and det(A)=1.0 thesecond eigenvector stretchedbyits eigenvalueλ2.Each rowdepictsthe effectofoneoffive λ1=0.0 transformation λ2=2.0 det(A)=0.0 matricesAiwith respecttothe standardbasis. λ1=0.5 λ2=1.5 det(A)=0.75 half of the vertical axis, and to the left vice versa. This mapping is area preserving (det(A ) = 1). The eigenvalue λ = 1 = λ is repeated 2 1 2 and the eigenvectors are collinear (drawn here for emphasis in two opposite directions). This indicates that the mapping acts only along onedirection(thehorizontalaxis). (cid:20) cos(π) sin(π) (cid:21) (cid:20)√3 1 (cid:21) A = 6 − 6 = 1 − The matrix A rotates the 3 sin(π) cos(π) 2 1 √3 3 6 6 points by π rad = 30◦ counter-clockwise and has only complex eigen6 values,reflectingthatthemappingisarotation(hence,noeigenvectors are drawn). A rotation has to be volume preserving, and so the determinantis1.Formoredetailsonrotations,werefertoSection3.9. (cid:20) (cid:21) 1 1 A = − represents a mapping in the standard basis that col4 1 1 − lapsesatwo-dimensionaldomainontoonedimension.Sinceoneeigen- (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 110 MatrixDecompositions valueis0,thespaceindirectionofthe(blue)eigenvectorcorresponding to λ = 0 collapses, while the orthogonal (red) eigenvector stretches 1 spacebyafactorλ = 2.Therefore,theareaoftheimageis0. 2 (cid:20) 1 1(cid:21) A = 2 is a shear-and-stretch mapping that scales space by 75% 5 1 1 2 since det(A ) = 3. It stretches space along the (blue) eigenvector | 5 | 4 of λ by a factor 1.5 and compresses it along the orthogonal (blue) 2 eigenvectorbyafactor0.5. Example 4.7 (Eigenspectrum of a Biological Neural Network) Figure4.5 0 Caenorhabditis elegansneural 50 network(Kaiserand Hilgetag,2006). 100 (a)Symmetrized connectivitymatrix; 150 (b)Eigenspectrum. 200 250 0 50 100 150 200 250 neuronindex xedninoruen 25 20 15 10 5 0 5 − 10 − 0 100 200 indexofsortedeigenvalue (a)Connectivitymatrix. eulavnegie (b)Eigenspectrum. Methods to analyze and learn from network data are an essential componentofmachinelearningmethods.Thekeytounderstandingnetworks is the connectivity between network nodes, especially if two nodes are connected to each other or not. In data science applications, it is often usefultostudythematrixthatcapturesthisconnectivitydata. Webuildaconnectivity/adjacencymatrixA R277×277ofthecomplete ∈ neural network of the worm C.Elegans. Each row/column represents one of the 277 neurons of this worm’s brain. The connectivity matrix A has a value of a = 1 if neuron i talks to neuron j through a synapse, and ij a = 0 otherwise. The connectivity matrix is not symmetric, which imij plies that eigenvalues may not be real valued. Therefore, we compute a symmetrizedversionoftheconnectivitymatrixasA := A+A(cid:62).This sym newmatrixA isshowninFigure4.5(a)andhasanonzerovaluea if sym ij and only if two neurons are connected (white pixels), irrespective of the direction of the connection. In Figure 4.5(b), we show the corresponding eigenspectrum of A . The horizontal axis shows the index of the sym eigenvalues,sortedindescendingorder.Theverticalaxisshowsthecorrespondingeigenvalue.TheS-likeshapeofthiseigenspectrumistypicalfor many biological neural networks. The underlying mechanism responsible forthisisanareaofactiveneuroscienceresearch. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 4.2 EigenvaluesandEigenvectors 111 Theorem 4.12. Theeigenvectorsx ,...,x ofamatrixA Rn×n withn 1 n ∈ distincteigenvaluesλ ,...,λ arelinearlyindependent. 1 n This theorem states