a column/row with λ R scales det(A) by λ. In ∈ particular,det(λA) = λndet(A). Swappingtworows/columnschangesthesignofdet(A). Becauseofthelastthreeproperties,wecanuseGaussianelimination(see Section 2.1) to compute det(A) by bringing A into row-echelon form. We can stop Gaussian elimination when we have A in a triangular form wheretheelementsbelowthediagonalareall0.Recallfrom(4.8)thatthe determinantofatriangularmatrixistheproductofthediagonalelements. Theorem 4.3. A square matrix A Rn×n has det(A) = 0 if and only if ∈ (cid:54) rk(A) = n.Inotherwords,Aisinvertibleifandonlyifitisfullrank. When mathematics was mainly performed by hand, the determinant calculation was considered an essential way to analyze matrix invertibility. However, contemporary approaches in machine learning use direct numerical methods that superseded the explicit calculation of the determinant. For example, in Chapter 2, we learned that inverse matrices can be computed by Gaussian elimination. Gaussian elimination can thus be usedtocomputethedeterminantofamatrix. Determinants will play an important theoretical role for the following sections, especially when we learn about eigenvalues and eigenvectors (Section4.2)throughthecharacteristicpolynomial. Definition 4.4. ThetraceofasquarematrixA Rn×n isdefinedas trace ∈ (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 104 MatrixDecompositions n (cid:88) tr(A) := a , (4.18) ii i=1 i.e.,thetraceisthesumofthediagonalelementsofA. Thetracesatisfiesthefollowingproperties: tr(A+B) = tr(A)+tr(B)forA,B Rn×n ∈ tr(αA) = αtr(A),α RforA Rn×n ∈ ∈ tr(I ) = n n tr(AB) = tr(BA)forA Rn×k,B Rk×n ∈ ∈ It can be shown that only one function satisfies these four properties together–thetrace(Gohbergetal.,2012). Thepropertiesofthetraceofmatrixproductsaremoregeneral.SpecifThetraceis ically,thetraceisinvariantundercyclicpermutations,i.e., invariantunder cyclicpermutations. tr(AKL) = tr(KLA) (4.19) formatricesA Ra×k,K Rk×l,L Rl×a.Thispropertygeneralizesto ∈ ∈ ∈ productsofanarbitrarynumberofmatrices.Asaspecialcaseof(4.19),it followsthatfortwovectorsx,y Rn ∈ tr(xy(cid:62)) = tr(y(cid:62)x) = y(cid:62)x R. (4.20) ∈ Given a linear mapping Φ : V V, where V is a vector space, we → define the trace of this map by using the trace of matrix representation of Φ. For a given basis of V, we can describe Φ by means of the transformation matrix A. Then the trace of Φ is the trace of A. For a different basis of V, it holds that the corresponding transformation matrix B of Φ canbeobtainedbyabasischangeoftheformS−1AS forsuitableS (see Section2.7.2).ForthecorrespondingtraceofΦ,thismeans tr(B) = tr(S−1AS) (4 = .19) tr(ASS−1) = tr(A). (4.21) Hence, while matrix representations of linear mappings are basis dependentthetraceofalinearmappingΦisindependentofthebasis. In this section, we covered determinants and traces as functions characterizingasquarematrix.Takingtogetherourunderstandingofdeterminants and traces we can now define an important equation describing a matrix A in terms of a polynomial, which we will use extensively in the followingsections. Definition 4.5 (Characteristic Polynomial). For λ R and a square matrixA Rn×n ∈ ∈ p (λ) := det(A λI) (4.22a) A − = c +c λ+c λ2+ +c λn−1+( 1)nλn, (4.22b) 0 1 2 n−1 ··· − characteristic c 0 ,...,c n−1 R,isthecharacteristicpolynomialofA.Inparticular, ∈ polynomial Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 4.2 EigenvaluesandEigenvectors 105 c = det(A), (4.23) 0 c = ( 1)n−1tr(A). (4.24) n−1 − The characteristic polynomial (4.22a) will allow us to compute eigenvaluesandeigenvectors,coveredinthenextsection. 4.2 Eigenvalues and Eigenvectors Wewillnowgettoknowanewwaytocharacterizeamatrixanditsassociated linear mapping. Recall from Section 2.7.1 that every linear mapping has a unique transformation matrix given an ordered basis. We can interpret linear mappings and their associated transformation matrices by performing an “eigen” analysis. As we will see, the eigenvalues of a lin- EigenisaGerman ear mapping will tell us how a special set of vectors, the eigenvectors, is wordmeaning “characteristic”, transformedbythelinearmapping. “self”,or“own”. Definition