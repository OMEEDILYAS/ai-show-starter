and especially if there is a structure linking which people like which movies. Applying the SVD to our data matrix A makesanumberofassumptions: 1. Allviewersratemoviesconsistentlyusingthesamelinearmapping. 2. Therearenoerrorsornoiseintheratings. 3. We interpret the left-singular vectors u as stereotypical movies and i theright-singularvectorsv asstereotypicalviewers. j Wethenmaketheassumptionthatanyviewer’sspecificmoviepreferences canbeexpressedasalinearcombinationofthev .Similarly,anymovie’s j like-abilitycanbeexpressedasalinearcombinationoftheu .Therefore, i a vector in the domain of the SVD can be interpreted as a viewer in the “space”ofstereotypicalviewers,andavectorinthecodomainoftheSVD Thesetwo“spaces” correspondingly as a movie in the “space” of stereotypical movies. Let us areonly inspecttheSVDofourmovie-usermatrix.Thefirstleft-singularvectoru 1 meaningfully has large absolute values for the two science fiction movies and a large spannedbythe first singular value (red shading in Figure 4.10). Thus, this groups a type respectiveviewer andmoviedataif ofuserswithaspecificsetofmovies(sciencefictiontheme).Similarly,the thedataitselfcovers firstright-singularv showslargeabsolutevaluesforAliandBeatrix,who asufficientdiversity 1 givehighratingstosciencefictionmovies(greenshadinginFigure4.10). ofviewersand movies. Thissuggeststhatv reflectsthenotionofasciencefictionlover. 1 Similarly, u , seems to capture a French art house film theme, and v 2 2 indicates that Chandra is close to an idealized lover of such movies. An idealized science fiction lover is a purist and only loves science fiction movies, so a science fiction lover v gives a rating of zero to everything 1 butsciencefictionthemed–thislogicisimpliedthediagonalsubstructure for the singular value matrix Σ. A specific movie is therefore represented by how it decomposes (linearly) into its stereotypical movies. Likewise, a person would be represented by how they decompose (via linear combination)intomoviethemes. Itisworth,tobrieflydiscussSVDterminologyandconventions,asthere are different versions used in the literature. The mathematics remains invarianttothesedifferences,butthesedifferencescanbeconfusing. For convenience in notation and abstraction, we use an SVD notation wheretheSVDisdescribedashavingtwosquareleft-andright-singular vector matrices, but a non-square singular value matrix. Our definifullSVD tion(4.64)fortheSVDissometimescalledthefullSVD. Some authors define the SVD a bit differently and focus on square sinDraft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 4.6 MatrixApproximation 129 gularmatrices.Then,forA Rm×n andm (cid:62) n, ∈ A = U Σ V(cid:62) . (4.89) m×n m×nn×nn×n SometimesthisformulationiscalledthereducedSVD(e.g.,Datta(2010)) reducedSVD or the SVD (e.g., Press et al. (2007)). This alternative format changes merely how the matrices are constructed but leaves the mathematical structure of the SVD unchanged. The convenience of this alternative formulationisthatΣisdiagonal,asintheeigenvaluedecomposition. In Section 4.6, we will learn about matrix approximation techniques usingtheSVD,whichisalsocalledthetruncatedSVD. truncatedSVD It is possible to define the SVD of a rank-r matrix A so that U is an m r matrix, Σ a diagonal matrix r r, and V an r n matrix. × × × This construction is very similar to our definition, and ensures that the diagonal matrix Σ has only nonzero entries along the diagonal. The main convenience of this alternative notation is that Σ is diagonal, as intheeigenvaluedecomposition. A restriction that the SVD for A only applies to m n matrices with × m > nispracticallyunnecessary.Whenm < n,theSVDdecomposition will yield Σ with more zero columns than rows and, consequently, the singularvaluesσ ,...,σ are0. m+1 n The SVD is used in a variety of applications in machine learning from least-squares problems in curve fitting to solving systems of linear equations.TheseapplicationsharnessvariousimportantpropertiesoftheSVD, itsrelationtotherankofamatrix,anditsabilitytoapproximatematrices of a given rank with lower-rank matrices. Substituting a matrix with its SVD has often the advantage of making calculation more robust to numerical roundingerrors. As wewill explore inthe next section,the SVD’s ability to approximate matrices with “simpler” matrices in