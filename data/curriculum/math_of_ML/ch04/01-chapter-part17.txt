(4.87) 2 − √2 5 5 (cid:20) (cid:21) 1 1 2 U = [u ,u ] = . (4.88) 1 2 √5 2 1 − Notethatonacomputertheapproachillustratedherehaspoornumerical behavior,andtheSVDofAisnormallycomputedwithoutresortingtothe eigenvaluedecompositionofA(cid:62)A. 4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition Let us consider the eigendecomposition A = PDP−1 and the SVD A = UΣV(cid:62) andreviewthecoreelementsofthepastsections. TheSVDalwaysexistsforanymatrixRm×n.Theeigendecompositionis only defined for square matrices Rn×n and only exists if we can find a basisofeigenvectorsofRn. The vectors in the eigendecomposition matrix P are not necessarily orthogonal,i.e.,thechangeofbasisisnotasimplerotationandscaling. Ontheotherhand,thevectorsinthematricesU andV intheSVDare orthonormal,sotheydorepresentrotations. Both the eigendecomposition and the SVD are compositions of three linearmappings: 1. Changeofbasisinthedomain 2. Independentscalingofeachnewbasisvectorandmappingfromdomaintocodomain 3. Changeofbasisinthecodomain Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 4.5 SingularValueDecomposition 127 Figure4.10 Movie ratingsofthree peopleforfour moviesanditsSVD   5 4 1 decomposition.  5 5 0     0 0 5  1 0 4 ilA xirtaeB ardnahC   StarWars 0.6710 0.0236 0.4647 0.5774 BladeR A u m n e n l e ie r =     − − 0 0. . 7 0 1 9 9 3 7 9 0 0 . . 2 7 0 7 5 0 4 5 − 0 0 . . 4 5 7 2 5 6 9 8 − 0 0 . . 4 3 6 4 1 6 9 4     − − − − Delicatessen 0.1515 0.6030 0.5293 0.5774 − − −   9.6438 0 0  0 6.3639 0     0 0 0.7056  0 0 0   0.7367 0.6515 0.1811   − 0.0852 − 0.1762 − 0.9807   − 0.6708 0.7379 0.0743 − − A key difference between the eigendecomposition and the SVD is that in the SVD, domain and codomain can be vector spaces of different dimensions. In the SVD, the left- and right-singular vector matrices U and V are generally not inverse of each other (they perform basis changes in different vector spaces). In the eigendecomposition, the basis change matricesP andP−1 areinversesofeachother. In the SVD, the entries in the diagonal matrix Σ are all real and nonnegative, which is not generally true for the diagonal matrix in the eigendecomposition. The SVD and the eigendecomposition are closely related through their projections – Theleft-singularvectorsofAareeigenvectorsofAA(cid:62) – Theright-singularvectorsofAareeigenvectorsofA(cid:62)A. – ThenonzerosingularvaluesofAarethesquarerootsofthenonzero eigenvalues of AA(cid:62) and are equal to the nonzero eigenvalues of A(cid:62)A. For symmetric matrices A Rn×n, the eigenvalue decomposition and ∈ the SVD are one and the same, which follows from the spectral theorem4.15. Example 4.14 (Finding Structure in Movie Ratings and Consumers) Let us add a practical interpretation of the SVD by analyzing data on people and their preferred movies. Consider three viewers (Ali, Beatrix, Chandra) rating four different movies (Star Wars, Blade Runner, Amelie, Delicatessen).Theirratingsarevaluesbetween0(worst)and5(best)and encoded in a data matrix A R4×3 as shown in Figure 4.10. Each row ∈ (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 128 MatrixDecompositions represents a movie and each column a user. Thus, the column vectors of movieratings,oneforeachviewer,arex ,x ,x . Ali Beatrix Chandra Factoring A using the SVD offers us a way to capture the relationships of how people rate movies,