case if all diagonal entries are nonzero so that the matrix is invertible). A special diagonal matrix is theidentitymatrixI. 4.8 Further Reading Most of the content in this chapter establishes underlying mathematics andconnectsthemtomethodsforstudyingmappings,manyofwhichare attheheartofmachinelearningatthelevelofunderpinningsoftwaresolutionsandbuildingblocksforalmostallmachinelearningtheory.Matrix characterization using determinants, eigenspectra, and eigenspaces providesfundamentalfeaturesandconditionsforcategorizingandanalyzing matrices. This extends to all forms of representations of data and mappings involving data, as well as judging the numerical stability of computationaloperationsonsuchmatrices(Pressetal.,2007). Determinants are fundamental tools in order to invert matrices and compute eigenvalues “by hand”. However, for almost all but the smallest instances, numerical computation by Gaussian elimination outperforms determinants (Press et al., 2007). Determinants remain nevertheless a powerful theoretical concept, e.g., to gain intuition about the orientation of a basis based on the sign of the determinant. Eigenvectors can be used to perform basis changes to transform data into the coordinates of meaningful orthogonal, feature vectors. Similarly, matrix decomposition methods, such as the Cholesky decomposition, reappear often when we computeorsimulaterandomevents(RubinsteinandKroese,2016).Therefore, (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 136 MatrixDecompositions the Cholesky decomposition enables us to compute the reparametrization trick where we want to perform continuous differentiation over random variables,e.g.,invariationalautoencoders(JimenezRezendeetal.,2014; KingmaandWelling,2014). Eigendecomposition is fundamental in enabling us to extract meaningful and interpretable information that characterizes linear mappings. Therefore, the eigendecomposition underlies a general class of machine learningalgorithmscalledspectralmethodsthatperformeigendecomposition of a positive-definite kernel. These spectral decomposition methods encompass classical approaches to statistical data analysis, such as the following: principalcomponent analysis Principalcomponentanalysis(PCA(Pearson,1901),seealsoChapter10), in which a low-dimensional subspace, which explains most of the variabilityinthedata,issought. Fisherdiscriminant analysis Fisher discriminant analysis, which aims to determine a separating hyperplanefordataclassification(Mikaetal.,1999). multidimensional scaling Multidimensionalscaling(MDS)(CarrollandChang,1970). Thecomputationalefficiencyofthesemethodstypicallycomesfromfinding the best rank-k approximation to a symmetric, positive semidefinite matrix. More contemporary examples of spectral methods have different origins, but each of them requires the computation of the eigenvectors Isomap andeigenvaluesofapositive-definitekernel,suchasIsomap(Tenenbaum Laplacian et al., 2000), Laplacian eigenmaps (Belkin and Niyogi, 2003), Hessian eigenmaps eigenmaps (Donoho and Grimes, 2003), and spectral clustering (Shi and Hessianeigenmaps Malik, 2000). The core computations of these are generally underpinned spectralclustering bylow-rankmatrixapproximationtechniques(BelabbasandWolfe,2009) asweencounteredhereviatheSVD. TheSVDallowsustodiscoversomeofthesamekindofinformationas the eigendecomposition. However, the SVD is more generally applicable to non-square matrices and data tables. These matrix factorization methods become relevant whenever we want to identify heterogeneity in data when we want to perform data compression by approximation, e.g., insteadofstoringn mvaluesjuststoring(n+m)kvalues,orwhenwewant × to perform data pre-processing, e.g., to decorrelate predictor variables of a design matrix (Ormoneit et al., 2001). The SVD operates on matrices, which we can interpret as rectangular arrays with two indices (rows and columns). The extension of matrix-like structure to higher-dimensional arrays are called tensors. It turns out that the SVD is the special case of a more general family of decompositions that operate on such tensors (Kolda and Bader, 2009). SVD-like operations and low-rank approximaTucker tionsontensorsare,forexample,theTuckerdecomposition(Tucker,1966) decomposition ortheCPdecomposition(CarrollandChang,1970). CPdecomposition The SVD low-rank approximation is frequently used in machine learning for computational efficiency reasons. This is because it reduces the Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. Exercises 137 amount ofmemory and operationswith nonzero multiplicationswe need toperformonpotentiallyverylargematricesofdata(TrefethenandBauIII, 1997). Moreover, low-rank approximations are used to operate on matrices that may contain missing values as well as for purposes of lossy compression and dimensionality