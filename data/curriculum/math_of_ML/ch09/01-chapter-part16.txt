likelihood estimator for the slopeparameteras X(cid:62)y θ = (X(cid:62)X)−1X(cid:62)y = R, (9.66) ML X(cid:62)X ∈ whereX = [x ,...,x ](cid:62) RN,y = [y ,...,y ](cid:62) RN. 1 N 1 N ∈ ∈ ThismeansforthetraininginputsX weobtaintheoptimal(maximum likelihood)reconstructionofthetrainingtargetsas X(cid:62)y XX(cid:62) Xθ = X = y, (9.67) ML X(cid:62)X X(cid:62)X (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 314 LinearRegression i.e., we obtain the approximation with the minimum least-squares error betweeny andXθ. As we are looking for a solution of y = Xθ, we can think of linear Linearregression regression as a problem for solving systems of linear equations. Therecanbethoughtofas fore,wecanrelatetoconceptsfromlinearalgebraandanalyticgeometry amethodforsolving that we discussed in Chapters 2 and 3. In particular, looking carefully systemsoflinear at (9.67) we see that the maximum likelihood estimator θ in our exequations. ML ample from (9.65) effectively does an orthogonal projection of y onto Maximum theone-dimensionalsubspacespannedbyX.Recallingtheresultsonorlikelihoodlinear thogonalprojectionsfromSection3.8,weidentify XX(cid:62) astheprojection regressionperforms X(cid:62)X matrix,θ asthecoordinatesoftheprojectionontotheone-dimensional anorthogonal ML subspace of RN spanned by X and Xθ as the orthogonal projection of projection. ML y ontothissubspace. Therefore, the maximum likelihood solution provides also a geometrically optimal solution by finding the vectors in the subspace spanned by X that are “closest” to the corresponding observations y, where “closest” means the smallest (squared) distance of the function values y to n x θ.Thisisachievedbyorthogonalprojections.Figure9.12(b)showsthe n projectionofthenoisyobservationsontothesubspacethatminimizesthe squareddistancebetweentheoriginaldatasetanditsprojection(notethat the x-coordinate is fixed), which corresponds to the maximum likelihood solution. Inthegenerallinearregressioncasewhere y = φ(cid:62)(x)θ+(cid:15), (cid:15) (cid:0) 0, σ2(cid:1) (9.68) ∼ N withvector-valuedfeaturesφ(x) RK,weagaincaninterpretthemaxi- ∈ mumlikelihoodresult y Φθ , (9.69) ML ≈ θ = (Φ(cid:62)Φ)−1Φ(cid:62)y (9.70) ML as a projection onto a K-dimensional subspace of RN, which is spanned bythecolumnsofthefeaturematrixΦ;seeSection3.8.2. If the feature functions φ that we use to construct the feature mak trix Φ are orthonormal (see Section 3.7), we obtain a special case where the columns of Φ form an orthonormal basis (see Section 3.5), such that Φ(cid:62)Φ = I.Thiswillthenleadtotheprojection (cid:32) (cid:33) K (cid:88) Φ(Φ(cid:62)Φ)−1Φ(cid:62)y = ΦΦ(cid:62)y = φ φ(cid:62) y (9.71) k k k=1 so that the coupling between different features has disappeared and the maximumlikelihoodprojectionissimplythesumofprojectionsofy onto theindividualbasisvectorsφ ,i.e.,thecolumnsofΦ.Manypopularbasis k functions in signal processing, such as wavelets and Fourier bases, are orthogonal basis functions. When the basis is not orthogonal, one can Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.5 FurtherReading 315 convertasetoflinearlyindependentbasisfunctionstoanorthogonalbasis byusingtheGram-Schmidtprocess(Strang,2003). 9.5 Further Reading In this chapter, we discussed linear regression for Gaussian likelihoods and conjugate Gaussian priors on the parameters of the model. This allowed for closed-form Bayesian inference. However, in some applications we may want to choose a different likelihood function. For example, in a binary classification setting, we observe only two possible (categorical) classification outcomes, and a Gaussian likelihood is inappropriate in this setting. Instead,wecanchooseaBernoullilikelihoodthatwillreturnaprobabilityof thepredictedlabeltobe1(or0).WerefertothebooksbyBarber(2012), Bishop(2006),andMurphy(2012)foranin-depthintroductiontoclassificationproblems.Adifferentexamplewherenon-Gaussianlikelihoodsare importantiscountdata.Countsarenon-negativeintegers,andinthiscase aBinomialorPoissonlikelihoodwouldbeabetterchoicethanaGaussian. Alltheseexamplesfallintothecategoryofgeneralizedlinearmodels,aflex- generalizedlinear ible generalization of linear regression that allows for response variables model thathaveerrordistributionsotherthanaGaussiandistribution.TheGLM Generalizedlinear generalizes linear regression by allowing the linear model to be related modelsarethe totheobservedvaluesviaasmoothandinvertiblefunctionσ( )thatmay buildingblocksof be nonlinear so that y = σ(f(x)), where f(x) = θ(cid:62)φ(x) is · the linear deepneural networks. regression model from (9.13). We can therefore think of a generalized linear model in terms of function composition y = σ f, where f is a