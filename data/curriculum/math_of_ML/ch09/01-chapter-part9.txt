boundaries of overfitting, it is not a general solution to this problem, so weneedamoreprincipledapproachtotackleoverfitting. Figure9.7 Polynomial 4 regression: maximumlikelihood 2 andMAPestimates. (a)Polynomialsof 0 degree6; 2 (b)polynomialsof − degree8. 4 − 4 2 0 2 4 − − x y 4 2 0 Trainingdata 2 − MLE MAP 4 − 4 2 0 2 4 − − x (a)Polynomialsofdegree6. y Trainingdata MLE MAP (b)Polynomialsofdegree8. 9.2.4 MAP Estimation as Regularization Instead of placing a prior distribution on the parameters θ, it is also possible to mitigate the effect of overfitting by penalizing the amplitude of regularization the parameter by means of regularization. In regularized least squares, we regularizedleast considerthelossfunction squares y Φθ 2 +λ θ 2 , (9.32) (cid:107) − (cid:107) (cid:107) (cid:107)2 which we minimize with respect to θ (see Section 8.2.3). Here, the first data-fitterm term is a data-fit term (also called misfit term), which is proportional to misfitterm the negative log-likelihood; see (9.10b). The second term is called the regularizer regularizer, and the regularization parameter λ (cid:62) 0 controls the “strictregularization ness”oftheregularization. parameter Remark. Instead of the Euclidean norm , we can choose any p-norm (cid:107)·(cid:107)2 in (9.32). In practice, smaller values for p lead to sparser solutions. (cid:107)·(cid:107)p Here, “sparse” means that many parameter values θ = 0, which is also d Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.3 BayesianLinearRegression 303 useful for variable selection. For p = 1, the regularizer is called LASSO LASSO (leastabsoluteshrinkageandselectionoperator)andwasproposedbyTibshirani(1996). ♦ The regularizer λ θ 2 in (9.32) can be interpreted as a negative log- (cid:107) (cid:107)2 Gaussianprior,whichweuseinMAPestimation;see(9.26).Morespecif- (cid:0) (cid:1) ically, with a Gaussian prior p(θ) = 0, b2I , we obtain the negative N log-Gaussianprior 1 logp(θ) = θ 2 +const (9.33) − 2b2 (cid:107) (cid:107)2 so that for λ = 1 the regularization term and the negative log-Gaussian 2b2 priorareidentical. Giventhattheregularizedleast-squareslossfunctionin(9.32)consists oftermsthatarecloselyrelatedtothenegativelog-likelihoodplusanegative log-prior, it is not surprising that, when we minimize this loss, we obtainasolutionthatcloselyresemblestheMAPestimatein(9.31).More specifically,minimizingtheregularizedleast-squareslossfunctionyields θ = (Φ(cid:62)Φ+λI)−1Φ(cid:62)y, (9.34) RLS which is identical to the MAP estimate in (9.31) for λ = σ2 , where σ2 is b2 the noise variance and b2 the variance of the (isotropic) Gaussian prior (cid:0) (cid:1) p(θ) = 0, b2I . Apointestimateisa N So far, we have covered parameter estimation using maximum likeli- singlespecific hood and MAP estimation where we found point estimates θ∗ that op- parametervalue, unlikeadistribution timize an objective function (likelihood or posterior). We saw that both overplausible maximum likelihood and MAP estimation can lead to overfitting. In the parametersettings. next section, we will discuss Bayesian linear regression, where we use Bayesian inference (Section 8.4) to find a posterior distribution over the unknown parameters, which we subsequently use to make predictions. Morespecifically,forpredictionswewillaverageoverallplausiblesetsof parametersinsteadoffocusingonapointestimate. 9.3 Bayesian Linear Regression Previously,welookedatlinearregressionmodelswhereweestimatedthe model parameters θ, e.g., by means of maximum likelihood or MAP estimation.WediscoveredthatMLEcanleadtosevereoverfitting,inparticular, in the small-data regime. MAP addresses this issue by placing a prior ontheparametersthatplaystheroleofaregularizer. Bayesianlinear Bayesian linear regression pushes the idea of the parameter prior a step regression further and does not even attempt to compute a point estimate of the parameters,butinsteadthefullposteriordistributionovertheparameters is taken into account when