making predictions. This means we do not fit any parameters, but we compute a mean over all plausible parameters settings(accordingtotheposterior). (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 304 LinearRegression 9.3.1 Model InBayesianlinearregression,weconsiderthemodel (cid:0) (cid:1) prior p(θ) = m , S , 0 0 N (9.35) likelihood p(y x,θ) = (cid:0) y φ(cid:62)(x)θ, σ2(cid:1) , | N | (cid:0) (cid:1) Figure9.8 wherewenowexplicitlyplaceaGaussianpriorp(θ) = m 0 , S 0 onθ, N Graphicalmodelfor which turns the parameter vector into a random variable. This allows us Bayesianlinear towritedownthecorrespondinggraphicalmodelinFigure9.8,wherewe regression. made the parameters of the Gaussian prior on θ explicit. The full probam 0 S 0 bilistic model, i.e., the joint distribution of observed and unobserved randomvariables,y andθ,respectively,is θ p(y,θ x) = p(y x,θ)p(θ). (9.36) σ | | x y 9.3.2 Prior Predictions Inpractice,weareusuallynotsomuchinterestedintheparametervalues θ themselves. Instead, our focus often lies in the predictions we make withthoseparametervalues.InaBayesiansetting,wetaketheparameter distribution and average over all plausible parameter settings when we make predictions. More specifically, to make predictions at an input x , ∗ weintegrateoutθ andobtain (cid:90) p(y x ) = p(y x ,θ)p(θ)dθ = E [p(y x ,θ)], (9.37) ∗ ∗ ∗ ∗ θ ∗ ∗ | | | which we can interpret as the average prediction of y x ,θ for all plau- ∗ ∗ | sibleparametersθ accordingtothepriordistributionp(θ).Notethatpredictions using the prior distribution only require us to specify the input x ,butnotrainingdata. ∗ In our model (9.35), we chose a conjugate (Gaussian) prior on θ so that the predictive distribution is Gaussian as well (and can be computed (cid:0) (cid:1) inclosedform):Withthepriordistributionp(θ) = m , S ,weobtain 0 0 N thepredictivedistributionas p(y x ) = (cid:0) φ(cid:62)(x )m , φ(cid:62)(x )S φ(x )+σ2(cid:1) , (9.38) ∗ ∗ ∗ 0 ∗ 0 ∗ | N where we exploited that (i) the prediction is Gaussian due to conjugacy (seeSection6.6)andthemarginalizationpropertyofGaussians(seeSection6.5),(ii)theGaussiannoiseisindependentsothat V[y ] = V [φ(cid:62)(x )θ]+V [(cid:15)], (9.39) ∗ θ ∗ (cid:15) and (iii) y is a linear transformation of θ so that we can apply the rules ∗ for computing the mean and covariance of the prediction analytically by using(6.50)and(6.51),respectively.In(9.38),thetermφ(cid:62)(x )S φ(x ) ∗ 0 ∗ inthepredictivevarianceexplicitlyaccountsfortheuncertaintyassociated Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.3 BayesianLinearRegression 305 with the parameters θ, whereas σ2 is the uncertainty contribution due to themeasurementnoise. If we are interested in predicting noise-free function values f(x ) = ∗ φ(cid:62)(x )θ insteadofthenoise-corruptedtargetsy weobtain ∗ ∗ p(f(x )) = (cid:0) φ(cid:62)(x )m , φ(cid:62)(x )S φ(x ) (cid:1) , (9.40) ∗ ∗ 0 ∗ 0 ∗ N whichonlydiffersfrom(9.38)intheomissionofthenoisevarianceσ2 in thepredictivevariance. Remark (Distribution over Functions). Since we can represent the distri- Theparameter bution p(θ) using a set of samples θ and every sample θ gives rise to a distributionp(θ) i i function f ( ) = θ(cid:62)φ( ), it follows that the parameter distribution p(θ) inducesa i · i · distributionover inducesadistributionp(f( ))overfunctions.Hereweusethenotation( ) · · functions. toexplicitlydenoteafunctionalrelationship. ♦ Example 9.7 (Prior over Functions) Figure9.9 Prior overfunctions. 4 (a)Distributionover functions 2 representedbythe meanfunction 0 (blackline)andthe marginal 2 − uncertainties 4 (shaded), − representingthe 4 2 0 2 4 − − x 67%and95% confidencebounds, respectively; (b)samplesfrom thepriorover functions,whichare inducedbythe samplesfromthe parameterprior. y