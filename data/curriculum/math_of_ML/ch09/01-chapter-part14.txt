us revisit the Bayesian linear regression problem with polynomials of degree 5. We choose a parameter prior p(θ) = (cid:0) 0, 1I (cid:1) . Figure 9.9 N 4 visualizes the prior over functions induced by the parameter prior and samplefunctionsfromthisprior. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 310 LinearRegression Figure 9.10 shows the posterior over functions that we obtain via Bayesian linear regression. The training dataset is shown in panel (a); panel (b) shows the posterior distribution over functions, including the functions we would obtain via maximum likelihood and MAP estimation. The function we obtain using the MAP estimate also corresponds to the posteriormeanfunctionintheBayesianlinearregressionsetting.Panel(c) shows some plausible realizations (samples) of functions under that posterioroverfunctions. Figure9.10 Bayesianlinear 4 regressionand 2 posteriorover 0 functions. (a)trainingdata; − 2 (b)posterior 4 − distributionover 4 2 0 2 4 − − x functions; (c)Samplesfrom theposteriorover functions. y 4 2 0 2 − 4 − 4 2 0 2 4 − − x (a)Trainingdata. y 4 2 0 Trainingdata MLE 2 MAP − BLR 4 − 4 2 0 2 4 − − x (b)Posterioroverfunctionsrepresentedbythemarginaluncertainties (shaded) showing the 67% and 95% predictive confidence bounds, the maximum likelihood estimate (MLE) and the MAP estimate (MAP), the latter of which is identical to theposteriormeanfunction. y (c)Samplesfromtheposterior over functions, which are inducedbythesamplesfromthe parameterposterior. Figure 9.11 shows some posterior distributions over functions induced by the parameter posterior. For different polynomial degrees M, the left panels show the maximum likelihood function θ(cid:62) φ( ), the MAP functionθ(cid:62) φ( )(whichisidenticaltotheposteriorm M e L an · function),andthe MAP · 67% and 95% predictive confidence bounds obtained by Bayesian linear regression,representedbytheshadedareas. Therightpanelsshowsamplesfromtheposterioroverfunctions:Here, we sampled parameters θ from the parameter posterior and computed i the function φ(cid:62)(x )θ , which is a single realization of a function under ∗ i the posterior distribution over functions. For low-order polynomials, the parameter posterior does not allow the parameters to vary much: The sampled functions are nearly identical. When we make the model more flexible by adding more parameters (i.e., we end up with a higher-order polynomial),theseparametersarenotsufficientlyconstrainedbytheposterior,andthesampledfunctionscanbeeasilyvisuallyseparated.Wealso seeinthecorrespondingpanelsonthelefthowtheuncertaintyincreases, especiallyattheboundaries. Althoughforaseventh-orderpolynomialtheMAPestimateyieldsareasonablefit,theBayesianlinearregressionmodeladditionallytellsusthat Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.3 BayesianLinearRegression 311 Figure9.11 4 Bayesianlinear regression.Left 2 panels:Shaded areasindicatethe 0 67%(darkgray) and95%(light 2 − gray)predictive confidencebounds. 4 − Themeanofthe 4 2 0 2 4 Bayesianlinear − − x regressionmodel coincideswiththe MAPestimate.The predictive uncertaintyisthe sumofthenoise termandthe posteriorparameter uncertainty,which dependsonthe locationofthetest input.Rightpanels: sampledfunctions fromtheposterior distribution. y 4 2 0 Trainingdata MLE 2 − MAP BLR 4 − 4 2 0 2 4 − − x y (a)PosteriordistributionforpolynomialsofdegreeM =3(left)andsamplesfromtheposterioroverfunctions(right). 4 2 0 2 − 4 − 4 2 0 2 4 − − x y 4 2 0 Trainingdata MLE 2 − MAP BLR 4 − 4 2 0 2 4 − − x y (b) Posterior distribution for polynomials of degree M = 5 (left) and samples from the posterioroverfunctions(right). 4 2 0 2 − 4 − 4 2 0 2 4 − − x y Trainingdata 4 MLE MAP 2 BLR 0 2 − 4 − 4 2 0 2 4 − − x y (c)PosteriordistributionforpolynomialsofdegreeM =7(left)andsamplesfromtheposterioroverfunctions(right). the posterior uncertainty is huge. This