findingthepolynomialdegreeM thatminimizestheobjective.Giventhat the polynomial degree is a natural number, we can perform a brute-force search and enumerate all (reasonable) values of M. For a training set of size N it is sufficient to test 0 (cid:54) M (cid:54) N 1. For M < N, the maximum likelihood estimator is unique. For M (cid:62)− N, we have more parameters Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.2 ParameterEstimation 299 Figure9.5 4 Maximum 2 likelihoodfitsfor 0 differentpolynomial − 2 degreesM. 4 − 4 2 0 2 4 − − x y Trainingdata 4 MLE 2 0 2 − 4 − 4 2 0 2 4 − − x (a)M =0 y Trainingdata 4 MLE 2 0 2 − 4 − 4 2 0 2 4 − − x (b)M =1 y Trainingdata MLE (c)M =3 4 2 0 2 − 4 − 4 2 0 2 4 − − x y Trainingdata 4 MLE 2 0 2 − 4 − 4 2 0 2 4 − − x (d)M =4 y Trainingdata 4 MLE 2 0 2 − 4 − 4 2 0 2 4 − − x (e)M =6 y Trainingdata MLE (f)M =9 thandatapoints,andwouldneedtosolveanunderdeterminedsystemof linear equations (Φ(cid:62)Φ in (9.19) would also no longer be invertible) so thatthereareinfinitelymanypossiblemaximumlikelihoodestimators. Figure9.5showsanumberofpolynomialfitsdeterminedbymaximum likelihood for the dataset from Figure 9.4(a) with N = 10 observations. We notice that polynomials of low degree (e.g., constants (M = 0) or linear (M = 1)) fit the data poorly and, hence, are poor representations of the true underlying function. For degrees M = 3,...,5, the fits look plausibleandsmoothlyinterpolatethedata.Whenwegotohigher-degree Thecaseof polynomials, we notice that they fit the data better and better. In the ex- M =N−1is tremecaseofM = N 1 = 9,thefunctionwillpassthrougheverysingle extremeinthesense − thatotherwisethe data point. However, these high-degree polynomials oscillate wildly and nullspaceofthe are a poor representation of the underlying function that generated the corresponding data,suchthatwesufferfromoverfitting. systemoflinear Remember that the goal is to achieve good generalization by making equationswouldbe non-trivial,andwe accurate predictions for new (unseen) data. We obtain some quantitawouldhave tiveinsightintothedependenceofthegeneralizationperformanceonthe infinitelymany polynomialofdegreeM byconsideringaseparatetestsetcomprising200 optimalsolutionsto data points generated using exactly the same procedure used to generate thelinearregression problem. thetrainingset.Astestinputs,wechosealineargridof200pointsinthe overfitting intervalof[ 5,5].ForeachchoiceofM,weevaluatetheRMSE(9.23)for − Notethatthenoise boththetrainingdataandthetestdata. varianceσ2>0. Looking now at the test error, which is a qualitive measure of the generalizationpropertiesofthecorrespondingpolynomial,wenoticethatinitially the test error decreases; see Figure 9.6 (orange). For fourth-order polynomials,thetesterrorisrelativelylowandstaysrelativelyconstantup todegree5.However,fromdegree6onwardthetesterrorincreasessignificantly, and high-order polynomials have very bad generalization properties.Inthisparticularexample,thisalsoisevidentfromthecorresponding (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 300 LinearRegression Figure9.6 Training 10 andtesterror. 8 6 4 2 0 0 2 4 6 8 10 Degreeofpolynomial ESMR Trainingerror Testerror trainingerror maximum likelihood fits in Figure 9.5. Note that the training error (blue curveinFigure9.6)neverincreaseswhenthedegreeofthepolynomialincreases.Inourexample,thebestgeneralization(thepointofthesmallest testerror testerror)isobtainedforapolynomialofdegreeM = 4. 9.2.3 Maximum A Posteriori Estimation We just saw that maximum likelihood estimation is prone to overfitting. We often observe that the magnitude of the parameter values becomes relativelylargeifwerunintooverfitting(Bishop,2006). To mitigate the effect of huge parameter values, we can place a prior distribution p(θ) on the parameters. The prior distribution explicitly encodeswhatparametervaluesareplausible(beforehavingseenanydata). (cid:0) (cid:1) For example, a Gaussian prior p(θ) = 0, 1 on a single parameter N θ encodes that parameter values are