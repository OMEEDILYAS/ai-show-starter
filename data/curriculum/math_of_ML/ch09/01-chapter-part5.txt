where φ : RD RK is a (nonlinear) transformation of the inputs x and φ k : RD R i → s the kth component of the feature vector φ. Note that the featurevector → modelparametersθ stillappearonlylinearly. Example 9.3 (Polynomial Regression) Weareconcernedwitharegressionproblemy = φ(cid:62)(x)θ+(cid:15),wherex R andθ RK.Atransformationthatisoftenusedinthiscontextis ∈ ∈   1   φ 0 (x)  x     φ 1 (x)   x2  φ(x) =    . . .    =    x3    ∈ RK. (9.14)  . .  φ K−1 (x)  .  xK−1 This means that we “lift” the original one-dimensional input space into a K-dimensional feature space consisting of all monomials xk for k = 0,...,K 1. With these features, we can model polynomials of degree (cid:54) K 1w − ithintheframeworkoflinearregression:Apolynomialofdegree − (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 296 LinearRegression K 1is − K−1 (cid:88) f(x) = θ xk = φ(cid:62)(x)θ, (9.15) k k=0 where φ is defined in (9.14) and θ = [θ ,...,θ ](cid:62) RK contains the 0 K−1 ∈ (linear)parametersθ . k Letusnowhavealookatmaximumlikelihoodestimationoftheparametersθ inthe linearregression model(9.13). Weconsidertraining inputs featurematrix x n RD andtargetsy n R,n = 1,...,N,anddefinethefeaturematrix ∈ ∈ designmatrix (designmatrix)as   Φ :=    φ(cid:62)( . . . x 1 )    =     φ φ 0 0 ( ( . . . x x 1 2 ) ) · · · · · · φ φ K K − − 1 1 . . . ( ( x x 1 2 ) )    ∈ RN×K, (9.16) φ(cid:62)(x ) N φ (x ) φ (x ) 0 N K−1 N ··· whereΦ = φ (x )andφ : RD R. ij j i j → Example 9.4 (Feature Matrix for Second-order Polynomials) For a second-order polynomial and N training points x R,n = n ∈ 1,...,N,thefeaturematrixis  1 x x2 1 1 Φ =    1 . . x . . 2 x . . 2 2    . (9.17) . . .  1 x x2 N N WiththefeaturematrixΦdefinedin(9.16),thenegativelog-likelihood forthelinearregressionmodel(9.13)canbewrittenas 1 logp( ,θ) = (y Φθ)(cid:62)(y Φθ)+const. (9.18) − Y|X 2σ2 − − Comparing(9.18)withthenegativelog-likelihoodin(9.10b)forthe“feature-free” model, we immediately see we just need to replace X with Φ. SincebothX andΦareindependentoftheparametersθ thatwewishto maximumlikelihood optimize,wearriveimmediatelyatthemaximumlikelihoodestimate estimate θ = (Φ(cid:62)Φ)−1Φ(cid:62)y (9.19) ML forthelinearregressionproblemwithnonlinearfeaturesdefinedin(9.13). Remark. When we were working without features, we required X(cid:62)X to be invertible, which is the case when rk(X) = D, i.e., the columns of X Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.2 ParameterEstimation 297 are linearly independent. In (9.19), we therefore require Φ(cid:62)Φ RK×K ∈ tobeinvertible.Thisisthecaseifandonlyifrk(Φ) = K. ♦ Example 9.5 (Maximum Likelihood Polynomial Fit) Figure9.4 Polynomial 4 regression: 2 (a)dataset consistingof 0 (xn,yn)pairs, n=1,...,10; 2 − (b)maximum 4 likelihood − polynomialof 4 2 0 2 4 − − x degree4. y 4 2 0 2 − 4 − 4 2 0 2 4 − − x (a)Regressiondataset. y Trainingdata MLE (b)Polynomialofdegree4determinedbymaximumlikelihoodestimation. Consider the dataset in Figure 9.4(a). The dataset consists