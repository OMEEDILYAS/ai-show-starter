Y | |X Y (cid:90) = (cid:0) y φ(cid:62)(x )θ, σ2(cid:1) (cid:0) θ m , S (cid:1) dθ (9.57b) ∗ ∗ N N N | N | = (cid:0) y φ(cid:62)(x )m , φ(cid:62)(x )S φ(x )+σ2(cid:1) . (9.57c) ∗ ∗ N ∗ N ∗ N | E[y∗|X,Y,x∗]= The term φ(cid:62)(x ∗ )S N φ(x ∗ ) reflects the posterior uncertainty associated φ(cid:62)(x∗)mN = with the parameters θ. Note that S N depends on the training inputs φ(cid:62)(x∗)θ MAP. through Φ; see (9.43b). The predictive mean φ(cid:62)(x )m coincides with ∗ N thepredictionsmadewiththeMAPestimateθ . MAP Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.3 BayesianLinearRegression 309 Remark (Marginal Likelihood and Posterior Predictive Distribution). By replacingtheintegralin(9.57a),thepredictivedistributioncanbeequivalently written as the expectation E [p(y x ,θ)], where the expecθ|X,Y ∗ ∗ | tationistakenwithrespecttotheparameterposteriorp(θ , ). |X Y Writing the posterior predictive distribution in this way highlights a close resemblance to the marginal likelihood (9.42). The key difference between the marginal likelihood and the posterior predictive distribution are (i) the marginal likelihood can be thought of predicting the training targets y and not the test targets y , and (ii) the marginal likelihood av- ∗ erages with respect to the parameter prior and not the parameter posterior. ♦ Remark (Mean and Variance of Noise-Free Function Values). In many cases, we are not interested in the predictive distribution p(y , ,x ) ∗ ∗ |X Y of a (noisy) observation y . Instead, we would like to obtain the distribu- ∗ tion of the (noise-free) function values f(x ) = φ(cid:62)(x )θ. We determine ∗ ∗ the corresponding moments by exploiting the properties of means and variances,whichyields E[f(x ) , ] = E [φ(cid:62)(x )θ , ] = φ(cid:62)(x )E [θ , ] ∗ θ ∗ ∗ θ |X Y |X Y |X Y (9.58) = φ(cid:62)(x )m = m(cid:62)φ(x ), ∗ N N ∗ V [f(x ) , ] = V [φ(cid:62)(x )θ , ] θ ∗ θ ∗ |X Y |X Y = φ(cid:62)(x )V [θ , ]φ(x ) (9.59) ∗ θ ∗ |X Y = φ(cid:62)(x )S φ(x ). ∗ N ∗ We see that the predictive mean is the same as the predictive mean for noisy observations as the noise has mean 0, and the predictive variance onlydiffersbyσ2,whichisthevarianceofthemeasurementnoise:When we predict noisy function values, we need to include σ2 as a source of uncertainty, but this term is not needed for noise-free predictions. Here, theonlyremaininguncertaintystemsfromtheparameterposterior. ♦ Integratingout Remark (Distribution over Functions). The fact that we integrate out the parametersinduces parameters θ induces a distribution over functions: If we sample θ adistributionover i p(θ , ) from the parameter posterior, we obtain a single function r ∼ e- functions. aliza | t X ion Y θ(cid:62)φ( ). The mean function, i.e., the set of all expected function meanfunction values E [ i f( ) · θ, , ], of this distribution over functions is m(cid:62)φ( ). θ · | X Y N · The(marginal)variance,i.e.,thevarianceofthefunctionf( ),isgivenby φ(cid:62)( )S φ( ). · N · · ♦ Example 9.8 (Posterior over Functions) Let