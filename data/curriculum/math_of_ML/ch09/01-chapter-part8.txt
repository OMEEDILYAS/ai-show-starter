expected lie in the interval [ 2,2] − (two standard deviations around the mean value). Once a dataset , X Y isavailable,insteadofmaximizingthelikelihoodweseekparametersthat maximize the posterior distribution p(θ , ). This procedure is called |X Y maximuma maximumaposteriori(MAP)estimation. posteriori The posterior over the parameters θ, given the training data , , is X Y MAP obtainedbyapplyingBayes’theorem(Section6.3)as p( ,θ)p(θ) p(θ , ) = Y|X . (9.24) |X Y p( ) Y|X Since the posterior explicitly depends on the parameter prior p(θ), the priorwillhaveaneffectontheparametervectorwefindasthemaximizer of the posterior. We will see this more explicitly in the following. The parameter vector θ that maximizes the posterior (9.24) is the MAP MAP estimate. To find the MAP estimate, we follow steps that are similar in flavor to maximum likelihood estimation. We start with the log-transform and computethelog-posterioras logp(θ , ) = logp( ,θ)+logp(θ)+const, (9.25) |X Y Y|X Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.2 ParameterEstimation 301 wheretheconstantcomprisesthetermsthatareindependentofθ.Wesee thatthelog-posteriorin(9.25)isthesumofthelog-likelihoodp( ,θ) Y|X andthelog-priorlogp(θ)sothattheMAPestimatewillbea“compromise” between the prior (our suggestion for plausible parameter values before observingdata)andthedata-dependentlikelihood. TofindtheMAPestimateθ ,weminimizethenegativelog-posterior MAP distributionwithrespecttoθ,i.e.,wesolve θ argmin logp( ,θ) logp(θ) . (9.26) MAP ∈ θ {− Y|X − } Thegradientofthenegativelog-posteriorwithrespecttoθ is dlogp(θ , ) dlogp( ,θ) dlogp(θ) |X Y = Y|X , (9.27) − dθ − dθ − dθ where we identify the first term on the right-hand side as the gradient of thenegativelog-likelihoodfrom(9.11c). (cid:0) (cid:1) Witha(conjugate)Gaussianpriorp(θ) = 0, b2I ontheparameters N θ, the negative log-posterior for the linear regression setting (9.13), we obtainthenegativelogposterior 1 1 logp(θ , ) = (y Φθ)(cid:62)(y Φθ)+ θ(cid:62)θ+const. (9.28) − |X Y 2σ2 − − 2b2 Here,thefirsttermcorrespondstothecontributionfromthelog-likelihood, andthesecondtermoriginatesfromthelog-prior.Thegradientofthelogposteriorwithrespecttotheparametersθ isthen dlogp(θ , ) 1 1 |X Y = (θ(cid:62)Φ(cid:62)Φ y(cid:62)Φ)+ θ(cid:62). (9.29) − dθ σ2 − b2 We will find the MAP estimate θ by setting this gradient to 0(cid:62) and MAP solvingforθ .Weobtain MAP 1 1 (θ(cid:62)Φ(cid:62)Φ y(cid:62)Φ)+ θ(cid:62) = 0(cid:62) (9.30a) σ2 − b2 (cid:18) (cid:19) 1 1 1 θ(cid:62) Φ(cid:62)Φ+ I y(cid:62)Φ = 0(cid:62) (9.30b) ⇐⇒ σ2 b2 − σ2 (cid:18) σ2 (cid:19) θ(cid:62) Φ(cid:62)Φ+ I = y(cid:62)Φ (9.30c) ⇐⇒ b2 (cid:18) σ2 (cid:19)−1 θ(cid:62) = y(cid:62)Φ Φ(cid:62)Φ+ I (9.30d) ⇐⇒ b2 sothattheMAPestimateis(bytransposingbothsidesofthelastequality) Φ(cid:62)Φissymmetric, positivesemi (cid:18) σ2 (cid:19)−1 θ = Φ(cid:62)Φ+ I Φ(cid:62)y. (9.31) definite.The MAP b2 additionalterm in(9.31)isstrictly Comparing the MAP estimate in (9.31) with the maximum likelihood es- positivedefiniteso timate in (9.19), we see that the only difference between both solutions thattheinverse is the additional term σ2 I in the inverse matrix. This term ensures that exists. b2 (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 302 LinearRegression Φ(cid:62)Φ + σ2 I is symmetric and strictly positive definite (i.e., its inverse b2 exists and the MAP estimate is the unique solution of a system of linear equations).Moreover,itreflectstheimpactoftheregularizer. Example 9.6 (MAP Estimation for Polynomial Regression) InthepolynomialregressionexamplefromSection9.2.1,weplaceaGaus- (cid:0) (cid:1) sian prior p(θ) = 0, I on the parameters θ and determine the MAP N estimatesaccordingto(9.31).InFigure9.7,weshowboththemaximum likelihood and the MAP estimates for polynomials of degree 6 (left) and degree 8 (right). The prior (regularizer) does not play a significant role for the low-degree polynomial, but keeps the function relatively smooth for higher-degree polynomials. Although the MAP estimate can push the