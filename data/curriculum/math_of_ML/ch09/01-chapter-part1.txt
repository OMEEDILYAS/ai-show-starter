9 Linear Regression In the following, we will apply the mathematical concepts from Chapters 2, 5, 6, and 7 to solve linear regression (curve fitting) problems. In regression,weaimtofindafunctionf thatmapsinputsx RD tocorre- regression spondingfunctionvaluesf(x) R.Weassumewearegive ∈ nasetoftrain- ∈ inginputsx andcorrespondingnoisyobservationsy = f(x )+(cid:15),where n n n (cid:15) is an i.i.d. random variable that describes measurement/observation noise and potentially unmodeled processes (which we will not consider further in this chapter). Throughout this chapter, we assume zero-mean Gaussian noise. Our task is to find a function that not only models the training data, but generalizes well to predicting function values at input locations that are not part of the training data (see Chapter 8). An illustration of such a regression problem is given in Figure 9.1. A typical regression setting is given in Figure 9.1(a): For some input values x , we n observe (noisy) function values y = f(x )+(cid:15). The task is to infer the n n functionf thatgeneratedthedataandgeneralizeswelltofunctionvalues atnewinputlocations.ApossiblesolutionisgiveninFigure9.1(b),where wealsoshowthreedistributionscenteredatthefunctionvaluesf(x)that representthenoiseinthedata. Regression is a fundamental problem in machine learning, and regression problems appear in a diverse range of research areas and applicaFigure9.1 0.4 (a)Dataset; (b)possiblesolution 0.2 totheregression problem. 0.0 0.2 − 0.4 − 4 2 0 2 4 − − x y 0.4 0.2 0.0 0.2 − 0.4 − 4 2 0 2 4 − − x (a)Regressionproblem:observednoisyfunctionvaluesfromwhichwewishtoinferthe underlyingfunctionthatgeneratedthedata. y (b) Regression solution: possible function that could have generated the data (blue) withindicationofthemeasurementnoiseof the function value at the corresponding inputs(orangedistributions). 289 ThismaterialwillbepublishedbyCambridgeUniversityPressasMathematicsforMachineLearningbyMarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng.Thispre-publicationversionis freetoviewanddownloadforpersonaluseonly.Notforre-distribution,re-saleoruseinderivativeworks.(cid:13)cbyM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2019.https://mml-book.com. 290 LinearRegression tions, including time-series analysis (e.g., system identification), control and robotics (e.g., reinforcement learning, forward/inverse model learning), optimization (e.g., line searches, global optimization), and deeplearning applications (e.g., computer games, speech-to-text translation, image recognition, automatic video annotation). Regression is also a key ingredient of classification algorithms. Finding a regression function requiressolvingavarietyofproblems,includingthefollowing: Choice of the model (type) and the parametrization of the regresNormally,thetype sion function. Given a dataset, what function classes (e.g., polynomiofnoisecouldalso als) are good candidates for modeling the data, and what particular bea“modelchoice”, parametrization (e.g., degree of the polynomial) should we choose? butwefixthenoise Model selection, as discussed in Section 8.6, allows us to compare vartobeGaussianin thischapter. ious models to find the simplest model that explains the training data reasonablywell. Finding good parameters. Having chosen a model of the regression function,howdowefindgoodmodelparameters?Here,wewillneedto lookatdifferentloss/objectivefunctions(theydeterminewhata“good” fitis)andoptimizationalgorithmsthatallowustominimizethisloss. Overfitting and model selection. Overfitting is a problem when the regression function fits the training data “too well” but does not generalize to unseen test data. Overfitting typically occurs if the underlyingmodel(oritsparametrization)isoverlyflexibleandexpressive;see Section8.6.Wewilllookattheunderlyingreasonsanddiscusswaysto mitigatetheeffectofoverfittinginthecontextoflinearregression. Relationshipbetweenlossfunctionsandparameterpriors.Lossfunctions(optimizationobjectives)areoftenmotivatedandinducedbyprobabilistic models. We will look at the connection between loss functions andtheunderlyingpriorassumptionsthatinducetheselosses. Uncertaintymodeling.Inanypracticalsetting,wehaveaccesstoonly a finite, potentially large, amount of (training) data for selecting the model class and the corresponding parameters. Given that this finite amount of training data does not cover all possible scenarios, we may wanttodescribetheremainingparameteruncertaintytoobtainameasureofconfidenceofthemodel’spredictionattesttime;thesmallerthe trainingset,themoreimportantuncertaintymodeling.Consistentmodelingofuncertaintyequipsmodelpredictionswithconfidencebounds. In the following, we will be using the mathematical tools from Chapters 3, 5, 6 and 7 to solve linear regression problems. We will discuss maximumlikelihoodandmaximumaposteriori(MAP)estimationtofind optimalmodelparameters.Usingtheseparameterestimates,wewillhave a brief look at generalization errors and overfitting. Toward the end of thischapter,wewilldiscussBayesianlinearregression,whichallowsusto reasonaboutmodelparametersatahigherlevel,therebyremovingsome oftheproblemsencounteredinmaximumlikelihoodandMAPestimation.