4 2 0 2 − 4 − 4 2 0 2 4 − − x (a)Priordistributionoverfunctions. y (b) Samples from the prior distribution over functions. Let us consider a Bayesian linear regression problem with polynomials of degree 5. We choose a parameter prior p(θ) = (cid:0) 0, 1I (cid:1) . Figure 9.9 N 4 visualizestheinducedpriordistributionoverfunctions(shadedarea:dark gray:67%confidencebound;lightgray:95%confidencebound)induced bythisparameterprior,includingsomefunctionsamplesfromthisprior. A function sample is obtained by first sampling a parameter vector θ p(θ) and then computing f ( ) = θ(cid:62)φ( ). We used 200 input loi ∼ i · i · cations x [ 5,5] to which we apply the feature function φ( ). The ∗ ∈ − · uncertainty(representedbytheshadedarea)inFigure9.9issolelydueto theparameteruncertaintybecauseweconsideredthenoise-freepredictive distribution(9.40). So far, we looked at computing predictions using the parameter prior p(θ). However, when we have a parameter posterior (given some training data , ), the same principles for prediction and inference hold X Y as in (9.37) – we just need to replace the prior p(θ) with the posterior (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 306 LinearRegression p(θ , ). In the following, we will derive the posterior distribution in |X Y detailbeforeusingittomakepredictions. 9.3.3 Posterior Distribution Given a training set of inputs x RD and corresponding observations n y R, n = 1,...,N, we comp ∈ ute the posterior over the parameters n ∈ usingBayes’theoremas p( ,θ)p(θ) p(θ , ) = Y|X , (9.41) |X Y p( ) Y|X where is the set of training inputs and the collection of correspondX Y ing training targets. Furthermore, p( ,θ) is the likelihood, p(θ) the Y|X parameterprior,and (cid:90) p( ) = p( ,θ)p(θ)dθ = E [p( ,θ)] (9.42) θ Y|X Y|X Y|X marginallikelihood the marginal likelihood/evidence, which is independent of the parameters evidence θ and ensures that the posterior is normalized, i.e., it integrates to 1. We Themarginal can think of the marginal likelihood as the likelihood averaged over all likelihoodisthe possibleparametersettings(withrespecttothepriordistributionp(θ)). expectedlikelihood undertheparameter Theorem 9.1 (Parameter Posterior). In our model (9.35), the parameter prior. posterior(9.41)canbecomputedinclosedformas (cid:0) (cid:1) p(θ , ) = θ m , S , (9.43a) N N |X Y N | S = (S−1+σ−2Φ(cid:62)Φ)−1, (9.43b) N 0 m = S (S−1m +σ−2Φ(cid:62)y), (9.43c) N N 0 0 wherethesubscriptN indicatesthesizeofthetrainingset. Proof Bayes’ theorem tells us that the posterior p(θ , ) is propor- |X Y tionaltotheproductofthelikelihoodp( ,θ)andthepriorp(θ): Y|X p( ,θ)p(θ) Posterior p(θ , ) = Y|X (9.44a) |X Y p( ) Y|X Likelihood p( ,θ) = (cid:0) y Φθ, σ2I (cid:1) (9.44b) Y|X N | (cid:0) (cid:1) Prior p(θ) = θ m , S . (9.44c) 0 0 N | Instead of looking at the product of the prior and the likelihood, we can transform the problem into log-space and solve for the mean and covarianceoftheposteriorbycompletingthesquares. Thesumofthelog-priorandthelog-likelihoodis log (cid:0) y Φθ, σ2I (cid:1) +log (cid:0) θ m , S (cid:1) (9.45a) 0 0 N | N | = 1(cid:0) σ−2(y Φθ)(cid:62)(y Φθ)+(θ m )(cid:62)S−1(θ m ) (cid:1) +const −2 − − − 0 0 − 0 (9.45b) Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.3 BayesianLinearRegression 307 where the constant contains terms independent of θ. We will ignore the constantinthefollowing.Wenowfactorize(9.45b),whichyields