n R, n = 1,...,N. Th ∈ e ∈ Probabilistic corresponding graphical model is given in Figure 9.3. Note that y and y i j graphicalmodelfor are conditionally independent given their respective inputs x ,x so that i j linearregression. thelikelihoodfactorizesaccordingto Observedrandom variablesare p( ,θ) = p(y ,...,y x ,...,x ,θ) (9.5a) shaded, 1 N 1 N Y|X | deterministic/ N N knownvaluesare = (cid:89) p(y x ,θ) = (cid:89) (cid:0) y x(cid:62)θ, σ2(cid:1) , (9.5b) withoutcircles. n | n N n | n n=1 n=1 θ where we defined := x ,...,x and := y ,...,y as the sets 1 N 1 N σ X { } Y { } of training inputs and corresponding targets, respectively. The likelihood x n y n and the factors p(y n | x n ,θ) are Gaussian due to the noise distribution; see(9.3). n=1,...,N In the following, we will discuss how to find optimal parameters θ∗ RD for the linear regression model (9.4). Once the parameters θ∗ ar ∈ e found, we can predict function values by using this parameter estimate in (9.4) so that at an arbitrary test input x the distribution of the corre- ∗ spondingtargety is ∗ p(y x ,θ∗) = (cid:0) y x(cid:62)θ∗, σ2(cid:1) . (9.6) ∗ | ∗ N ∗ | ∗ In the following, we will have a look at parameter estimation by maximizing the likelihood, a topic that we already covered to some degree in Section8.3. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.2 ParameterEstimation 293 9.2.1 Maximum Likelihood Estimation Awidelyusedapproachtofindingthedesiredparametersθ ML ismaximum maximumlikelihood likelihood estimation, where we find parameters θ that maximize the estimation ML likelihood (9.5b). Intuitively, maximizing the likelihood means maximiz- Maximizingthe ingthepredictivedistributionofthetrainingdatagiventhemodelparam- likelihoodmeans maximizingthe eters.Weobtainthemaximumlikelihoodparametersas predictive θ = argmaxp( ,θ). (9.7) distributionofthe ML θ Y|X (training)data giventhe Remark. Thelikelihoodp(y x,θ)isnotaprobabilitydistributioninθ:It parameters. | is simply a function of the parameters θ but does not integrate to 1 (i.e., Thelikelihoodisnot it is unnormalized), and may not even be integrable with respect to θ. aprobability However, the likelihood in (9.7) is a normalized probability distribution distributioninthe parameters. iny. ♦ To find the desired parameters θ that maximize the likelihood, we ML typically perform gradient ascent (or gradient descent on the negative likelihood). In the case of linear regression we consider here, however, Sincethelogarithm a closed-form solution exists, which makes iterative gradient descent un- isa(strictly) monotonically necessary. In practice, instead of maximizing the likelihood directly, we increasingfunction, apply the log-transformation to the likelihood function and minimize the theoptimumofa negativelog-likelihood. functionf is identicaltothe Remark (Log-Transformation). Since the likelihood (9.5b) is a product of optimumoflogf. N Gaussiandistributions,thelog-transformationisusefulsince(a)itdoes notsufferfromnumericalunderflow,and(b)thedifferentiationruleswill turn out simpler. More specifically, numerical underflow will be a problem when we multiply N probabilities, where N is the number of data points, since we cannot represent very small numbers, such as 10−256. Furthermore, the log-transform will turn the product into a sum of logprobabilities such that the corresponding gradient is a sum of individual gradients, instead of a repeated application of the product rule (5.46) to computethegradientofaproductofN terms. ♦ To find the optimal