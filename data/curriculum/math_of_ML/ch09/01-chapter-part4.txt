parameters θ of our linear regression problem, ML weminimizethenegativelog-likelihood N N (cid:89) (cid:88) logp( ,θ) = log p(y x ,θ) = logp(y x ,θ), (9.8) n n n n − Y|X − | − | n=1 n=1 where we exploited that the likelihood (9.5b) factorizes over the number ofdatapointsduetoourindependenceassumptiononthetrainingset. In the linear regression model (9.4), the likelihood is Gaussian (due to theGaussianadditivenoiseterm),suchthatwearriveat 1 logp(y x ,θ) = (y x(cid:62)θ)2+const, (9.9) n | n −2σ2 n − n wheretheconstantincludesalltermsindependentofθ.Using(9.9)inthe (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 294 LinearRegression negativelog-likelihood(9.8),weobtain(ignoringtheconstantterms) N 1 (cid:88) (θ) := (y x(cid:62)θ)2 (9.10a) L 2σ2 n − n n=1 1 1 = (y Xθ)(cid:62)(y Xθ) = y Xθ 2, (9.10b) 2σ2 − − 2σ2(cid:107) − (cid:107) Thenegative where we define the design matrix X := [x 1 ,...,x N ](cid:62) RN×D as the log-likelihood collectionoftraininginputsandy := [y ,...,y ](cid:62) RN ∈ asavectorthat 1 N functionisalso collectsalltrainingtargets.Notethatthenthrowin ∈ thedesignmatrixX callederrorfunction. correspondstothetraininginputx .In(9.10b),weusedthefactthatthe designmatrix n sumofsquarederrorsbetweentheobservationsy andthecorresponding Thesquarederroris n modelpredictionx(cid:62)θ equalsthesquareddistancebetweeny andXθ. oftenusedasa n measureofdistance. With(9.10b),wehavenowaconcreteformofthenegativelog-likelihood Recallfrom functionweneedtooptimize.Weimmediatelyseethat(9.10b)isquadratic Section3.1that in θ. This means that we can find a unique global solution θ for mini- (cid:107)x(cid:107)2=x(cid:62)xifwe ML mizing the negative log-likelihood . We can find the global optimum by choosethedot L productastheinner computingthegradientof ,settingitto0andsolvingforθ. L product. Using the results from Chapter 5, we compute the gradient of with L respecttotheparametersas (cid:18) (cid:19) d d 1 L = (y Xθ)(cid:62)(y Xθ) (9.11a) dθ dθ 2σ2 − − 1 d (cid:16) (cid:17) = y(cid:62)y 2y(cid:62)Xθ+θ(cid:62)X(cid:62)Xθ (9.11b) 2σ2dθ − 1 = ( y(cid:62)X +θ(cid:62)X(cid:62)X) R1×D. (9.11c) σ2 − ∈ The maximum likelihood estimator θ solves dL = 0(cid:62) (necessary optiML dθ Ignoringthe malitycondition)andweobtain possibilityof duplicatedata d L = 0(cid:62) (9.11c) θ(cid:62) X(cid:62)X = y(cid:62)X (9.12a) points,rk(X)=D dθ ⇐⇒ ML ifN (cid:62)D,i.e.,we θ(cid:62) = y(cid:62)X(X(cid:62)X)−1 (9.12b) donothavemore ⇐⇒ ML parametersthan θ = (X(cid:62)X)−1X(cid:62)y. (9.12c) ML datapoints. ⇐⇒ Wecouldright-multiplythefirstequationby(X(cid:62)X)−1 becauseX(cid:62)X is positivedefiniteifrk(X) = D,whererk(X)denotestherankofX. Remark. Settingthegradientto0(cid:62) isanecessaryandsufficientcondition, and we obtain a global minimum since the Hessian 2 (θ) = X(cid:62)X RD×D ispositivedefinite. ∇θL ∈ ♦ Remark. Themaximumlikelihoodsolutionin(9.12c)requiresustosolve a system of linear equations of the form Aθ = b with A = (X(cid:62)X) and b = X(cid:62)y. ♦ Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.2 ParameterEstimation 295 Example 9.2 (Fitting Lines) LetushavealookatFigure9.2,whereweaimtofitastraightlinef(x) = θx,whereθ isanunknownslope,toadatasetusingmaximumlikelihood estimation. Examples of functions in this model class (straight lines) are shown in Figure 9.2(a). For the dataset shown in Figure 9.2(b), we find the maximum likelihood estimate of the slope parameter θ using (9.12c) andobtainthemaximumlikelihoodlinearfunctioninFigure9.2(c). Maximum Likelihood Estimation with Features So far, we considered the linear regression setting described in (9.4), which allowed us to fit straight lines to data using maximum likelihood estimation. However, straight lines are not sufficiently expressive when it Linearregression comestofittingmoreinterestingdata.Fortunately,linearregressionoffers refersto“linear-inthe-parameters” usawaytofitnonlinearfunctionswithinthelinearregressionframework: regressionmodels, Since “linear regression” only refers to “linear in the parameters”, we can buttheinputscan perform an arbitrary nonlinear transformation φ(x) of the inputs x and undergoany then linearly combine the components of this transformation. The corre- nonlinear spondinglinearregressionmodelis transformation. p(y x,θ) = (cid:0) y φ(cid:62)(x)θ, σ2(cid:1) | N | K (cid:88) −1 (9.13) y = φ(cid:62)(x)θ+(cid:15) = θ φ (x)+(cid:15), k k ⇐⇒ k=0