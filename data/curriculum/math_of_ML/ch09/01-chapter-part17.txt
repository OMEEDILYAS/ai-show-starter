◦ linearregressionmodelandσ theactivationfunction.Notethatalthough we are talking about “generalized linear models”, the outputs y are no longer linear in the parameters θ. In logistic regression, we choose the logisticregression logistic sigmoid σ(f) = 1 [0,1], which can be interpreted as the logisticsigmoid 1+exp(−f) ∈ probability of observing y = 1 of a Bernoulli random variable y 0,1 . ∈ { } The function σ( ) is called transfer function or activation function, and its transferfunction · inverse is called the canonical link function. From this perspective, it is activationfunction alsoclearthatgeneralizedlinearmodelsarethebuildingblocksof(deep) canonicallink function feedforward neural networks: If we consider a generalized linear model Forordinarylinear y = σ(Ax+b),whereAisaweightmatrixandbabiasvector,weidenregressionthe tify this generalized linear model as a single-layer neural network with activationfunction activationfunctionσ( ).Wecannowrecursivelycomposethesefunctions wouldsimplybethe · identity. via Agreatpostonthe x = f (x ) k+1 k k (9.72) relationbetween f (x ) = σ (A x +b ) GLMsanddeep k k k k k k networksis for k = 0,...,K 1, where x are the input features and x = y are availableat 0 K − the observed outputs, such that f f is a K-layer deep neural https://tinyurl. K−1 ◦···◦ 0 com/glm-dnn. network. Therefore, the building blocks of this deep neural network are thegeneralizedlinearmodelsdefinedin(9.72).Neuralnetworks(Bishop, (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 316 LinearRegression 1995;Goodfellowetal.,2016)aresignificantlymoreexpressiveandflexiblethanlinearregressionmodels.However,maximumlikelihoodparameterestimationisanon-convexoptimizationproblem,andmarginalization oftheparametersinafullyBayesiansettingisanalyticallyintractable. We briefly hinted at the fact that a distribution over parameters inGaussianprocess duces a distribution over regression functions. Gaussian processes (Rasmussen and Williams, 2006) are regression models where the concept of a distribution over function is central. Instead of placing a distribution over parameters, a Gaussian process places a distribution directly on the space of functions without the “detour” via the parameters. To do so, the kerneltrick Gaussian process exploits the kernel trick (Scho¨lkopf and Smola, 2002), which allows us to compute inner products between two function values f(x ),f(x ) only by looking at the corresponding input x ,x . A Gausi j i j sian process is closely related to both Bayesian linear regression and support vector regression but can also be interpreted as a Bayesian neural network with a single hidden layer where the number of units tends to infinity(Neal,1996;Williams,1997).ExcellentintroductionstoGaussian processes can be found in MacKay (1998) and Rasmussen and Williams (2006). WefocusedonGaussianparameterpriorsinthediscussionsinthischapter,becausetheyallowforclosed-forminferenceinlinearregressionmodels. However, even in a regression setting with Gaussian likelihoods, we maychooseanon-Gaussianprior.Considerasetting,wheretheinputsare x RD andourtrainingsetissmallandofsizeN D.Thismeansthat ∈ (cid:28) the regression problem is underdetermined. In this case, we can choose a parameter prior that enforces sparsity, i.e., a prior that tries to set as variableselection many parameters to 0 as possible (variable selection). This prior provides astrongerregularizerthantheGaussianprior,whichoftenleadstoanincreasedpredictionaccuracyandinterpretabilityofthemodel.TheLaplace prior is one example that is frequently used for this purpose. A linear regression model with the Laplace prior on the parameters is equivalent to LASSO linear regression with L1 regularization (LASSO) (Tibshirani, 1996). The Laplacedistributionissharplypeakedatzero(itsfirstderivativeisdiscontinuous) and it concentrates its probability mass closer to zero than the Gaussian distribution, which encourages parameters to be 0. Therefore, the nonzero parameters are relevant for the regression problem, which is thereasonwhywealsospeakof“variableselection”. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.