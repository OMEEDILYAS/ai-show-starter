of N = 10 pairs(x ,y ),wherex [ 5,5]andy = sin(x /5)+cos(x )+(cid:15), n n n n n n (cid:0) (cid:1) ∼ U − − where(cid:15) 0, 0.22 . ∼ N We fit a polynomial of degree 4 using maximum likelihood estimation, i.e.,parametersθ aregivenin(9.19).Themaximumlikelihoodestimate ML yields function values φ(cid:62)(x )θ at any test location x . The result is ∗ ML ∗ showninFigure9.4(b). Estimating the Noise Variance Thus far, we assumed that the noise variance σ2 is known. However, we canalsousetheprincipleofmaximumlikelihoodestimationtoobtainthe maximum likelihood estimator σ2 for the noise variance. To do this, we ML follow the standard procedure: We write down the log-likelihood, compute its derivative with respect to σ2 > 0, set it to 0, and solve. The log-likelihoodisgivenby N log p( ,θ,σ2) = (cid:88) log (cid:0) y φ(cid:62)(x )θ, σ2(cid:1) (9.20a) n n Y|X N | n=1 N (cid:18) (cid:19) (cid:88) 1 1 1 = log(2π) logσ2 (y φ(cid:62)(x )θ)2 (9.20b) −2 − 2 − 2σ2 n − n n=1 N 1 (cid:88) N = logσ2 (y φ(cid:62)(x )θ)2+const. (9.20c) − 2 − 2σ2 n − n n=1 (cid:124) (cid:123)(cid:122) (cid:125) =:s (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 298 LinearRegression Thepartialderivativeofthelog-likelihoodwithrespecttoσ2 isthen ∂logp( ,θ,σ2) N 1 Y|X = + s = 0 (9.21a) ∂σ2 −2σ2 2σ4 N s = (9.21b) ⇐⇒ 2σ2 2σ4 sothatweidentify s 1 (cid:88) N σ2 = = (y φ(cid:62)(x )θ)2. (9.22) ML N N n − n n=1 Therefore, the maximum likelihood estimate of the noise variance is the empirical mean of the squared distances between the noise-free function values φ(cid:62)(x )θ and the corresponding noisy observations y at input lon n cationsx . n 9.2.2 Overfitting in Linear Regression We just discussed how to use maximum likelihood estimation to fit linear models (e.g., polynomials) to data. We can evaluate the quality of the model by computing the error/loss incurred. One way of doing this is to compute the negative log-likelihood (9.10b), which we minimized to determine the maximum likelihood estimator. Alternatively, given that the noise parameter σ2 is not a free model parameter, we can ignore the scaling by 1/σ2, so that we end up with a squared-error-loss function rootmeansquare y Φθ 2 .Insteadofusingthissquaredloss,weoftenusetherootmean (cid:107) − (cid:107) error squareerror(RMSE) RMSE (cid:118) (cid:114) (cid:117) N 1 y Φθ 2 = (cid:117) (cid:116) 1 (cid:88) (y φ(cid:62)(x )θ)2, (9.23) N (cid:107) − (cid:107) N n − n n=1 which (a) allows us to compare errors of datasets with different sizes TheRMSEis and (b) has the same scale and the same units as the observed funcnormalized. tion values y . For example, if we fit a model that maps post-codes (x n is given in latitude, longitude) to house prices (y-values are EUR) then the RMSE is also measured in EUR, whereas the squared error is given Thenegative in EUR2. If we choose to include the factor σ2 from the original negative log-likelihoodis log-likelihood (9.10b), then we end up with a unitless objective, i.e., in unitless. theprecedingexample,ourobjectivewouldnolongerbeinEURorEUR2. For model selection (see Section 8.6), we can use the RMSE (or the negativelog-likelihood)todeterminethebestdegreeofthepolynomialby