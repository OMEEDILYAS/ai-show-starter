information can be critical when we use these predictions in a decision-making system, where bad decisions can have significant consequences (e.g., in reinforcement learning orrobotics). (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 312 LinearRegression 9.3.5 Computing the Marginal Likelihood InSection8.6.2,wehighlightedtheimportanceofthemarginallikelihood for Bayesian model selection. In the following, we compute the marginal likelihood for Bayesian linear regression with a conjugate Gaussian prior ontheparameters,i.e.,exactlythesettingwehavebeendiscussinginthis chapter. Justtorecap,weconsiderthefollowinggenerativeprocess: (cid:0) (cid:1) θ m , S (9.60a) 0 0 ∼ N y x ,θ (cid:0) x(cid:62)θ, σ2(cid:1) , (9.60b) n | n ∼ N n Themarginal n = 1,...,N.Themarginallikelihoodisgivenby likelihoodcanbe (cid:90) interpretedasthe p( ) = p( ,θ)p(θ)dθ (9.61a) expectedlikelihood Y|X Y|X undertheprior,i.e., (cid:90) E θ [p(Y|X,θ)]. = N (cid:0) y | Xθ, σ2I (cid:1) N (cid:0) θ | m 0 , S 0 (cid:1) dθ, (9.61b) whereweintegrateoutthemodelparametersθ.Wecomputethemarginal likelihood in two steps: First, we show that the marginal likelihood is Gaussian (as a distribution in y); second, we compute the mean and covarianceofthisGaussian. 1. ThemarginallikelihoodisGaussian:FromSection6.5.2,weknowthat (i)theproductoftwoGaussianrandomvariablesisan(unnormalized) Gaussian distribution, and (ii) a linear transformation of a Gaussian randomvariableisGaussiandistributed.In(9.61b),werequirealinear (cid:0) (cid:1) (cid:0) (cid:1) transformationtobring y Xθ, σ2I intotheform θ µ, Σ for N | N | someµ,Σ.Oncethisisdone,theintegralcanbesolvedinclosedform. The result is the normalizing constant of the product of the two Gaussians.ThenormalizingconstantitselfhasGaussianshape;see(6.76). 2. Mean and covariance. We compute the mean and covariance matrix ofthemarginallikelihoodbyexploitingthestandardresultsformeans andcovariancesofaffinetransformationsofrandomvariables;seeSection6.4.4.Themeanofthemarginallikelihoodiscomputedas E[ ] = E [Xθ+(cid:15)] = XE [θ] = Xm . (9.62) θ,(cid:15) θ 0 Y|X (cid:0) (cid:1) Note that (cid:15) 0, σ2I is a vector of i.i.d. random variables. The ∼ N covariancematrixisgivenas Cov[ ] = Cov [Xθ+(cid:15)] = Cov [Xθ]+σ2I (9.63a) θ,(cid:15) θ Y|X = XCov [θ]X(cid:62)+σ2I = XS X(cid:62)+σ2I. (9.63b) θ 0 Hence,themarginallikelihoodis N 1 p( ) = (2π)− 2 det(XS 0 X(cid:62)+σ2I)− 2 (9.64a) Y|X exp (cid:0) 1(y Xm )(cid:62)(XS X(cid:62)+σ2I)−1(y Xm ) (cid:1) · − 2 − 0 0 − 0 Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 9.4 MaximumLikelihoodasOrthogonalProjection 313 4 Figure9.12 Geometric interpretationof 2 leastsquares. (a)Dataset; 0 (b)maximum likelihoodsolution 2 − interpretedasa projection. 4 − 4 2 0 2 4 − − x y 4 2 0 2 − 4 − 4 2 0 2 4 − − x (a) Regression dataset consisting of noisy observationsyn (blue)offunctionvaluesf(xn) atinputlocationsxn. y Projection Observations Maximumlikelihoodestimate (b) The orange dots are the projections of the noisy observations (blue dots) onto the lineθ ML x.Themaximumlikelihoodsolutionto a linear regression problem finds a subspace (line) onto which the overall projection error(orangelines)oftheobservationsisminimized. = (cid:0) y Xm , XS X(cid:62)+σ2I (cid:1) . (9.64b) 0 0 N | Given the close connection with the posterior predictive distribution (see Remark on Marginal Likelihood and Posterior Predictive Distribution earlierinthissection),thefunctionalformofthemarginallikelihoodshould notbetoosurprising. 9.4 Maximum Likelihood as Orthogonal Projection Having crunched through much algebra to derive maximum likelihood and MAP estimates, we will now provide a geometric interpretation of maximumlikelihoodestimation.Letusconsiderasimplelinearregression setting y = xθ+(cid:15), (cid:15) (cid:0) 0, σ2(cid:1) , (9.65) ∼ N in which we consider linear functions f : R R that go through the → origin(weomitfeatureshereforclarity).Theparameterθdeterminesthe slopeoftheline.Figure9.12(a)showsaone-dimensionaldataset. With a training data set (x ,y ),...,(x ,y ) we recall the results 1 1 N N { } from Section 9.2.1 and obtain the maximum