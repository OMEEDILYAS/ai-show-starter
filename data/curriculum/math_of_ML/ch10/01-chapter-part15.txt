(or MAP) estimation should only be a function of the data x and the model parameters, butmustnotdependonthelatentvariables. ♦ From Section 6.5, we know that a Gaussian random variable z and a linear/affine transformation x = Bz of it are jointly Gaussian dis- (cid:0) (cid:1) tributed. We already know the marginals p(z) = z 0, I and p(x) = (cid:0) x µ, BB(cid:62)+σ2I (cid:1) .Themissingcross-covaria N ncei | sgivenas N | Cov[x,z] = Cov [Bz+µ] = BCov [z,z] = B. (10.71) z z Therefore, the probabilistic model of PPCA, i.e., the joint distribution of latentandobservedrandomvariablesisexplicitlygivenby (cid:18)(cid:20) x (cid:21) (cid:12) (cid:12) (cid:20) µ (cid:21) (cid:20) BB(cid:62)+σ2I B (cid:21)(cid:19) p(x,z B,µ,σ2) = (cid:12) , , (10.72) | N z (cid:12) 0 B(cid:62) I with a mean vector of length D + M and a covariance matrix of size (D+M) (D+M). × 10.7.3 Posterior Distribution The joint Gaussian distribution p(x,z B,µ,σ2) in (10.72) allows us to | determine the posterior distribution p(z x) immediately by applying the | Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.8 FurtherReading 343 rules of Gaussian conditioning fromSection 6.5.1. The posterior distributionofthelatentvariablegivenanobservationxisthen (cid:0) (cid:1) p(z x) = z m, C , (10.73) | N | m = B(cid:62)(BB(cid:62)+σ2I)−1(x µ), (10.74) − C = I B(cid:62)(BB(cid:62)+σ2I)−1B. (10.75) − Note that the posterior covariance does not depend on the observed data x. For a new observation x in data space, we use (10.73) to determine ∗ the posteriordistribution of the correspondinglatent variablez . The co- ∗ variance matrix C allows us to assess how confident the embedding is. A covariancematrixC withasmalldeterminant(whichmeasuresvolumes) tells us that the latent embedding z is fairly certain. If we obtain a pos- ∗ terior distribution p(z x ) with much variance, we may be faced with ∗ ∗ | an outlier. However, we can explore this posterior distribution to understand what other data points x are plausible under this posterior. To do this, we exploit the generative process underlying PPCA, which allows us to explore the posterior distribution on the latent variables by generating newdatathatisplausibleunderthisposterior: 1. Samplealatentvariablez p(z x )fromtheposteriordistribution ∗ ∗ ∼ | overthelatentvariables(10.73). 2. Sampleareconstructedvectorx˜ p(x z ,B,µ,σ2)from(10.64). ∗ ∗ ∼ | If we repeat this process many times, we can explore the posterior distribution (10.73) on the latent variables z and its implications on the ∗ observeddata.Thesamplingprocesseffectivelyhypothesizesdata,which isplausibleundertheposteriordistribution. 10.8 Further Reading WederivedPCAfromtwoperspectives:(a)maximizingthevarianceinthe projected space; (b) minimizing the average reconstruction error. However,PCAcanalsobeinterpretedfromdifferentperspectives.Letusrecap what we have done: We took high-dimensional data x RD and used a matrix B(cid:62) to find a lower-dimensional representation ∈ z RM. The ∈ columnsofBaretheeigenvectorsofthedatacovariancematrixSthatare associated with the largest eigenvalues. Once we have a low-dimensional representationz,wecangetahigh-dimensionalversionofit(intheoriginal data space) as x x˜ = Bz = BB(cid:62)x RD, where BB(cid:62) is a ≈ ∈ projectionmatrix. We can also think of PCA as a linear auto-encoder as illustrated in Fig- auto-encoder ure10.16.Anauto-encoderencodesthedatax n RD toacodez n RM code ∈ ∈ and decodes it to a x˜ similar to x . The mapping from the data to the n n codeiscalledtheencoder,andthemappingfromthecodebacktotheorig- encoder inaldataspaceiscalledthedecoder.Ifweconsiderlinearmappingswhere decoder (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 344 DimensionalityReductionwithPrincipalComponentAnalysis Figure10.16 PCA Original canbeviewedasa RD