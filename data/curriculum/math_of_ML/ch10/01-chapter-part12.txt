a new eigenvector/eigenvalue equation: λ remains eigenm value, which confirms our results from Section 4.5.3 that the nonzero eigenvaluesofXX(cid:62) equalthenonzeroeigenvaluesofX(cid:62)X.Weobtain the eigenvector of the matrix 1X(cid:62)X RN×N associated with λ as N ∈ m c := X(cid:62)b . Assuming we have no duplicate data points, this matrix m m hasrankN andisinvertible.Thisalsoimpliesthat 1X(cid:62)X hasthesame N (nonzero)eigenvaluesasthedatacovariancematrixS.Butthisisnowan N N matrix, so that we can compute the eigenvalues and eigenvectors × muchmoreefficientlythanfortheoriginalD Ddatacovariancematrix. Now that we have the eigenvectors of 1X ×(cid:62)X, we are going to reN cover the original eigenvectors, which we still need for PCA. Currently, weknowtheeigenvectorsof 1X(cid:62)X.Ifweleft-multiplyoureigenvalue/ N eigenvectorequationwithX,weget 1 XX(cid:62)Xc = λ Xc (10.57) N m m m (cid:124) (cid:123)(cid:122) (cid:125) S and we recover the data covariance matrix again. This now also means thatwerecoverXc asaneigenvectorofS. m Remark. IfwewanttoapplythePCAalgorithmthatwediscussedinSection 10.6, we need to normalize the eigenvectors Xc of S so that they m havenorm1. ♦ 10.6 Key Steps of PCA in Practice In the following, we will go through the individual steps of PCA using a running example, which is summarized in Figure 10.11. We are given a two-dimensional dataset (Figure 10.11(a)), and we want to use PCA to projectitontoaone-dimensionalsubspace. 1. Mean subtraction We start by centering the data by computing the mean µ of the dataset and subtracting it from every single data point. Thisensuresthatthedatasethasmean0(Figure10.11(b)).Meansubtractionisnotstrictlynecessarybutreducestheriskofnumericalproblems. 2. Standardization Dividethedatapointsbythestandarddeviationσ d of the dataset for every dimension d = 1,...,D. Now the data is unit free, and it has variance 1 along each axis, which is indicated by the standardization twoarrowsinFigure10.11(c).Thisstepcompletesthestandardization ofthedata. 3. Eigendecomposition of the covariance matrix Compute the data covariancematrixanditseigenvaluesandcorrespondingeigenvectors. Since the covariance matrix is symmetric, the spectral theorem (Theorem 4.15) states that we can find an ONB of eigenvectors. In Figure10.11(d),theeigenvectorsarescaledbythemagnitudeofthecorDraft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.6 KeyStepsofPCAinPractice 337 Figure10.11 Steps 5.0 ofPCA.(a)Original dataset; 2.5 (b)centering; (c)divideby 0.0 standarddeviation; 2.5 (d)eigendecomposi- − tion;(e)projection; 0 5 x1 (f)mappingbackto originaldataspace. 2x 5.0 2.5 0.0 2.5 − 0 5 x1 (a)Originaldataset. 2x 5.0 2.5 0.0 2.5 − 0 5 x1 (b) Step 1: Centering by subtracting the mean from each datapoint. 2x (c) Step 2: Dividing by the standard deviation to make the data unit free. Data has variance1alongeachaxis. 5.0 2.5 0.0 2.5 − 0 5 x1 2x 5.0 2.5 0.0 2.5 − 0 5 x1 (d)Step3:Computeeigenvaluesandeigenvectors(arrows) of the data covariance matrix (ellipse). 2x 5.0 2.5 0.0 2.5 − 0 5 x1 (e) Step 4: Project data onto theprincipalsubspace. 2x (f) Undo the standardization andmoveprojecteddataback into the original data space from(a). respondingeigenvalue.Thelongervectorspanstheprincipalsubspace, which we denote by U. The data covariance matrix is represented by theellipse. 4. Projection Wecanprojectanydatapointx RD ontotheprincipal ∗ ∈ subspace: To get this right, we need to standardize x using the mean ∗ µ andstandarddeviationσ ofthetrainingdatainthedthdimension, d d respectively,sothat x(d) µ x(d) ∗ − d , d = 1,...,D, (10.58) ∗ ← σ d wherex(d) isthedthcomponentofx .Weobtaintheprojectionas ∗ ∗ x˜ = BB(cid:62)x (10.59) ∗ ∗ withcoordinates z = B(cid:62)x (10.60) ∗ ∗ with respect to the basis of the principal subspace. Here, B is the matrix that contains the eigenvectors that are