streaming setting, where data arrives sequentially, it is recommendedtousetheiterativeexpectationmaximization(EM)algorithmfor maximumlikelihoodestimation(Roweis,1998). To determine the dimensionality of the latent variables (the length of thecode,thedimensionalityofthelower-dimensionalsubspaceontowhich we project the data), Gavish and Donoho (2014) suggest the heuristic that, if we can estimate the noise variance σ2 of the data, we should (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 346 DimensionalityReductionwithPrincipalComponentAnalysis √ discard all singular values smaller than 4σ√ D. Alternatively, we can use 3 (nested) cross-validation (Section 8.6.1) or Bayesian model selection criteria (discussed in Section 8.6.2) to determine a good estimate of the intrinsicdimensionalityofthedata(Minka,2001b). SimilartoourdiscussiononlinearregressioninChapter9,wecanplace a prior distribution on the parameters of the model and integrate them out. By doing so, we (a) avoid point estimates of the parameters and the issues that come with these point estimates (see Section 8.6) and (b) allowforanautomaticselectionoftheappropriatedimensionalityM ofthe BayesianPCA latentspace.InthisBayesianPCA,whichwasproposedbyBishop(1999), a prior p(µ,B,σ2) is placed on the model parameters. The generative processallowsustointegratethemodelparametersoutinsteadofconditioningonthem,whichaddressesoverfittingissues.Sincethisintegration isanalyticallyintractable,Bishop(1999)proposestouseapproximateinference methods, such as MCMC or variational inference. We refer to the workbyGilksetal.(1996)andBleietal.(2017)formoredetailsonthese approximateinferencetechniques. (cid:0) In PPCA, we considered the linear model p(x z ) = x Bz + n n n n (cid:1) (cid:0) (cid:1) | N | µ, σ2I with prior p(z ) = 0, I , where all observation dimensions n N are affected by the same amount of noise. If we allow each observation factoranalysis dimension d to have a different variance σ2, we obtain factor analysis d (FA) (Spearman, 1904; Bartholomew et al., 2011). This means that FA gives the likelihood some more flexibility than PPCA, but still forces the Anoverlyflexible data to be explained by the model parameters B, µ.However, FA no likelihoodwouldbe longer allows for a closed-form maximum likelihood solution so that we abletoexplainmore need to use an iterative scheme, such as the expectation maximization thanjustthenoise. algorithm, to estimate the model parameters. While in PPCA all stationary points are global optima, this no longer holds for FA. Compared to PPCA,FAdoesnotchangeifwescalethedata,butitdoesreturndifferent solutionsifwerotatethedata. independent An algorithm that is also closely related to PCA is independent comcomponentanalysis ponent analysis (ICA (Hyvarinen et al., 2001)). Starting again with the (cid:0) (cid:1) ICA latent-variable perspective p(x z ) = x Bz +µ, σ2I we now n n n n | N | change the prior on z to non-Gaussian distributions. ICA can be used n blind-source for blind-source separation. Imagine you are in a busy train station with separation many people talking. Your ears play the role of microphones, and they linearlymixdifferentspeechsignalsinthetrainstation.Thegoalofblindsourceseparationistoidentifytheconstituentpartsofthemixedsignals. As discussed previously in the context of maximum likelihood estimation forPPCA,theoriginalPCAsolutionisinvarianttoanyrotation.Therefore, PCA can identify the best lower-dimensional subspace in which the signals live, but not the signals themselves (Murphy, 2012). ICA addresses this issue by modifying the prior distribution p(z) on the latent sources Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.8 FurtherReading 347 to require non-Gaussian priors p(z). We refer to the books by Hyvarinen etal.(2001)andMurphy(2012)formoredetailsonICA. PCA,factoranalysis,andICAarethreeexamplesfordimensionalityreductionwithlinearmodels.CunninghamandGhahramani(2015)provide abroadersurveyoflineardimensionalityreduction. The (P)PCA model we discussed here allows for several important extensions. In Section 10.5, we explained how to do PCA when the input dimensionality D is significantly greater than the number N of data points.ByexploitingtheinsightthatPCAcanbeperformedbycomputing (many)innerproducts,thisideacanbepushedtotheextremebyconsidering infinite-dimensional features. The kernel trick is the