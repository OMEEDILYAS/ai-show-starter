10 Dimensionality Reduction with Principal Component Analysis Workingdirectlywithhigh-dimensionaldata,suchasimages,comeswith A640×480pixel somedifficulties:Itishardtoanalyze,interpretationisdifficult,visualiza- colorimageisadata pointina tion is nearly impossible, and (from a practical point of view) storage of million-dimensional the data vectors can be expensive. However, high-dimensional data often space,whereevery haspropertiesthatwecanexploit.Forexample,high-dimensionaldatais pixelrespondsto often overcomplete, i.e., many dimensions are redundant and can be ex- threedimensions, plained by a combination of other dimensions. Furthermore, dimensions oneforeachcolor channel(red,green, inhigh-dimensionaldataareoftencorrelatedsothatthedatapossessesan blue). intrinsic lower-dimensional structure. Dimensionality reduction exploits structureandcorrelationandallowsustoworkwithamorecompactrepresentation of the data, ideally without losing information. We can think ofdimensionalityreductionasacompressiontechnique,similartojpegor mp3,whicharecompressionalgorithmsforimagesandmusic. In this chapter, we will discuss principal component analysis (PCA), an principalcomponent algorithm for linear dimensionality reduction. PCA, proposed by Pearson analysis (1901) and Hotelling (1933), has been around for more than 100 years PCA dimensionality and is still one of the most commonly used techniques for data compresreduction sion and data visualization. It is also used for the identification of simple patterns, latent factors, and structures of high-dimensional data. In the Figure10.1 Illustration: 4 dimensionality reduction.(a)The 2 originaldataset doesnotvarymuch 0 alongthex2 direction.(b)The datafrom(a)canbe 2 − representedusing thex1-coordinate 4 alonewithnearlyno − loss. 5.0 2.5 0.0 2.5 5.0 − − x1 2x 4 2 0 2 − 4 − 5.0 2.5 0.0 2.5 5.0 − − x1 (a)Datasetwithx1andx2coordinates. 2x (b)Compresseddatasetwhereonlythex1coordinateisrelevant. 317 ThismaterialwillbepublishedbyCambridgeUniversityPressasMathematicsforMachineLearningbyMarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng.Thispre-publicationversionis freetoviewanddownloadforpersonaluseonly.Notforre-distribution,re-saleoruseinderivativeworks.(cid:13)cbyM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2019.https://mml-book.com. 318 DimensionalityReductionwithPrincipalComponentAnalysis Karhunen-Lo`eve signal processing community, PCA is also known as the Karhunen-Lo`eve transform transform.Inthischapter,wederivePCAfromfirstprinciples,drawingon our understanding of basis and basis change (Sections 2.6.1 and 2.7.2), projections (Section 3.8), eigenvalues (Section 4.2), Gaussian distributions(Section6.5),andconstrainedoptimization(Section7.2). Dimensionality reduction generally exploits a property of high-dimensionaldata(e.g.,images)thatitoftenliesonalow-dimensionalsubspace. Figure 10.1 gives an illustrative example in two dimensions. Although the data in Figure 10.1(a) does not quite lie on a line, the data does not vary much in the x -direction, so that we can express it as if it were on 2 a line – with nearly no loss; see Figure 10.1(b). To describe the data in Figure 10.1(b), only the x -coordinate is required, and the data lies in a 1 one-dimensionalsubspaceofR2. 10.1 Problem Setting In PCA,we areinterested infinding projectionsx˜ ofdata pointsx that n n areassimilartotheoriginaldatapointsaspossible,butwhichhaveasignificantly lower intrinsic dimensionality. Figure 10.1 gives an illustration ofwhatthiscouldlooklike. Moreconcretely,weconsiderani.i.d.dataset = x ,...,x ,x 1 N n datacovariance RD,withmean0thatpossessesthedatacovaria X ncem { atrix (6.42) } ∈ matrix N 1 (cid:88) S = x x(cid:62). (10.1) N n n n=1 Furthermore, we assume there exists a low-dimensional compressed representation(code) z = B(cid:62)x RM (10.2) n n ∈ ofx ,wherewedefinetheprojectionmatrix n B := [b ,...,b ] RD×M . (10.3) 1 M ∈ WeassumethatthecolumnsofBareorthonormal(Definition3.7)sothat T b1 h , e . c . o . l , u b m M ns ofB s b u (cid:62) i b b s j pa = ce 0 U if an R d D o , n d ly im if (U i (cid:54) = ) = j M and < b D (cid:62) i b o i n = to 1 w . h W ic e h s w ee e k p a ro n je M ct - t d h i e m d e