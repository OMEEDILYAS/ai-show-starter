find a matrix B (see (10.3)) that retains as much information as possible when compressing data by projecting it onto the subspace spanned by thecolumnsb ,...,b ofB.Retainingmostinformationafterdatacom1 M pression is equivalent to capturing the largest amount of variance in the low-dimensionalcode(Hotelling,1933). Remark. (Centered Data) For the data covariance matrix in (10.1), we assumedcentereddata.Wecanmakethisassumptionwithoutlossofgenerality:Letusassumethatµisthemeanofthedata.Usingtheproperties ofthevariance,whichwediscussedinSection6.4.4,weobtain V [z] = V [B(cid:62)(x µ)] = V [B(cid:62)x B(cid:62)µ] = V [B(cid:62)x], (10.6) z x x x − − i.e., the variance of the low-dimensional code does not depend on the meanofthedata.Therefore,weassumewithoutlossofgeneralitythatthe data has mean 0 for the remainder of this section. With this assumption themeanofthelow-dimensionalcodeisalso0sinceE [z] = E [B(cid:62)x] = z x B(cid:62)E [x] = 0. x ♦ Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.2 MaximumVariancePerspective 321 Figure10.4 PCA findsa lower-dimensional subspace(line)that maintainsasmuch variance(spreadof thedata)aspossible whenthedata (blue)isprojected ontothissubspace (orange). 10.2.1 Direction with Maximal Variance Wemaximizethevarianceofthelow-dimensionalcodeusingasequential approach.Westartbyseekingasinglevectorb 1 RD thatmaximizesthe Thevectorb1will ∈ variance of the projected data, i.e., we aim to maximize the variance of bethefirstcolumn thefirstcoordinatez ofz RM sothat ofthematrixBand 1 ∈ thereforethefirstof M orthonormal N V := V[z ] = 1 (cid:88) z2 (10.7) basisvectorsthat 1 1 N 1n spanthe n=1 lower-dimensional subspace. is maximized, where we exploited the i.i.d. assumption of the data and defined z as the first coordinate of the low-dimensional representation 1n z RM ofx RD.Notethatfirstcomponentofz isgivenby n n n ∈ ∈ z = b(cid:62)x , (10.8) 1n 1 n i.e., it is the coordinate of the orthogonal projection of x onto the onen dimensional subspace spanned by b (Section 3.8). We substitute (10.8) 1 into(10.7),whichyields N N 1 (cid:88) 1 (cid:88) V = (b(cid:62)x )2 = b(cid:62)x x(cid:62)b (10.9a) 1 N 1 n N 1 n n 1 n=1 n=1 (cid:32) (cid:33) N 1 (cid:88) = b(cid:62) x x(cid:62) b = b(cid:62)Sb , (10.9b) 1 N n n 1 1 1 n=1 where S is the data covariance matrix defined in (10.1). In (10.9a), we have used the fact that the dot product of two vectors is symmetric with respecttoitsarguments,thatis,b(cid:62)x = x(cid:62)b . 1 n n 1 Notice that arbitrarily increasing the magnitude of the vector b in1 creases V , that is, a vector b that is two times longer can result in V 1 1 1 that is potentially four times larger. Therefore, we restrict all solutions to (cid:107)b1(cid:107)2=1 b 1 2 = 1, which results in a constrained optimization problem in which ⇐⇒ (cid:107)b1(cid:107)=1. (cid:107) (cid:107) weseekthedirectionalongwhichthedatavariesmost. With the restriction of the solution space to unit vectors the vector b 1 that points in the direction of maximum variance can be found by the (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 322 DimensionalityReductionwithPrincipalComponentAnalysis constrainedoptimizationproblem maxb(cid:62)Sb 1 1 b1 (10.10) subjectto b 2 = 1. 1 (cid:107) (cid:107) FollowingSection7.2,weobtaintheLagrangian L(b ,λ) = b(cid:62)Sb +λ (1 b(cid:62)b ) (10.11) 1 1 1 1 − 1 1 to solve this constrained optimization problem. The partial derivatives of Lwithrespecttob andλ are 1 1 ∂L ∂L = 2b(cid:62)S 2λ b(cid:62), = 1 b(cid:62)b , (10.12) ∂b 1 − 1 1 ∂λ − 1 1 1 1 respectively.Settingthesepartialderivativesto0givesustherelations Sb = λ b