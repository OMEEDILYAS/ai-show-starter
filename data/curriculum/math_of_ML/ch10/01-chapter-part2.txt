a n t s a io . n W a e l formabasisofthe denotethep ⊆ rojecteddatabyx˜ U,andtheircoordinates(withrespect n M-dimensional ∈ tothebasisvectorsb ,...,b ofU)byz .Ouraimistofindprojections subspaceinwhich 1 M n theprojecteddata x˜ n RD (or equivalently the codes z n and the basis vectors b 1 ,...,b M ) ∈ x˜ =BB(cid:62)x∈RD so that they are as similar to the original data x and minimize the loss n live. duetocompression. Example 10.1 (Coordinate Representation/Code) Consider R2 with the canonical basis e = [1,0](cid:62), e = [0,1](cid:62). From 1 2 Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.1 ProblemSetting 319 Original Reconstructed Figure10.2 RD RD Graphical Compressed illustrationofPCA. InPCA,wefinda RM compressedversion x z x˜ zoforiginaldatax. Thecompressed datacanbe reconstructedinto x˜,whichlivesinthe originaldataspace, buthasanintrinsic lower-dimensional Chapter2, weknow thatx R2 canbe representedas alinear combina- representationthan tionofthesebasisvectors,e ∈ .g., x. (cid:20) (cid:21) 5 = 5e +3e . (10.4) 3 1 2 However,whenweconsidervectorsoftheform (cid:20) (cid:21) 0 x˜ = R2, z R, (10.5) z ∈ ∈ they can always be written as 0e +ze . To represent these vectors it is 1 2 sufficient to remember/store the coordinate/code z of x˜ with respect to thee 2 vector. Thedimensionofa More precisely, the set of x˜ vectors (with the standard vector addition vectorspace correspondstothe and scalar multiplication) forms a vector subspace U (see Section 2.4) numberofitsbasis withdim(U) = 1becauseU = span[e 2 ]. vectors(see Section2.6.1). In Section 10.2, we will find low-dimensional representations that retain as much information as possible and minimize the compression loss. An alternative derivation of PCA is given in Section 10.3, where we will 2 belookingatminimizingthesquaredreconstructionerror x x˜ ben n (cid:107) − (cid:107) tweentheoriginaldatax anditsprojectionx˜ . n n Figure 10.2 illustrates the setting we consider in PCA, where z representsthelower-dimensionalrepresentationofthecompresseddatax˜ and plays the role of a bottleneck, which controls how much information can flowbetweenxandx˜.InPCA,weconsideralinearrelationshipbetween the original data x and its low-dimensional code z so that z = B(cid:62)x and x˜ = Bz for a suitable matrix B. Based the motivation of thinking of PCA as a data compression technique, we can interpret the arrows in Figure 10.2 as a pair of operations representing encoders and decoders. ThelinearmappingrepresentedbyB canbethoughtofadecoder,which mapsthelow-dimensionalcodez RM backintotheoriginaldataspace RD. Similarly, B(cid:62) can be thought ∈ of an encoder, which encodes the originaldataxasalow-dimensional(compressed)codez. Throughout this chapter, we will use the MNIST digits dataset as a re- (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 320 DimensionalityReductionwithPrincipalComponentAnalysis Figure10.3 Examplesof handwrittendigits fromtheMNIST dataset.http: //yann.lecun. com/exdb/mnist/. occurringexample,whichcontains60,000examplesofhandwrittendigits 0through9.Eachdigitisagrayscaleimageofsize28 28,i.e.,itcontains × 784pixelssothatwecaninterpreteveryimageinthisdatasetasavector x R784.ExamplesofthesedigitsareshowninFigure10.3. ∈ 10.2 Maximum Variance Perspective Figure 10.1 gave an example of how a two-dimensional dataset can be represented using a single coordinate. In Figure 10.1(b), we chose to ignore the x -coordinate of the data because it did not add too much in2 formation so that the compressed data is similar to the original data in Figure 10.1(a). We could have chosen to ignore the x -coordinate, but 1 thenthecompresseddatahadbeenverydissimilarfromtheoriginaldata, andmuchinformationinthedatawouldhavebeenlost. If we interpret information content in the data as how “space filling” thedatasetis,thenwecandescribetheinformationcontainedinthedata bylookingatthespreadofthedata.FromSection6.4.1,weknowthatthe varianceisanindicatorofthespreadofthedata,andwecanderivePCAas a dimensionality reduction algorithm that maximizes the variance in the low-dimensionalrepresentationofthedatatoretainasmuchinformation aspossible.Figure10.4illustratesthis. Considering the setting discussed in Section 10.1, our aim is to