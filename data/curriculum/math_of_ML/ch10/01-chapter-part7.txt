ProjectionPerspective 327 tion,wewouldarriveatexactlythesamesolution,butthenotationwould besubstantiallymorecluttered. Weareinterestedinfindingthebestlinearprojectionof ontoalowerdimensionalsubspaceU ofRD withdim(U) = M andort X honormalbasis vectors b 1 ,...,b M . We will call this subspace U the principal subspace. principalsubspace Theprojectionsofthedatapointsaredenotedby M (cid:88) x˜ := z b = Bz RD, (10.28) n mn m n ∈ m=1 where z := [z ,...,z ](cid:62) RM is the coordinate vector of x˜ with n 1n Mn n ∈ respect to the basis (b ,...,b ). More specifically, we are interested in 1 M havingthex˜ assimilartox aspossible. n n ThesimilaritymeasureweuseinthefollowingisthesquaredEuclidean 2 norm x x˜ between x and x˜. We therefore define our objective as (cid:107) − (cid:107) theminimizingtheaveragesquaredEuclideandistance(reconstructioner- reconstructionerror ror)(Pearson,1901) N 1 (cid:88) J := x x˜ 2, (10.29) M N (cid:107) n − n (cid:107) n=1 where we make it explicit that the dimension of the subspace onto which we project the data is M. In order to find this optimal linear projection, we need to find the orthonormal basis of the principal subspace and the coordinatesz RM oftheprojectionswithrespecttothisbasis. n ∈ To find the coordinates z and the ONB of the principal subspace, we n follow a two-step approach. First, we optimize the coordinates z for a n givenONB(b ,...,b );second,wefindtheoptimalONB. 1 M 10.3.2 Finding Optimal Coordinates Letusstartbyfindingtheoptimalcoordinatesz ,...,z oftheprojec1n Mn tions x˜ for n = 1,...,N. Consider Figure 10.7(b), where the principal n subspace is spanned by a single vector b. Geometrically speaking, finding theoptimalcoordinatesz correspondstofindingtherepresentationofthe linearprojectionx˜ withrespecttobthatminimizesthedistancebetween x˜ x. From Figure 10.7(b), it is clear that this will be the orthogonal − projection,andinthefollowingwewillshowexactlythis. We assume an ONB (b ,...,b ) of U RD. To find the optimal co1 M ⊆ ordinates z with respect to this basis, we require the partial derivatives m ∂J ∂J ∂x˜ M = M n , (10.30a) ∂z ∂x˜ ∂z in n in ∂J 2 M = (x x˜ )(cid:62) R1×D, (10.30b) ∂x˜ −N n − n ∈ n (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 328 DimensionalityReductionwithPrincipalComponentAnalysis Figure10.8 3.25 Optimalprojection ofavectorx∈R2 3.00 ontoa 2.75 one-dimensional 2.50 subspace (continuationfrom 2.25 Figure10.7). 2.00 (a)Distances (cid:107)x−x˜(cid:107)forsome 1.75 x˜ ∈U. 1.50 (b)Orthogonal 1.25 projectionand 1.0 0.5 0.0 0.5 1.0 1.5 2.0 optimalcoordinates. − − x1 x˜ x k − k 2.5 2.0 1.5 1.0 0.5 0.0 0.5 − 1.0 0.5 0.0 0.5 1.0 1.5 2.0 − − x1 (a)Distances(cid:107)x−x˜(cid:107)forsomex˜ = z1b ∈ U =span[b];seepanel(b)forthesetting. 2x U x˜ b (b)Thevectorx˜thatminimizesthedistance inpanel(a)isitsorthogonalprojectiononto U. The coordinate of the projection x˜ with respect to the basis vector b that spans U is the factor we need to scale b in order to “reach”x˜. (cid:32) (cid:33) ∂x˜ n (10 = .28) ∂ (cid:88) M z b = b (10.30c) ∂z ∂z mn m i in in m=1 fori = 1,...,M,suchthatweobtain (cid:32) (cid:33)(cid:62) ∂J M ( ( 1 1 0 0 = . . 3 3 0 0 b c) ) 2 (x x˜ )(cid:62)b (10 = .28) 2 x (cid:88) M z b b ∂z −N n − n i −N n − mn m i in m=1 (10.31a) 2 2 O = NB (x(cid:62)b z b(cid:62)b ) = (x(cid:62)b