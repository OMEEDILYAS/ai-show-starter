RD linearauto-encoder. Code Itencodesthe high-dimensional RM B B dataxintoa (cid:62) lower-dimensional x z x˜ representation (code)z∈RM and decodeszusinga decoder.The decodedvectorx˜is theorthogonal Encoder Decoder projectionofthe originaldataxonto theM-dimensional principalsubspace. thecodeisgivenbyz = B(cid:62)x RM andweareinterestedinminimizn n ∈ ingtheaveragesquarederrorbetweenthedatax anditsreconstruction n x˜ = Bz ,n = 1,...,N,weobtain n n 1 (cid:88) N 1 (cid:88) N (cid:13) (cid:13)2 x x˜ 2 = (cid:13)x B(cid:62)Bx (cid:13) . (10.76) N (cid:107) n − n (cid:107) N (cid:13) n − n(cid:13) n=1 n=1 Thismeansweendupwiththesameobjectivefunctionasin(10.29)that wediscussedinSection10.3sothatweobtainthePCAsolutionwhenwe minimize the squared auto-encoding loss. If we replace the linear mapping of PCA with a nonlinear mapping, we get a nonlinear auto-encoder. Aprominentexampleofthisisadeepauto-encoderwherethelinearfunctionsarereplacedwithdeepneuralnetworks.Inthiscontext,theencoder recognitionnetwork is also known as a recognition network or inference network, whereas the inferencenetwork decoderisalsocalledagenerator. generator Another interpretation of PCA is related to information theory. We can think of the code as a smaller or compressed version of the original data point. When we reconstruct our original data using the code, we do not get the exact data point back, but a slightly distorted or noisy version Thecodeisa of it. This means that our compression is “lossy”. Intuitively, we want compressedversion to maximize the correlation between the original data and the loweroftheoriginaldata. dimensionalcode.Moreformally,thisisrelatedtothemutualinformation. WewouldthengetthesamesolutiontoPCAwediscussedinSection10.3 bymaximizingthemutualinformation,acoreconceptininformationtheory(MacKay,2003). In our discussion on PPCA, we assumed that the parameters of the model, i.e., B,µ, and the likelihood parameter σ2, are known. Tipping andBishop(1999)describehowtoderivemaximumlikelihoodestimates for these parameters in the PPCA setting (note that we use a different notationinthischapter).Themaximumlikelihoodparameters,whenproDraft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.8 FurtherReading 345 jectingD-dimensionaldataontoanM-dimensionalsubspace,are N 1 (cid:88) µ = x , (10.77) ML N n n=1 B ML = T(Λ σ2I) 1 2R, (10.78) − D 1 (cid:88) σ2 = λ , (10.79) ML D M j − j=M+1 whereT RD×M containsM eigenvectorsofthedatacovariancematrix, ThematrixΛ−σ2I Λ = diag ∈ (λ ,...,λ ) RM×M isadiagonalmatrixwiththeeigenvalues in(10.78)is 1 M associated with the pri ∈ ncipal axes on its diagonal, and R RM×M is guaranteedtobe ∈ positivesemidefinite an arbitrary orthogonal matrix. The maximum likelihood solution B is ML asthesmallest unique up to an arbitrary orthogonal transformation, e.g., we can right- eigenvalueofthe multiply B with any rotation matrix R so that (10.78) essentially is a datacovariance ML singularvaluedecomposition(seeSection4.5).Anoutlineoftheproofis matrixisbounded frombelowbythe givenbyTippingandBishop(1999). noisevarianceσ2. The maximum likelihood estimate for µ given in (10.77) is the sample mean of the data. The maximum likelihood estimator for the observation noise variance σ2 given in (10.79) is the average variance in the orthogonalcomplementoftheprincipalsubspace,i.e.,theaverageleftovervariance that we cannot capture with the first M principal components is treatedasobservationnoise. In the noise-free limit where σ 0, PPCA and PCA provide identical → solutions: Since the data covariance matrix S is symmetric, it can be diagonalized (see Section 4.4), i.e., there exists a matrix T of eigenvectors ofS sothat S = TΛT−1. (10.80) InthePPCAmodel,thedatacovariancematrixisthecovariancematrixof theGaussianlikelihoodp(x B,µ,σ2),whichisBB(cid:62)+σ2I,see(10.70b). For σ 0, we obtain BB(cid:62)| so that this data covariance must equal the → PCAdatacovariance(anditsfactorizationgivenin(10.80))sothat Cov[ ] = TΛT−1 = BB(cid:62) B = TΛ1 2R, (10.81) X ⇐⇒ i.e., we obtain the maximum likelihood estimate in (10.78) for σ = 0. From (10.78) and (10.80), it becomes clear that (P)PCA performs a decompositionofthedatacovariancematrix. In a