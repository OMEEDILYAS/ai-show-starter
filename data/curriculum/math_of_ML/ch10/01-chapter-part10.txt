covariance matrix of the data, (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 332 DimensionalityReductionwithPrincipalComponentAnalysis projectedontotheorthogonalcomplementoftheprincipalsubspace.MinMinimizingthe imizingtheaveragesquaredreconstructionerroristhereforeequivalentto averagesquared minimizingthevarianceofthedatawhenprojectedontothesubspacewe reconstructionerror ignore,i.e.,theorthogonalcomplementoftheprincipalsubspace.Equivaisequivalentto lently, we maximize the variance of the projection that we retain in the minimizingthe projectionofthe principal subspace, which links the projection loss immediately to the datacovariance maximum-varianceformulationofPCAdiscussedinSection10.2.Butthis matrixontothe then also means that we will obtain the same solution that we obtained orthogonal for the maximum-variance perspective. Therefore, we omit a derivation complementofthe principalsubspace. that is identical to the one presented in Section 10.2 and summarize the Minimizingthe resultsfromearlierinthelightoftheprojectionperspective. averagesquared Theaveragesquaredreconstructionerror,whenprojectingontotheMreconstructionerror dimensionalprincipalsubspace,is isequivalentto maximizingthe D (cid:88) varianceofthe J = λ , (10.44) M j projecteddata. j=M+1 where λ are the eigenvalues of the data covariance matrix. Therefore, j to minimize (10.44) we need to select the smallest D M eigenvalues, − which then implies that their corresponding eigenvectors are the basis of the orthogonal complement of the principal subspace. Consequently, this meansthatthebasisoftheprincipalsubspacecomprisestheeigenvectors b ,...,b thatareassociatedwiththelargestM eigenvaluesofthedata 1 M covariancematrix. Example 10.3 (MNIST Digits Embedding) Figure10.10 Embeddingof MNISTdigits0 (blue)and1 (orange)ina two-dimensional principalsubspace usingPCA.Four embeddingsofthe digits“0”and“1”in theprincipal subspaceare highlightedinred withtheir corresponding originaldigit. Figure 10.10 visualizes the training data of the MMIST digits “0” and “1” embedded in the vector subspace spanned by the first two principal components.Weobservearelativelyclearseparationbetween“0”s(bluedots) and “1”s (orange dots), and we see the variation within each individual Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.4 EigenvectorComputationandLow-RankApproximations 333 cluster.Fourembeddingsofthedigits“0”and“1”intheprincipalsubspace are highlighted in red with their corresponding original digit. The figure revealsthatthevariationwithinthesetof“0”issignificantlygreaterthan thevariationwithinthesetof“1”. 10.4 Eigenvector Computation and Low-Rank Approximations In the previous sections, we obtained the basis of the principal subspace astheeigenvectorsthatareassociatedwiththelargesteigenvaluesofthe datacovariancematrix N 1 (cid:88) 1 S = x x(cid:62) = XX(cid:62), (10.45) N n n N n=1 X = [x ,...,x ] RD×N . (10.46) 1 N ∈ Note that X is a D N matrix, i.e., it is the transpose of the “typical” × data matrix (Bishop, 2006; Murphy, 2012). To get the eigenvalues (and thecorrespondingeigenvectors)ofS,wecanfollowtwoapproaches: Use eigendecomposition We perform an eigendecomposition (see Section 4.2) and compute the orSVDtocompute eigenvaluesandeigenvectorsofS directly. eigenvectors. We use a singular value decomposition (see Section 4.5). Since S is symmetricandfactorizesintoXX(cid:62) (ignoringthefactor 1),theeigenN valuesofS arethesquaredsingularvaluesofX. Morespecifically,theSVDofX isgivenby X = U Σ V(cid:62) , (10.47) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) D×N D×DD×NN×N where U RD×D and V(cid:62) RN×N are orthogonal matrices and Σ RD×N isa ∈ matrixwhoseonly ∈ nonzeroentriesarethesingularvaluesσ (cid:62)∈ ii 0.Itthenfollowsthat 1 1 1 S = XX(cid:62) = UΣV(cid:62)V Σ(cid:62)U(cid:62) = UΣΣ(cid:62)U(cid:62). (10.48) N N (cid:124) (cid:123)(cid:122) (cid:125) N =IN With the results from Section 4.5, we get that the columns of U are the ThecolumnsofU eigenvectors of XX(cid:62) (and therefore S). Furthermore, the eigenvalues aretheeigenvectors λ ofS arerelatedtothesingularvaluesofX via ofS. d σ2 λ = d . (10.49) d N This relationship between the eigenvalues of S and the singular values ofX providestheconnectionbetweenthemaximumvarianceview(Section10.2)andthesingularvaluedecomposition. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 334 DimensionalityReductionwithPrincipalComponentAnalysis 10.4.1 PCA Using Low-Rank Matrix Approximations To maximize the variance of the projected data (or minimize the average squared reconstruction error), PCA chooses the columns of U in (10.48) to be the eigenvectors that are associated with the M largest eigenvalues ofthedatacovariancematrixSsothatweidentifyU astheprojectionmatrixBin(10.3),whichprojectstheoriginaldataontoalower-dimensional Eckart-Young subspace of dimension M. The Eckart-Young theorem (Theorem 4.25 in