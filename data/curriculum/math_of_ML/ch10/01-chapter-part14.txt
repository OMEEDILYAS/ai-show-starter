in the projected space or by minimizing the reconstruction error is obtained as the special case of maximumlikelihoodestimationinanoise-freesetting. 10.7.1 Generative Process and Probabilistic Model In PPCA, we explicitly write down the probabilistic model for linear dimensionality reduction. For this we assume a continuous latent variable z RM with a standard-normal prior p(z) = (cid:0) 0, I (cid:1) and a linear rela- ∈ N tionshipbetweenthelatentvariablesandtheobservedxdatawhere x = Bz+µ+(cid:15) RD, (10.63) ∈ where (cid:15) (cid:0) 0, σ2I (cid:1) is Gaussian observation noise and B RD×M and µ ∼RD N describe the linear/affine mapping from latent to ∈ observed ∈ variables.Therefore,PPCAlinkslatentandobservedvariablesvia p(x z,B,µ,σ2) = (cid:0) x Bz+µ, σ2I (cid:1) . (10.64) | N | Overall,PPCAinducesthefollowinggenerativeprocess: (cid:0) (cid:1) z z 0, I (10.65) n ∼ N | x z (cid:0) x Bz +µ, σ2I (cid:1) (10.66) n n n | ∼ N | To generate a data point that is typical given the model parameters, we ancestralsampling follow an ancestral sampling scheme: We first sample a latent variable z n fromp(z).Thenweusez in(10.64)tosampleadatapointconditioned n onthesampledz ,i.e.,x p(x z ,B,µ,σ2). n n n ∼ | Thisgenerativeprocessallowsustowritedowntheprobabilisticmodel (i.e.,thejointdistributionofallrandomvariables;seeSection8.4)as p(x,z B,µ,σ2) = p(x z,B,µ,σ2)p(z), (10.67) | | whichimmediatelygivesrisetothegraphicalmodelinFigure10.14using theresultsfromSection8.5. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.7 LatentVariablePerspective 341 Figure10.14 z n Graphicalmodelfor probabilisticPCA. Theobservationsxn B µ explicitlydependon corresponding latentvariables x n σ zn∼N (cid:0) 0,I (cid:1) .The modelparameters n=1,...,N B,µandthe likelihood parameterσare Remark. Notethedirectionofthearrowthatconnectsthelatentvariables sharedacrossthe z and the observed data x: The arrow points from z to x, which means dataset. thatthePPCAmodelassumesalower-dimensionallatentcausezforhighdimensional observations x. In the end, we are obviously interested in finding something out about z given some observations. To get there we willapplyBayesianinferenceto“invert”thearrowimplicitlyandgofrom observationstolatentvariables. ♦ Example 10.5 (Generating New Data Using Latent Variables) Figure10.15 Generatingnew MNISTdigits.The latentvariablesz canbeusedto generatenewdata x˜ =Bz.Thecloser westaytothe trainingdata,the morerealisticthe generateddata. Figure10.15showsthelatentcoordinatesoftheMNISTdigits“8”found by PCA when using a two-dimensional principal subspace (blue dots). We can query any vector z in this latent space and generate an image ∗ x˜ = Bz that resembles the digit “8”. We show eight of such generated ∗ ∗ images with their corresponding latent space representation. Depending on where we query the latent space, the generated images look different (shape, rotation, size, etc.). If we query away from the training data, we seemoreanmoreartifacts,e.g.,thetop-leftandtop-rightdigits.Notethat theintrinsicdimensionalityofthesegeneratedimagesisonlytwo. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 342 DimensionalityReductionwithPrincipalComponentAnalysis 10.7.2 Likelihood and Joint Distribution Thelikelihooddoes notdependonthe Using the results from Chapter 6, we obtain the likelihood of this probalatentvariablesz. bilistic model by integrating out the latent variable z (see Section 8.4.3) sothat (cid:90) p(x B,µ,σ2) = p(x z,µ,σ2)p(z)dz (10.68a) | | (cid:90) = (cid:0) x Bz+µ, σ2I (cid:1) (cid:0) z 0, I (cid:1) dz. (10.68b) N | N | FromSection6.5,weknowthatthesolutiontothisintegralisaGaussian distributionwithmean E [x] = E [Bz+µ]+E [(cid:15)] = µ (10.69) x z (cid:15) andwithcovariancematrix V[x] = V [Bz+µ]+V [(cid:15)] = V [Bz]+σ2I (10.70a) z (cid:15) z = BV [z]B(cid:62)+σ2I = BB(cid:62)+σ2I. (10.70b) z The likelihood in (10.68b) can be used for maximum likelihood or MAP estimationofthemodelparameters. Remark. We cannot use the conditional distribution in (10.64) for maximum likelihood estimation as it still depends on the latent variables. The likelihood function we require for maximum likelihood