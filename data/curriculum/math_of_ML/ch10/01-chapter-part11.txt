theorem Section4.6)offersadirectwaytoestimatethelow-dimensionalrepresentation.Considerthebestrank-M approximation X˜ := argmin X A RD×N (10.50) M rk(A)(cid:54)M (cid:107) − (cid:107)2 ∈ ofX,where isthespectralnormdefinedin(4.93).TheEckart-Young (cid:107)·(cid:107)2 theorem states that X˜ is given by truncating the SVD at the top-M M singularvalue.Inotherwords,weobtain X˜ = U Σ V(cid:62) RD×N (10.51) M (cid:124)(cid:123)(cid:122) M (cid:125) (cid:124)(cid:123)(cid:122) M (cid:125) (cid:124)(cid:123)(cid:122) M (cid:125) ∈ D×MM×MM×N with orthogonal matrices U := [u ,...,u ] RD×M and V := M 1 M M [v ,...,v ] RN×M and a diagonal matrix Σ ∈ RM×M whose diago1 M M ∈ ∈ nalentriesaretheM largestsingularvaluesofX. 10.4.2 Practical Aspects Finding eigenvalues and eigenvectors is also important in other fundamentalmachinelearningmethodsthatrequirematrixdecompositions.In theory,aswediscussedinSection4.2,wecansolvefortheeigenvaluesas roots of the characteristic polynomial. However, for matrices larger than 4 4thisisnotpossiblebecausewewouldneedtofindtherootsofapoly- × Abel-Ruffini nomial of degree 5 or higher. However, the Abel-Ruffini theorem (Ruffini, theorem 1799; Abel, 1826) states that there exists no algebraic solution to this problem for polynomials of degree 5 or more. Therefore, in practice, we np.linalg.eigh solveforeigenvaluesorsingularvaluesusingiterativemethods,whichare or np.linalg.svd implementedinallmodernpackagesforlinearalgebra. In many applications (such as PCA presented in this chapter), we only require a few eigenvectors. It would be wasteful to compute the full decomposition, and then discard all eigenvectors with eigenvalues that are beyond the first few. It turns out that if we are interested in only the first few eigenvectors (with the largest eigenvalues), then iterative processes, whichdirectlyoptimizetheseeigenvectors,arecomputationallymoreefficientthanafulleigendecomposition(orSVD).Intheextremecaseofonly poweriteration needing the first eigenvector, a simple method called the power iteration isveryefficient.Poweriterationchoosesarandomvectorx thatisnotin 0 Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.5 PCAinHighDimensions 335 thenullspaceofS andfollowstheiteration Sx x = k , k = 0,1,... . (10.52) k+1 Sx k (cid:107) (cid:107) This means the vector x k is multiplied by S in every iteration and then IfSisinvertible,it normalized,i.e.,wealwayshave x = 1.Thissequenceofvectorscon- issufficientto k vergestotheeigenvectorassociat (cid:107) edw (cid:107) iththelargesteigenvalueofS.The ensurethatx0(cid:54)=0. original Google PageRank algorithm (Page et al., 1999) uses such an algorithmforrankingwebpagesbasedontheirhyperlinks. 10.5 PCA in High Dimensions InordertodoPCA,weneedtocomputethedatacovariancematrix.InD dimensions,thedatacovariancematrixisaD Dmatrix.Computingthe × eigenvalues and eigenvectors of this matrix is computationally expensive asitscalescubicallyinD.Therefore,PCA,aswediscussedearlier,willbe infeasibleinveryhighdimensions.Forexample,ifourx areimageswith n 10,000 pixels (e.g., 100 100 pixel images), we would need to compute × the eigendecomposition of a 10,000 10,000 covariance matrix. In the × following,weprovideasolutiontothisproblemforthecasethatwehave substantiallyfewerdatapointsthandimensions,i.e.,N D. Assume we have a centered dataset x ,...,x , x (cid:28) RD. Then the 1 N n ∈ datacovariancematrixisgivenas 1 S = XX(cid:62) RD×D, (10.53) N ∈ where X = [x ,...,x ] is a D N matrix whose columns are the data 1 N × points. WenowassumethatN D,i.e.,thenumberofdatapointsissmaller (cid:28) than the dimensionality of the data. If there are no duplicate data points, therankofthecovariancematrixS isN,soithasD N+1manyeigen- − valuesthatare0.Intuitively,thismeansthattherearesomeredundancies. Inthefollowing,wewillexploitthisandturntheD Dcovariancematrix × intoanN N covariancematrixwhoseeigenvaluesareallpositive. × InPCA,weendedupwiththeeigenvectorequation Sb = λ b , m = 1,...,M , (10.54) m m m where b is a basis vector of the principal subspace. Let us rewrite this m equationabit:WithS definedin(10.53),weobtain 1 Sb = XX(cid:62)b = λ b . (10.55) m N m m m WenowmultiplyX(cid:62) RN×D fromtheleft-handside,whichyields ∈ 1 1 X(cid:62)XX(cid:62)b = λ X(cid:62)b X(cid:62)Xc = λ c , (10.56) N (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) m (cid:125) m m ⇐⇒ N m m m N×N =:cm (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 336 DimensionalityReductionwithPrincipalComponentAnalysis and we get