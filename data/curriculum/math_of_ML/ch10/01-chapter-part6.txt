100 150 200 associatedwiththe Index largesteigenvalues. eulavnegiE 500 400 300 200 100 0 50 100 150 200 Numberofprincipalcomponents (a)Eigenvalues(sortedindescendingorder)of the data covariance matrix of all digits “8” in theMNISTtrainingset. ecnairavderutpaC (b)Variancecapturedbytheprincipalcomponents. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.3 ProjectionPerspective 325 Figure10.6 Illustrationofthe projection approach:Finda subspace(line)that minimizesthe lengthofthe differencevector betweenprojected (orange)and original(blue)data. Takingalldigits“8”intheMNISTtrainingdata,wecomputetheeigenvaluesofthedatacovariancematrix.Figure10.5(a)showsthe200largest eigenvalues of the data covariance matrix. We see that only a few of them have a value that differs significantly from 0. Therefore, most of thevariance,whenprojectingdataontothesubspacespannedbythecorrespondingeigenvectors,iscapturedbyonlyafewprincipalcomponents, asshowninFigure10.5(b). Overall,tofindanM-dimensionalsubspaceofRD thatretainsasmuch information as possible, PCA tells us to choose the columns of the matrix B in (10.3) as the M eigenvectors of the data covariance matrix S that are associated with the M largest eigenvalues. The maximum amount of variancePCAcancapturewiththefirstM principalcomponentsis M (cid:88) V = λ , (10.24) M m m=1 wheretheλ aretheM largesteigenvaluesofthedatacovariancematrix m S.Consequently,thevariancelostbydatacompressionviaPCAis D (cid:88) J := λ = V V . (10.25) M j D M − j=M+1 Instead of these absolute quantities, we can define the relative variance capturedas VM,andtherelativevariancelostbycompressionas1 VM. VD − VD 10.3 Projection Perspective In the following, we will derive PCA as an algorithm that directly minimizes the average reconstruction error. This perspective allows us to interpretPCAasimplementinganoptimallinearauto-encoder.Wewilldraw heavilyfromChapters2and3. In the previous section, we derived PCA by maximizing the variance in the projected space to retain as much information as possible. In the (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 326 DimensionalityReductionwithPrincipalComponentAnalysis Figure10.7 2.5 Simplified projectionsetting. 2.0 (a)Avectorx∈R2 (redcross)shallbe 1.5 projectedontoa one-dimensional 1.0 subspaceU ⊆R2 spannedbyb.(b) 0.5 showsthedifference vectorsbetweenx 0.0 andsome candidatesx˜. 0.5 − 1.0 0.5 0.0 0.5 1.0 1.5 2.0 − − x1 2x 2.5 2.0 1.5 1.0 U 0.5 b 0.0 0.5 − 1.0 0.5 0.0 0.5 1.0 1.5 2.0 − − x1 (a)Setting. 2x U b (b) Differences x−x˜i for 50 different x˜i are shownbytheredlines. following,wewilllookatthedifferencevectorsbetweentheoriginaldata x andtheirreconstructionx˜ andminimizethisdistancesothatx and n n n x˜ areascloseaspossible.Figure10.6illustratesthissetting. n 10.3.1 Setting and Objective Assume an (ordered) orthonormal basis (ONB) B = (b ,...,b ) of RD, 1 D i.e.,b(cid:62)b = 1ifandonlyifi = j and0otherwise. i j From Section 2.5 we know that for a basis (b ,...,b ) of RD any x 1 D RD canbewrittenasalinearcombinationofthebasisvectorsofRD,i.e ∈ ., Vectorsx˜ ∈U could bevectorsona D M D planeinR3.The (cid:88) (cid:88) (cid:88) x = ζ b = ζ b + ζ b (10.26) dimensionalityof d d m m j j theplaneis2,but d=1 m=1 j=M+1 thevectorsstillhave forsuitablecoordinatesζ R. threecoordinates d ∈ withrespecttothe We are interested in finding vectors x˜ RD, which live in lowerstandardbasisof dimensionalsubspaceU RD,dim(U) = M ∈ ,sothat R3. ⊆ M (cid:88) x˜ = z b U RD (10.27) m m ∈ ⊆ m=1 is as similar to x as possible. Note that at this point we need to assume thatthecoordinatesz ofx˜ andζ ofxarenotidentical. m m Inthefollowing,weuseexactlythiskindofrepresentationofx˜ tofind optimal coordinates z and basis vectors b ,...,b such that x˜ is as sim1 M ilar to the original data point x, i.e., we aim to minimize the (Euclidean) distance x x˜ .Figure10.7illustratesthissetting. (cid:107) − (cid:107) Withoutlossofgenerality,weassumethatthedataset = x ,...,x , 1 N x RD,iscenteredat0,i.e.,E[ ] = 0.Withouttheze X ro-m { eanassump- } n ∈ X Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.3