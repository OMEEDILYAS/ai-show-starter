basis of kernel kerneltrick PCA and allows us to implicitly compute inner products between infinite- kernelPCA dimensionalfeatures(Scho¨lkopfetal.,1998;Scho¨lkopfandSmola,2002). There are nonlinear dimensionality reduction techniques that are derived from PCA (Burges (2010) provides a good overview). The autoencoder perspective of PCA that we discussed previously in this section can be used to render PCA as a special case of a deep auto-encoder. In the deepauto-encoder deep auto-encoder, both the encoder and the decoder are represented by multilayer feedforward neural networks, which themselves are nonlinear mappings.Ifwesettheactivationfunctionsintheseneuralnetworkstobe theidentity,themodelbecomesequivalenttoPCA.Adifferentapproachto nonlinear dimensionality reduction is the Gaussian process latent-variable Gaussianprocess model(GP-LVM)proposedbyLawrence(2005).TheGP-LVMstartsoffwith latent-variable model the latent-variable perspective that we used to derive PPCA and replaces GP-LVM thelinearrelationshipbetweenthelatentvariableszandtheobservations x with a Gaussian process (GP). Instead of estimating the parameters of themapping(aswedoinPPCA),theGP-LVMmarginalizesoutthemodel parameters and makes point estimates of the latent variables z. Similar to Bayesian PCA, the Bayesian GP-LVM proposed by Titsias and Lawrence BayesianGP-LVM (2010)maintainsadistributiononthelatentvariablesz andusesapproximateinferencetointegratethemoutaswell. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress.