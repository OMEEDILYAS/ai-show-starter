, (10.13) 1 1 1 b(cid:62)b = 1. (10.14) 1 1 By comparing this with the definition of an eigenvalue decomposition (Section 4.4), we see that b is an eigenvector of the data covariance 1 matrixS,andtheLagrangemultiplierλ playstheroleofthecorrespond1 √ Thequantity λ1is ingeigenvalue.Thiseigenvectorproperty(10.13)allowsustorewriteour alsocalledthe varianceobjective(10.10)as loadingoftheunit vectorb1and V 1 = b(cid:62) 1 Sb 1 = λ 1 b(cid:62) 1 b 1 = λ 1 , (10.15) representsthe standarddeviation i.e., the variance of the data projected onto a one-dimensional subspace ofthedata equalstheeigenvaluethatisassociatedwiththebasisvectorb thatspans 1 accountedforbythe thissubspace.Therefore,tomaximizethevarianceofthelow-dimensional principalsubspace code, we choose the basis vector associated with the largest eigenvalue span[b1]. ofthedatacovariancematrix.Thiseigenvectoriscalledthefirstprincipal principalcomponent component.Wecandeterminetheeffect/contributionoftheprincipalcomponent b in the original data space by mapping the coordinate z back 1 1n intodataspace,whichgivesustheprojecteddatapoint x˜ = b z = b b(cid:62)x RD (10.16) n 1 1n 1 1 n ∈ intheoriginaldataspace. Remark. Although x˜ is a D-dimensional vector, it only requires a single n coordinatez torepresentitwithrespecttothebasisvectorb RD. 1n 1 ∈ ♦ 10.2.2 M-dimensional Subspace with Maximal Variance Assumewehavefoundthefirstm 1principalcomponentsasthem 1 − − eigenvectors of S that are associated with the largest m 1 eigenvalues. − SinceS issymmetric,thespectraltheorem(Theorem4.15)statesthatwe can use these eigenvectors to construct an orthonormal eigenbasis of an Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.2 MaximumVariancePerspective 323 (m 1)-dimensional subspace of RD. Generally, the mth principal com- − ponent can be found by subtracting the effect of the first m 1 principal − components b ,...,b from the data, thereby trying to find principal 1 m−1 components that compress the remaining information. We then arrive at thenewdatamatrix m−1 (cid:88) Xˆ := X b b(cid:62)X = X B X, (10.17) − i i − m−1 i=1 where X = [x 1 ,...,x N ] RD×N contains the data points as column ThematrixXˆ := t v h e e ct s o u r b s s a p n a d ce B s m pa − n 1 n : e = d (cid:80) by m i= b − 1 ∈1 , b .. i b ., (cid:62) i b isap . rojectionmatrixthatprojectsonto [ R xˆ D 1, × . N .., in xˆ N (1 ] 0 ∈ .17) 1 m−1 containsthe Remark (Notation). Throughout this chapter, we do not follow the con- informationinthe vention of collecting data x ,...,x as the rows of the data matrix, but datathathasnotyet 1 N beencompressed. we define them to be the columns of X. This means that our data matrixX isaD N matrixinsteadoftheconventionalN D matrix.The × × reason for our choice is that the algebra operations work out smoothly without the need to either transpose the matrix or to redefine vectors as rowvectorsthatareleft-multipliedontomatrices. ♦ Tofindthemthprincipalcomponent,wemaximizethevariance N N 1 (cid:88) 1 (cid:88) V = V[z ] = z2 = (b(cid:62)xˆ )2 = b(cid:62)Sˆb , (10.18) m m N mn N m n m m n=1 n=1 2 subject to b = 1, where we followed the same steps as in (10.9b) m (cid:107) (cid:107) and defined Sˆ as the data covariance matrix of the transformed dataset ˆ := xˆ ,...,xˆ . As previously, when we looked at the