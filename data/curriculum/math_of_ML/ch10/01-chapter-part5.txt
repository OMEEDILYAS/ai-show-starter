first principal 1 N X { } component alone, we solve a constrained optimization problem and discoverthattheoptimalsolutionb istheeigenvectorofSˆ thatisassociated m withthelargesteigenvalueofSˆ. Itturnsoutthatb isalsoaneigenvectorofS.Moregenerally,thesets m of eigenvectors of S and Sˆ are identical. Since both S and Sˆ are symmetric, we can find an ONB of eigenvectors (spectral theorem 4.15), i.e., there exist D distinct eigenvectors for both S and Sˆ. Next, we show that every eigenvector of S is an eigenvector of Sˆ. Assume we have already found eigenvectors b ,...,b of Sˆ. Consider an eigenvector b of S, 1 m−1 i i.e.,Sb = λ b .Ingeneral, i i i Sˆb = 1 XˆXˆ(cid:62) b = 1 (X B X)(X B X)(cid:62)b (10.19a) i N i N − m−1 − m−1 i = (S SB B S +B SB )b . (10.19b) m−1 m−1 m−1 m−1 i − − We distinguish between two cases. If i (cid:62) m, i.e., b is an eigenvector i thatisnotamongthefirstm 1principalcomponents,thenb isorthogoi − naltothefirstm 1principalcomponentsandB b = 0.Ifi < m,i.e., m−1 i − b isamongthefirstm 1principalcomponents,thenb isabasisvector i i − (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 324 DimensionalityReductionwithPrincipalComponentAnalysis of the principal subspace onto which B projects. Since b ,...,b m−1 1 m−1 are an ONB of this principal subspace, we obtain B b = b . The two m−1 i i casescanbesummarizedasfollows: B b = b ifi < m, B b = 0 ifi (cid:62) m. (10.20) m−1 i i m−1 i In the case i (cid:62) m, by using (10.20) in (10.19b), we obtain Sˆb = (S i − B S)b = Sb = λ b , i.e., b is also an eigenvector of Sˆ with eigenm−1 i i i i i valueλ .Specifically, i Sˆb = Sb = λ b . (10.21) m m m m Equation (10.21) reveals that b is not only an eigenvector of S but also m of Sˆ. Specifically, λ is the largest eigenvalue of Sˆ and λ is the mth m m largesteigenvalueofS,andbothhavetheassociatedeigenvectorb . m Inthecasei < m,byusing(10.20)in(10.19b),weobtain Sˆb = (S SB B S +B SB )b = 0 = 0b (10.22) i m−1 m−1 m−1 m−1 i i − − This means that b ,...,b are also eigenvectors of Sˆ, but they are as1 m−1 sociatedwitheigenvalue0sothatb ,...,b spanthenullspaceofSˆ. 1 m−1 Overall, every eigenvector of S is also an eigenvector of Sˆ. However, if the eigenvectors of S are part of the (m 1) dimensional principal − Thisderivation subspace,thentheassociatedeigenvalueofSˆ is0. showsthatthereis With the relation (10.21) and b(cid:62)b = 1, the variance of the data prom m anintimate jectedontothemthprincipalcomponentis connectionbetween theM-dimensional V = b(cid:62)Sb (10 = .21) λ b(cid:62)b = λ . (10.23) subspacewith m m m m m m m maximalvariance This means that the variance of the data, when projected onto an Mandtheeigenvalue dimensional subspace, equals the sum of the eigenvalues that are associdecomposition.We ated with the corresponding eigenvectors of the data covariance matrix. willrevisitthis connectionin Section10.4. Example 10.2 (Eigenvalues of MNIST “8”) Figure10.5 Propertiesofthe 50 trainingdataof 40 MNIST“8”.(a) Eigenvaluessorted 30 indescendingorder; (b)Variance 20 capturedbythe 10 principal components 0 0 50