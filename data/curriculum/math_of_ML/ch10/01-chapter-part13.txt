associated with the largest eigenvaluesofthedatacovariancematrixascolumns.PCAreturnsthe coordinates(10.60),nottheprojectionsx . ∗ (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 338 DimensionalityReductionwithPrincipalComponentAnalysis Havingstandardizedourdataset,(10.59)onlyyieldstheprojectionsin the context of the standardized dataset. To obtain our projection in the original data space (i.e., before standardization), we need to undo the standardization (10.58) and multiply by the standard deviation before addingthemeansothatweobtain x˜(d) x˜(d)σ +µ , d = 1,...,D. (10.61) ∗ ← ∗ d d Figure10.11(f)illustratestheprojectionintheoriginaldataspace. Example 10.4 (MNIST Digits: Reconstruction) In the following, we will apply PCA to the MNIST digits dataset, which contains 60,000 examples of handwritten digits 0 through 9. Each digit is animageofsize28 28,i.e.,itcontains784pixelssothatwecaninterpret everyimageinthis × datasetasavectorx R784.Examplesofthesedigits ∈ areshowninFigure10.3. Figure10.12 Effect ofincreasingthe Original numberofprincipal componentson reconstruction. PCs: 1 PCs: 10 PCs: 100 PCs: 500 Forillustrationpurposes,weapplyPCAtoasubsetoftheMNISTdigits, and we focus on the digit “8”. We used 5,389 training images of the digit “8” and determined the principal subspace as detailed in this chapter. We then used the learned projection matrix to reconstruct a set of test images, which is illustrated in Figure 10.12. The first row of Figure 10.12 shows a set of four original digits from the test set. The following rows show reconstructions of exactly these digits when using a principal subspace of dimensions 1, 10, 100, and 500, respectively. We see that even with a single-dimensional principal subspace we get a halfway decent reconstruction of the original digits, which, however, is blurry and generic. Withanincreasingnumberofprincipalcomponents(PCs),thereconstructions become sharper and more details are accounted for. With 500 prinDraft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.7 LatentVariablePerspective 339 cipal components, we effectively obtain a near-perfect reconstruction. If weweretochoose784PCs,wewouldrecovertheexactdigitwithoutany compressionloss. Figure10.13showstheaveragesquaredreconstructionerror,whichis N D 1 (cid:88) x x˜ 2 = (cid:88) λ , (10.62) N (cid:107) n − n (cid:107) i n=1 i=M+1 as a function of the number M of principal components. We can see that the importance of the principal components drops off rapidly, and only marginalgainscanbeachievedbyaddingmorePCs.Thismatchesexactly our observation in Figure 10.5, where we discovered that the most of the variance of the projected data is captured by only a few principal components.Withabout550PCs,wecanessentiallyfullyreconstructthetraining datathatcontainsthedigit“8”(somepixelsaroundtheboundariesshow novariationacrossthedatasetastheyarealwaysblack). Figure10.13 500 Averagesquared reconstructionerror 400 asafunctionofthe numberofprincipal 300 components.The averagesquared 200 reconstructionerror isthesumofthe 100 eigenvaluesinthe 0 orthogonal 0 200 400 600 800 complementofthe NumberofPCs principalsubspace. rorrenoitcurtsnocerderauqsegarevA 10.7 Latent Variable Perspective In the previous sections, we derived PCA without any notion of a probabilistic model using the maximum-variance and the projection perspectives. On the one hand, this approach may be appealing as it allows us to sidestep all the mathematical difficulties that come with probability theory,butontheotherhand,aprobabilisticmodelwouldofferusmoreflexibilityandusefulinsights.Morespecifically,aprobabilisticmodelwould Come with a likelihood function, and we can explicitly deal with noisy observations(whichwedidnotevendiscussearlier) Allow us to do Bayesian model comparison via the marginal likelihood asdiscussedinSection8.6 ViewPCAasagenerativemodel,whichallowsustosimulatenewdata (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 340 DimensionalityReductionwithPrincipalComponentAnalysis Allowustomakestraightforwardconnectionstorelatedalgorithms Deal with data dimensions that are missing at random by applying Bayes’theorem Giveusanotionofthenoveltyofanewdatapoint Giveusaprincipledwaytoextendthemodel,e.g.,toamixtureofPCA models HavethePCAwederivedinearliersectionsasaspecialcase Allow for a fully Bayesian treatment by marginalizing out the model parameters By introducing a continuous-valued latent variable z RM it is possible ∈ tophrasePCAasaprobabilisticlatent-variablemodel.TippingandBishop probabilisticPCA (1999) proposed this latent-variable model as probabilistic PCA (PPCA). PPCA PPCA addresses most of the aforementioned issues, and the PCA solution that we obtained by maximizing the variance