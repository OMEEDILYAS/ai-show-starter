D x = (cid:88) z b (10 = .32) (cid:88) (x(cid:62)b )b = (cid:88) b b(cid:62) x (10.37a) n dn d n d d d d n d=1 d=1 d=1 (cid:32) (cid:33) (cid:32) (cid:33) M D (cid:88) (cid:88) = b b(cid:62) x + b b(cid:62) x , (10.37b) m m n j j n m=1 j=M+1 where we split the sum with D terms into a sum over M and a sum overD M terms.Withthisresult,wefindthatthedisplacementvector − x x˜ ,i.e.,thedifferencevectorbetweentheoriginaldatapointandits n n − projection,is (cid:32) (cid:33) D (cid:88) x x˜ = b b(cid:62) x (10.38a) n − n j j n j=M+1 D (cid:88) = (x(cid:62)b )b . (10.38b) n j j j=M+1 This means the difference is exactly the projection of the data point onto theorthogonalcomplementoftheprincipalsubspace:Weidentifythematrix (cid:80)D b b(cid:62) in (10.38a) as the projection matrix that performs this j=M+1 j j projection. Hence the displacement vector x x˜ lies in the subspace n n − thatisorthogonaltotheprincipalsubspaceasillustratedinFigure10.9. Remark(Low-RankApproximation). In(10.38a),wesawthattheprojectionmatrix,whichprojectsxontox˜,isgivenby M (cid:88) b b(cid:62) = BB(cid:62). (10.39) m m m=1 Byconstructionasasumofrank-onematricesb b(cid:62) weseethatBB(cid:62) is m m Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 10.3 ProjectionPerspective 331 symmetricandhasrankM.Therefore,theaveragesquaredreconstruction errorcanalsobewrittenas 1 (cid:88) N x x˜ 2 = 1 (cid:88) N (cid:13) (cid:13)x BB(cid:62)x (cid:13) (cid:13) 2 (10.40a) N (cid:107) n − n (cid:107) N (cid:13) n − n(cid:13) n=1 n=1 1 (cid:88) N (cid:13) (cid:13)2 = (cid:13)(I BB(cid:62))x (cid:13) . (10.40b) N (cid:13) − n(cid:13) n=1 Findingorthonormalbasisvectorsb 1 ,...,b M ,whichminimizethediffer- PCAfindsthebest ence between the original data x and their projections x˜ , is equivalent rank-M n n to finding the best rank-M approximation BB(cid:62) of the identity matrix I approximationof theidentitymatrix. (seeSection4.6). ♦ Nowwehaveallthetoolstoreformulatethelossfunction(10.29). (cid:13) (cid:13)2 J = 1 (cid:88) N x x˜ 2 (10 = .38b) 1 (cid:88) N (cid:13) (cid:13) (cid:88) D (b(cid:62)x )b (cid:13) (cid:13) . (10.41) M N (cid:107) n − n (cid:107) N (cid:13) (cid:13) j n j(cid:13) (cid:13) n=1 n=1 j=M+1 Wenowexplicitlycomputethesquarednormandexploitthefactthatthe b formanONB,whichyields j N D N D 1 (cid:88) (cid:88) 1 (cid:88) (cid:88) J = (b(cid:62)x )2 = b(cid:62)x b(cid:62)x (10.42a) M N j n N j n j n n=1j=M+1 n=1j=M+1 N D 1 (cid:88) (cid:88) = b(cid:62)x x(cid:62)b , (10.42b) N j n n j n=1j=M+1 where we exploited the symmetry of the dot product in the last step to writeb(cid:62)x = x(cid:62)b .Wenowswapthesumsandobtain j n n j (cid:32) (cid:33) D N D (cid:88) 1 (cid:88) (cid:88) J = b(cid:62) x x(cid:62) b = b(cid:62)Sb (10.43a) M j N n n j j j j=M+1 n=1 j=M+1 (cid:124) (cid:123)(cid:122) (cid:125) =:S (cid:88) D (cid:88) D (cid:16)(cid:16) (cid:88) D (cid:17) (cid:17) = tr(b(cid:62)Sb ) = tr(Sb b(cid:62)) = tr b b(cid:62) S , j j j j j j j=M+1 j=M+1 j=M+1 (cid:124) (cid:123)(cid:122) (cid:125) projectionmatrix (10.43b) whereweexploitedthepropertythatthetraceoperatortr( )(see(4.18)) · is linear and invariant to cyclic permutations of its arguments. Since we assumedthatourdatasetiscentered,i.e.,E[ ] = 0,weidentifyS asthe X data covariance matrix. Since the projection matrix in (10.43b) is constructedasasumofrank-onematricesb b(cid:62) ititselfisofrankD M. j j − Equation (10.43a) implies that we can formulate the average squared reconstruction error equivalently as the