an SVM on example–label pairs. Intuitively, we imagine binary classification data,whichcanbeseparatedbyahyperplaneasillustratedinFigure12.1. Here, every example x (a vector of dimension 2) is a two-dimensional n location (x(1) and x(2)), and the corresponding binary label y is one of n n n twodifferentsymbols(orangecrossorbluedisc).“Hyperplane”isaword that is commonly used in machine learning, and we encountered hyperplanes already in Section 2.8. A hyperplane is an affine subspace of dimension D 1 (if the corresponding vector space is of dimension D). − The examples consist of two classes (there are two possible labels) that have features (the components of the vector representing the example) arranged in such a way as to allow us to separate/classify them by drawingastraightline. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 372 ClassificationwithSupportVectorMachines In the following, we formalize the idea of finding a linear separator of the two classes. We introduce the idea of the margin and then extend linear separators to allow for examples to fall on the “wrong” side, incurring a classification error. We present two equivalent ways of formalizing the SVM: the geometric view (Section 12.2.4) and the loss function view (Section 12.2.5). We derive the dual version of the SVM using Lagrange multipliers (Section 7.2). The dual SVM allows us to observe a third way of formalizing the SVM: in terms of the convex hulls of the examples of eachclass(Section12.3.2).Weconcludebybrieflydescribingkernelsand howtonumericallysolvethenonlinearkernel-SVMoptimizationproblem. 12.1 Separating Hyperplanes Giventwoexamplesrepresentedasvectorsx andx ,onewaytocompute i j thesimilaritybetweenthemisusinganinnerproduct x ,x .Recallfrom i j (cid:104) (cid:105) Section 3.2 that inner products are closely related to the angle between twovectors.Thevalueoftheinnerproductbetweentwovectorsdepends on the length (norm) of each vector. Furthermore, inner products allow ustorigorouslydefinegeometricconceptssuchasorthogonalityandprojections. The main idea behind many classification algorithms is to represent data in RD and then partition this space, ideally in a way that examples with the same label (and no other examples) are in the same partition. In the case of binary classification, the space would be divided into two parts corresponding to the positive and negative classes, respectively. We consider a particularly convenient partition, which is to (linearly) split the space into two halves using a hyperplane. Let example x RD be an ∈ elementofthedataspace.Considerafunction f : RD R (12.2a) → x w,x +b, (12.2b) (cid:55)→ (cid:104) (cid:105) parametrized by w RD and b R. Recall from Section 2.8 that hy- ∈ ∈ perplanes are affine subspaces. Therefore, we define the hyperplane that separatesthetwoclassesinourbinaryclassificationproblemas (cid:8) x RD : f(x) = 0 (cid:9) . (12.3) ∈ An illustration of the hyperplane is shown in Figure 12.2, where the vectorw isavectornormaltothehyperplaneandbtheintercept.Wecan derive that w is a normal vector to the hyperplane in (12.3) by choosing any two examples x and x on the hyperplane and showing that the a b vectorbetweenthemisorthogonaltow.Intheformofanequation, f(x ) f(x ) = w,x +b ( w,x +b) (12.4a) a b a b − (cid:104) (cid:105) − (cid:104) (cid:105) = w,x x , (12.4b) a b (cid:104) − (cid:105) Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 12.1 SeparatingHyperplanes 373 Figure12.2 w Equationofa separating w hyperplane(12.3). (a)Thestandard b wayofrepresenting . theequationin3D. Positive (b)Foreaseof . . drawing,welookat 0 Negative thehyperplaneedge on. (a)Separatinghyperplanein3D (b)Projectionofthesettingin(a)onto aplane where the second line