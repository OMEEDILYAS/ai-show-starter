α+ = 1 and α− = 1. (12.49) n n n:yn=+1 n:yn=−1 Thisimpliestheconstraint N (cid:88) y α = 0. (12.50) n n n=1 (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 388 ClassificationwithSupportVectorMachines Thisresultcanbeseenbymultiplyingouttheindividualclasses N (cid:88) (cid:88) (cid:88) y α = (+1)α++ ( 1)α− (12.51a) n n n − n n=1 n:yn=+1 n:yn=−1 (cid:88) (cid:88) = α+ α− = 1 1 = 0. (12.51b) n − n − n:yn=+1 n:yn=−1 Theobjectivefunction(12.48)andtheconstraint(12.50),alongwiththe assumptionthatα (cid:62) 0,giveusaconstrained(convex)optimizationproblem. This optimization problem can be shown to be the same as that of thedualhardmarginSVM(BennettandBredensteiner,2000a). Remark. Toobtainthesoftmargindual,weconsiderthereducedhull.The reducedhull reduced hull is similar to the convex hull but has an upper bound to the size of the coefficients α. The maximum possible value of the elements of α restricts the size that the convex hull can take. In other words, the bound on α shrinks the convex hull to a smaller volume (Bennett and Bredensteiner,2000b). ♦ 12.4 Kernels Consider the formulation of the dual SVM (12.41). Notice that the inner product in the objective occurs only between examples x and x . i j There are no inner products between the examples and the parameters. Therefore, if we consider a set of features φ(x ) to represent x , the only i i change in the dual SVM will be to replace the inner product. This modularity, where the choice of the classification method (the SVM) and the choice of the feature representation φ(x) can be considered separately, provides flexibility for us to explore the two problems independently. In thissection,wediscusstherepresentationφ(x)andbrieflyintroducethe ideaofkernels,butdonotgointothetechnicaldetails. Sinceφ(x)couldbeanon-linearfunction,wecanusetheSVM(which assumes a linear classifier) to construct classifiers that are nonlinear in the examples x . This provides a second avenue, in addition to the soft n margin, for users to deal with a dataset that is not linearly separable. It turnsoutthattherearemanyalgorithmsandstatisticalmethodsthathave this property that we observed in the dual SVM: the only inner products are those that occur between examples. Instead of explicitly defining a non-linear feature map φ( ) and computing the resulting inner product · betweenexamplesx andx ,wedefineasimilarityfunctionk(x ,x )bei j i j kernel tweenx i andx j .Foracertainclassofsimilarityfunctions,calledkernels, the similarity function implicitly defines a non-linear feature map φ( ). TheinputsX ofthe Kernels are by definition functions k : R for which there exis · ts X ×X → kernelfunctioncan aHilbertspace andφ : afeaturemapsuchthat beverygeneraland H X → H arenotnecessarily k(x ,x ) = φ(x ),φ(x ) . (12.52) restrictedtoRD. i j (cid:104) i j (cid:105)H Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 12.4 Kernels 389 Figure12.10 SVM withdifferent kernels.Notethat whilethedecision boundaryis nonlinear,the underlyingproblem beingsolvedisfora linearseparating hyperplane(albeit withanonlinear kernel). Firstfeature erutaefdnoceS Firstfeature (a)SVMwithlinearkernel erutaefdnoceS (b)SVMwithRBFkernel Firstfeature erutaefdnoceS Firstfeature (c)SVMwithpolynomial(degree2)kernel erutaefdnoceS (d)SVMwithpolynomial(degree3)kernel There is a unique reproducing kernel Hilbert space associated with every kernel k (Aronszajn, 1950; Berlinet and Thomas-Agnan, 2004). In this unique association, φ(x) = k( ,x) is called the canonical feature map. canonicalfeature · The generalization from an inner product to a kernel function (12.52) is map knownasthekerneltrick(Scho¨lkopfandSmola,2002;Shawe-Taylorand kerneltrick Cristianini,2004),asithidesawaytheexplicitnon-linearfeaturemap. ThematrixK RN×N,resultingfromtheinnerproductsortheappli- ∈ cation of k( , ) to a dataset, is called the Gram matrix, and is often just Grammatrix · · referred to