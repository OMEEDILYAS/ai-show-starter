x , and, hence, change n n thedistancetothehyperplane.Aswewillseeshortly,wedefinethescale basedontheequationofthehyperplane(12.3)itself. Consider a hyperplane w,x +b, and an example x as illustrated in a (cid:104) (cid:105) Figure 12.4. Without loss of generality, we can consider the example x a to be on the positive side of the hyperplane, i.e., w,x + b > 0. We a (cid:104) (cid:105) would like to compute the distance r > 0 of x from the hyperplane. We a do so by considering the orthogonal projection (Section 3.8) of x onto a the hyperplane, which we denote by x(cid:48). Since w is orthogonal to the a Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 12.2 PrimalSupportVectorMachine 375 . Figure12.4 Vector x a r additiontoexpress w . distanceto x (cid:48)a hyperplane: xa=x(cid:48) a +r (cid:107)w w (cid:107) . . 0 hyperplane,weknowthatthedistancer isjustascalingofthisvectorw. If the length of w is known, then we can use this scaling factor r factor to work out the absolute distance between x and x(cid:48). For convenience, a a we choose to use a vector of unit length (its norm is 1) and obtain this by dividing w by its norm, w . Using vector addition (Section 2.4), we (cid:107)w(cid:107) obtain w x = x(cid:48) +r . (12.8) a a w (cid:107) (cid:107) Another way of thinking about r is that it is the coordinate of x in the a subspacespannedbyw/ w .Wehavenowexpressedthedistanceofx a (cid:107) (cid:107) from the hyperplane as r, and if we choose x to be the point closest to a thehyperplane,thisdistancer isthemargin. Recall that we would like the positive examples to be further than r from the hyperplane, and the negative examples to be further than distance r (in the negative direction) from the hyperplane. Analogously to the combination of (12.5) and (12.6) into (12.7), we formulate this objectiveas y ( w,x +b) (cid:62) r. (12.9) n n (cid:104) (cid:105) In other words, we combine the requirements that examples are at least r away from the hyperplane (in the positive and negative direction) into onesingleinequality. Since we are interested only in the direction, we add an assumption to our model that the parameter vector w is of unit length, i.e., w = 1, (cid:107) (cid:107) where we use the Euclidean norm w = √w(cid:62)w (Section 3.1). This Wewillseeother (cid:107) (cid:107) assumption also allows a more intuitive interpretation of the distance r choicesofinner products (12.8)sinceitisthescalingfactorofavectoroflength1. (Section3.2)in Remark. A reader familiar with other presentations of the margin would Section12.4. notice that our definition of w = 1 is different from the standard (cid:107) (cid:107) presentation if the SVM was the one provided by Scho¨lkopf and Smola (2002), for example. In Section 12.2.3, we will show the equivalence of bothapproaches. ♦ Collectingthethreerequirementsintoasingleconstrainedoptimization (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 376 ClassificationwithSupportVectorMachines Figure12.5 . x a Derivationofthe r w margin:r= 1 . . (cid:107)w(cid:107) x (cid:48)a (cid:104)w , (cid:104)w x , x (cid:105)+ (cid:105)+ b = b = 1 0 problem,weobtaintheobjective max r w,b,r (cid:124)(cid:123)(cid:122)(cid:125) margin (12.10) subjectto y ( w,x +b) (cid:62) r, w = 1, r > 0, n n (cid:104) (cid:105) (cid:107) (cid:107) (cid:124) (cid:123)(cid:122) (cid:125)