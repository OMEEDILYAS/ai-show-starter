the margin or even on the wrong side of the hyperplane (refer to (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 380 ClassificationwithSupportVectorMachines Figure12.7 Soft marginSVMallows . w examplestobe withinthemarginor ξ onthewrongsideof thehyperplane.The slackvariableξ . (cid:104)w measuresthe x + (cid:104)w , x distanceofa , x (cid:105)+ positiveexample x+tothepositive (cid:105)+ b = b marginhyperplane = 1 (cid:104)w,x(cid:105)+b=1 0 whenx+isonthe wrongside. Figure 12.7). We subtract the value of ξ from the margin, constraining n ξ to be non-negative. To encourage correct classification of the samples, n weaddξ totheobjective n N 1 (cid:88) min w 2+C ξ (12.26a) n w,b,ξ 2(cid:107) (cid:107) n=1 subjectto y ( w,x +b) (cid:62) 1 ξ (12.26b) n n n (cid:104) (cid:105) − ξ (cid:62) 0 (12.26c) n forn = 1,...,N.In contrasttothe optimizationproblem(12.18) forthe softmarginSVM hard margin SVM, this one is called the soft margin SVM. The parameter C > 0tradesoffthesizeofthemarginandthetotalamountofslackthat regularization we have. This parameter is called the regularization parameter since, as parameter wewillseeinthefollowingsection,themargintermintheobjectivefunction (12.26a) is a regularization term. The margin term w 2 is called (cid:107) (cid:107) regularizer the regularizer, and in many books on numerical optimization, the regularization parameter is multiplied with this term (Section 8.2.3). This is in contrast to our formulation in this section. Here a large value of C implies low regularization, as we give the slack variables larger weight, hence giving more priority to examples that do not lie on the correct side Thereare ofthemargin. alternative Remark. In the formulation of the soft margin SVM (12.26a) w is regparametrizationsof thisregularization, ularized, but b is not regularized. We can see this by observing that the whichis regularization term does not contain b. The unregularized term b comwhy(12.26a)isalso plicatestheoreticalanalysis(SteinwartandChristmann,2008,chapter1) oftenreferredtoas anddecreasescomputationalefficiency(Fanetal.,2008). theC-SVM. ♦ 12.2.5 Soft Margin SVM: Loss Function View Let us consider a different approach for deriving the SVM, following the principle of empirical risk minimization (Section 8.2). For the SVM, we Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 12.2 PrimalSupportVectorMachine 381 choosehyperplanesasthehypothesisclass,thatis f(x) = w,x +b. (12.27) (cid:104) (cid:105) We will see in this section that the margin corresponds to the regularization term. The remaining question is, what is the loss function? In con- lossfunction trast to Chapter 9, where we consider regression problems (the output of the predictor is a real number), in this chapter, we consider binary classification problems (the output of the predictor is one of two labels +1, 1 ). Therefore, the error/loss function for each single example– { − } label pair needs to be appropriate for binary classification. For example, the squared loss that is used for regression (9.10b) is not suitable for binaryclassification. Remark. Theideallossfunctionbetweenbinarylabelsistocountthenumber of mismatches between the prediction and the label. This means that forapredictorf appliedtoanexamplex ,wecomparetheoutputf(x ) n n with the label y . We define the loss to be zero if they match, and one if n they do not match. This is denoted by 1(f(x ) = y ) and is called the n n (cid:54) zero-one loss. Unfortunately, the zero-one loss results in a combinatorial zero-oneloss optimizationproblemforfindingthebestparametersw,b.Combinatorial optimization problems (in contrast to continuous optimization problems discussedinChapter7)areingeneralmorechallengingtosolve. ♦ WhatisthelossfunctioncorrespondingtotheSVM?Considertheerror between the output