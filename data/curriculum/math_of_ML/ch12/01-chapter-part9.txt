In the following, we consider an equivalent optimization problem (the so-called dual view), which is independent of the number of features. Instead, the number of parameters increases with the number of examples inthetrainingset.WesawasimilarideaappearinChapter10,wherewe expressedthelearningprobleminawaythatdoesnotscalewiththenumber of features. This is useful for problems where we have more features than the number of examples in the training dataset. The dual SVM also has the additional advantage that it easily allows kernels to be applied, as we shall see at the end of this chapter. The word “dual” appears often in mathematical literature, and in this particular case it refers to convex duality.Thefollowingsubsectionsareessentiallyanapplicationofconvex duality,whichwediscussedinSection7.2. 12.3.1 Convex Duality via Lagrange Multipliers Recall the primal soft margin SVM (12.26a). We call the variables w, b, andξ correspondingtotheprimalSVMtheprimalvariables.Weuseα n (cid:62) InChapter7,we 0astheLagrangemultipliercorrespondingtotheconstraint(12.26b)that usedλasLagrange the examples are classified correctly and γ (cid:62) 0 as the Lagrange multi- multipliers.Inthis n section,wefollow plier corresponding to the non-negativity constraint of the slack variable; thenotation see(12.26c).TheLagrangianisthengivenby commonlychosenin SVMliterature,and useαandγ. N 1 (cid:88) L(w,b,ξ,α,γ) = w 2+C ξ (12.34) n 2(cid:107) (cid:107) n=1 N N (cid:88) (cid:88) α (y ( w,x +b) 1+ξ ) γ ξ . n n n n n n − (cid:104) (cid:105) − − n=1 n=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) constraint(12.26b) constraint(12.26c) (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 384 ClassificationwithSupportVectorMachines BydifferentiatingtheLagrangian(12.34)withrespecttothethreeprimal variablesw,b,andξ respectively,weobtain ∂L (cid:88) N = w(cid:62) α y x (cid:62), (12.35) ∂w − n n n n=1 ∂L (cid:88) N = α y , (12.36) ∂b − n n n=1 ∂L = C α γ . (12.37) ∂ξ − n − n n We now find the maximum of the Lagrangian by setting each of these partialderivativestozero.Bysetting(12.35)tozero,wefind N (cid:88) w = α y x , (12.38) n n n n=1 representertheorem which is a particular instance of the representer theorem (Kimeldorf and Therepresenter Wahba, 1970). Equation (12.38) states that the optimal weight vector in theoremisactually the primal is a linear combination of the examples x . Recall from Secn acollectionof tion 2.6.1 that this means that the solution of the optimization problem theoremssaying lies in the span of training data. Additionally, the constraint obtained by thatthesolutionof minimizing setting (12.36) to zero implies that the optimal weight vector is an affine empiricalriskliesin combination of the examples. The representer theorem turns out to hold thesubspace for very general settings of regularized empirical risk minimization (Hof- (Section2.4.3) mann et al., 2008; Argyriou and Dinuzzo, 2014). The theorem has more definedbythe examples. general versions (Scho¨lkopf et al., 2001), and necessary and sufficient conditionsonitsexistencecanbefoundinYuetal.(2013). Remark. The representer theorem (12.38) also provides an explanation of the name “support vector machine.” The examples x , for which the n corresponding parameters α = 0, do not contribute to the solution w at n supportvector all. The other examples, where α n > 0, are called support vectors since they“support”thehyperplane. ♦ By substituting the expression for w into the Lagrangian (12.34), we obtainthedual (cid:42) (cid:43) N N N N 1 (cid:88)(cid:88) (cid:88) (cid:88) D(ξ,α,γ) = y y α α x ,x y α y α x ,x i