of a predictor f(x ) and the label y . The loss den n scribes the error that is made on the training data. An equivalent way to derive(12.26a)istousethehingeloss hingeloss (cid:96)(t) = max 0,1 t where t = yf(x) = y( w,x +b). (12.28) { − } (cid:104) (cid:105) If f(x) is on the correct side (based on the corresponding label y) of the hyperplane, and further than distance 1, this means that t (cid:62) 1 and the hinge loss returns a value of zero. If f(x) is on the correct side but too close to the hyperplane (0 < t < 1), the example x is within the margin, and the hinge loss returns a positive value. When the example is on the wrongsideofthehyperplane(t < 0),thehingelossreturnsanevenlarger value,whichincreaseslinearly.Inotherwords,wepayapenaltyoncewe are closer than the margin to the hyperplane, even if the prediction is correct, and the penalty increases linearly. An alternative way to express thehingelossisbyconsideringitastwolinearpieces (cid:40) 0 if t (cid:62) 1 (cid:96)(t) = , (12.29) 1 t if t < 1 − as illustrated in Figure 12.8. The loss corresponding to the hard margin SVM12.18isdefinedas (cid:40) 0 if t (cid:62) 1 (cid:96)(t) = . (12.30) if t < 1 ∞ (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 382 ClassificationwithSupportVectorMachines Figure12.8 The 4 hingelossisa convexupperbound ofzero-oneloss. 2 0 2 0 2 − t t 1,0 xam } − { Zero-oneloss Hingeloss This loss can be interpreted as never allowing any examples inside the margin. For a given training set (x ,y ),...,(x ,y ) , we seek to minimize 1 1 N N { } the total loss, while regularizing the objective with (cid:96) -regularization (see 2 Section 8.2.3). Using the hinge loss (12.28) gives us the unconstrained optimizationproblem N 1 (cid:88) min w 2+C max 0,1 y ( w,x +b) . (12.31) n n w,b 2(cid:107) (cid:107) { − (cid:104) (cid:105) } (cid:124) (cid:123)(cid:122) (cid:125) n=1 (cid:124) (cid:123)(cid:122) (cid:125) regularizer errorterm regularizer Thefirsttermin(12.31)iscalledtheregularizationtermortheregularizer lossterm (seeSection9.2.3),andthesecondtermiscalledthelosstermortheerror errorterm term.RecallfromSection12.2.4thattheterm 1 w 2 arisesdirectlyfrom 2 (cid:107) (cid:107) the margin. In other words, margin maximization can be interpreted as regularization regularization. In principle, the unconstrained optimization problem in (12.31) can be directly solved with (sub-)gradient descent methods as described in Section7.1.Toseethat(12.31)and(12.26a)areequivalent,observethat thehingeloss(12.28)essentiallyconsistsoftwolinearparts,asexpressed in(12.29).Considerthehingelossforasingleexample–labelpair(12.28). We can equivalently replace minimization of the hinge loss over t with a minimizationofaslackvariableξ withtwoconstraints.Inequationform, minmax 0,1 t (12.32) t { − } isequivalentto min ξ ξ,t (12.33) subjectto ξ (cid:62) 0, ξ (cid:62) 1 t. − By substituting this expression into (12.31) and rearranging one of the constraints,weobtainexactlythesoftmarginSVM(12.26a). Remark. Letuscontrastourchoiceofthelossfunctioninthissectiontothe loss function for linear regression in Chapter 9. Recall from Section 9.2.1 that for finding maximum likelihood estimators, we usually minimize the Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 12.3 DualSupportVectorMachine 383 negative log-likelihood. Furthermore, since the likelihood term for linear regressionwithGaussiannoiseisGaussian,thenegativelog-likelihoodfor eachexampleisasquarederrorfunction.Thesquarederrorfunctionisthe lossfunctionthatisminimizedwhenlookingforthemaximumlikelihood solution. ♦ 12.3 Dual Support Vector Machine The description of the SVM in the previous sections, in terms of the variableswandb,isknownastheprimalSVM.Recallthatweconsiderinputs x RD with D features. Since w is of the same dimension as x, this ∈ means that the number of parameters (the dimension of w) of the optimizationproblemgrowslinearlywiththenumberoffeatures.