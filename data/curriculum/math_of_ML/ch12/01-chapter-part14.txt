(12.41) has optimization variablesthathavethesizeofthenumberN ofexamples. To express the primal SVM in the standard form (7.45) for quadratic programming, let us assume that we use the dot product (3.5) as the inner product. We rearrange the equation for the primal SVM (12.26a), Recallfrom suchthattheoptimizationvariablesareallontherightandtheinequality Section3.2thatwe usethephrasedot oftheconstraintmatchesthestandardform.Thisyieldstheoptimization producttomeanthe N innerproducton min 1 w 2+C (cid:88) ξ Euclideanvector n w,b,ξ 2(cid:107) (cid:107) space. n=1 (12.55) y x(cid:62)w y b ξ (cid:54) 1 subjectto − ξ n (cid:54) n 0 − n − n − n − n = 1,...,N.Byconcatenatingthevariablesw,b,x intoasinglevector, n andcarefullycollectingtheterms,weobtainthefollowingmatrixformof thesoftmarginSVM:  (cid:62)     w (cid:20) (cid:21) w w w m ,b in ,ξ 2 1  ξ b 0 N I + D 1,D 0 0 N D + , 1 N ,N + + 1 1  ξ b+ (cid:2) 0 D+1,1 C1 N,1 (cid:3)(cid:62)  ξ b   (cid:20) (cid:21) w (cid:20) (cid:21) YX y I 1 subjectto − − − N b (cid:54) − N,1 . 0 I 0 N,D+1 − N ξ N,1 (12.56) In the preceding optimization problem, the minimization is over the parameters [w(cid:62),b,ξ(cid:62)](cid:62) RD+1+N, and we use the notation: I to repm ∈ resent the identity matrix of size m m, 0 to represent the matrix m,n × of zeros of size m n, and 1 to represent the matrix of ones of size m,n × m n. In addition, y is the vector of labels [y , ,y ](cid:62), Y = diag(y) 1 N × ··· (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 392 ClassificationwithSupportVectorMachines is an N by N matrix where the elements of the diagonal are from y, and X RN×D isthematrixobtainedbyconcatenatingalltheexamples. ∈ Wecansimilarlyperformacollectionoftermsforthedualversionofthe SVM (12.41). To express the dual SVM in standard form, we first have to expressthekernelmatrixK suchthateachentryisK = k(x ,x ).Ifwe ij i j have an explicit feature representation x then we define K = x ,x . i ij i j (cid:104) (cid:105) Forconvenienceofnotationweintroduceamatrixwithzeroseverywhere except on the diagonal, where we store the labels, that is, Y = diag(y). ThedualSVMcanbewrittenas 1 min α(cid:62)YKYα 1(cid:62) α α 2 − N,1  y(cid:62)  subjectto  − y(cid:62)  α (cid:54) (cid:20) 0 N+2,1 (cid:21) . (12.57)  I N C1 N,1 − I N Remark. In Sections 7.3.1 and 7.3.2, we introduced the standard forms of the constraints to be inequality constraints. We will express the dual SVM’sequalityconstraintastwoinequalityconstraints,i.e., Ax = b isreplacedby Ax (cid:54) b and Ax (cid:62) b. (12.58) Particularsoftwareimplementationsofconvexoptimizationmethodsmay providetheabilitytoexpressequalityconstraints. ♦ Since there are many different possible views of the SVM, there are many approaches for solving the resulting optimization problem. The approach presented here, expressing the SVM problem in standard convex optimizationform,isnotoftenusedinpractice.Thetwomainimplementations of SVM solvers are Chang and Lin (2011) (which is open source) andJoachims(1999).SinceSVMshaveaclearandwell-definedoptimization problem, many approaches based on numerical optimization techniques (Nocedal and Wright, 2006) can be applied (Shawe-Taylor and Sun,2011). 12.6 Further Reading The SVM is one of many approaches for studying binary classification. Other approaches include the perceptron, logistic regression, Fisher discriminant,nearestneighbor,naiveBayes,andrandomforest(Bishop,2006; Murphy, 2012). A short tutorial on SVMs and kernels on