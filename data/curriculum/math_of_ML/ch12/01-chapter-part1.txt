12 Classification with Support Vector Machines In many situations, we want our machine learning algorithm to predict oneofanumberof(discrete)outcomes.Forexample,anemailclientsorts mail into personal mail and junk mail, which has two outcomes. Another example is a telescope that identifies whether an object in the night sky isagalaxy,star,orplanet.Thereareusuallyasmallnumberofoutcomes, and more importantly there is usually no additional structure on these Anexampleof outcomes. In this chapter, we consider predictors that output binary valstructureisifthe ues,i.e.,thereareonlytwopossibleoutcomes.Thismachinelearningtask outcomeswere is called binary classification. This is in contrast to Chapter 9, where we ordered,likeinthe consideredapredictionproblemwithcontinuous-valuedoutputs. caseofsmall, medium,andlarge Forbinaryclassification,thesetofpossiblevaluesthatthelabel/output t-shirts. can attain is binary, and for this chapter we denote them by +1, 1 . In { − } binaryclassification otherwords,weconsiderpredictorsoftheform f : RD +1, 1 . (12.1) → { − } Recall from Chapter 8 that we represent each example (data point) x n Inputexamplexn as a feature vector of D real numbers. The labels are often referred to as mayalsobereferred the positive and negative classes, respectively. One should be careful not toasinputs,data to infer intuitive attributes of positiveness of the +1 class. For example, points,features,or in a cancer detection task, a patient with cancer is often labeled +1. In instances. class principle, any two distinct values can be used, e.g., True,False , 0,1 { } { } Forprobabilistic or red,blue . The problem of binary classification is well studied, and { } models,itis wedeferasurveyofotherapproachestoSection12.6. mathematically We present an approach known as the support vector machine (SVM), convenienttouse whichsolvesthebinaryclassificationtask.Asinregression,wehaveasu- {0,1}asabinary representation;see pervised learning task, where we have a set of examples x n RD along ∈ theremarkafter with their corresponding (binary) labels y +1, 1 . Given a trainn Example6.12. ingdatasetconsistingofexample–labelpairs ∈ (x { ,y ) − ,.. } .,(x ,y ) ,we 1 1 N N { } wouldliketoestimateparametersofthemodelthatwillgivethesmallest classification error. Similar to Chapter 9, we consider a linear model, and hide away the nonlinearity in a transformation φ of the examples (9.13). WewillrevisitφinSection12.4. The SVM provides state-of-the-art results in many applications, with sound theoretical guarantees (Steinwart and Christmann, 2008). There aretwomainreasonswhywechosetoillustratebinaryclassificationusing 370 ThismaterialwillbepublishedbyCambridgeUniversityPressasMathematicsforMachineLearningbyMarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng.Thispre-publicationversionis freetoviewanddownloadforpersonaluseonly.Notforre-distribution,re-saleoruseinderivativeworks.(cid:13)cbyM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2019.https://mml-book.com. ClassificationwithSupportVectorMachines 371 Figure12.1 Example2Ddata, illustratingthe intuitionofdata wherewecanfinda linearclassifierthat separatesorange crossesfromblue discs. x(1) )2(x SVMs.First,theSVMallowsforageometricwaytothinkaboutsupervised machinelearning.WhileinChapter9weconsideredthemachinelearning problem in terms of probabilistic models and attacked it using maximum likelihood estimation and Bayesian inference, here we will consider an alternative approach where we reason geometrically about the machine learning task. It relies heavily on concepts, such as inner products and projections,whichwediscussedinChapter3.Thesecondreasonwhywe find SVMs instructive is that in contrast to Chapter 9, the optimization problem for SVM does not admit an analytic solution so that we need to resorttoavarietyofoptimizationtoolsintroducedinChapter7. The SVM view of machine learning is subtly different from the maximum likelihood view of Chapter 9. The maximum likelihood view proposesamodelbasedonaprobabilisticviewofthedatadistribution,from whichanoptimizationproblemisderived.Incontrast,theSVMviewstarts bydesigningaparticularfunctionthatistobeoptimizedduringtraining, based on geometric intuitions. We have seen something similar already in Chapter 10, where we derived PCA from geometric principles. In the SVM case, we start by designing a loss function that is to be minimized on training data, following the principles of empirical risk minimization (Section8.2). Let us derive the optimization problem corresponding to training