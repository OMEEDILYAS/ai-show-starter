(cid:124) (cid:123)(cid:122) (cid:125) datafitting normalization which says that we want to maximize the margin r while ensuring that thedataliesonthecorrectsideofthehyperplane. Remark. Theconceptofthemarginturnsouttobehighlypervasiveinmachine learning. It was used by Vladimir Vapnik and Alexey Chervonenkis to show that when the margin is large, the “complexity” of the function class is low, and hence learning is possible (Vapnik, 2000). It turns out that the concept is useful for various different approaches for theoretically analyzing generalization error (Steinwart and Christmann, 2008; Shalev-ShwartzandBen-David,2014). ♦ 12.2.2 Traditional Derivation of the Margin Intheprevioussection,wederived(12.10)bymakingtheobservationthat we are only interested in the direction of w and not its length, leading to the assumption that w = 1. In this section, we derive the margin max- (cid:107) (cid:107) imizationproblembymakingadifferentassumption.Insteadofchoosing that the parameter vector is normalized, we choose a scale for the data. Wechoosethisscalesuchthatthevalueofthepredictor w,x +bis1at (cid:104) (cid:105) Recallthatwe the closest example. Let us also denote the example in the dataset that is currentlyconsider closesttothehyperplanebyx . a linearlyseparable Figure12.5isidenticaltoFigure12.4,exceptthatnowwerescaledthe data. axes, such that the example x lies exactly on the margin, i.e., w,x + a a (cid:104) (cid:105) b = 1. Since x(cid:48) is the orthogonal projection of x onto the hyperplane, it a a mustbydefinitionlieonthehyperplane,i.e., w,x(cid:48) +b = 0. (12.11) (cid:104) a(cid:105) Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 12.2 PrimalSupportVectorMachine 377 Bysubstituting(12.8)into(12.11),weobtain (cid:28) (cid:29) w w,x r +b = 0. (12.12) a − w (cid:107) (cid:107) Exploitingthebilinearityoftheinnerproduct(seeSection3.2),weget w,w w,x +b r(cid:104) (cid:105) = 0. (12.13) a (cid:104) (cid:105) − w (cid:107) (cid:107) Observethatthefirsttermis1byourassumptionofscale,i.e., w,x + a (cid:104) (cid:105) b = 1. From (3.16) in Section 3.1, we know that w,w = w 2. Hence, (cid:104) (cid:105) (cid:107) (cid:107) thesecondtermreducestor w .Usingthesesimplifications,weobtain (cid:107) (cid:107) 1 r = . (12.14) w (cid:107) (cid:107) This means we derived the distance r in terms of the normal vector w of the hyperplane. At first glance, this equation is counterintuitive as we Wecanalsothinkof seem to have derived the distance from the hyperplane in terms of the thedistanceasthe projectionerrorthat length of the vector w, but we do not yet know this vector. One way to incurswhen think about it is to consider the distance r to be a temporary variable projectingxaonto that we only use for this derivation. Therefore, for the rest of this section thehyperplane. we will denote the distance to the hyperplane by 1 . In Section 12.2.3, (cid:107)w(cid:107) we will see that the choice that the margin equals 1 is equivalent to our previousassumptionof w = 1inSection12.2.1. (cid:107) (cid:107) Similar to the argument to obtain (12.9), we want the positive and negativeexamplestobeatleast1awayfromthehyperplane,whichyields thecondition y ( w,x +b) (cid:62) 1. (12.15) n n (cid:104) (cid:105) Combining the margin maximization with the fact that examples need to beonthecorrectsideofthehyperplane(basedontheirlabels)givesus 1 max (12.16) w,b w (cid:107) (cid:107) subjecttoy ( w,x +b) (cid:62) 1 forall n = 1,...,N. (12.17) n n (cid:104) (cid:105) Instead of maximizing the reciprocal of the norm as in (12.16), we often minimize thesquared norm. Wealso often includea constant 1 that does Thesquarednorm 2 not affect the optimal w,b but yields a tidier form when we compute the resultsinaconvex quadratic gradient.Then,ourobjectivebecomes programming