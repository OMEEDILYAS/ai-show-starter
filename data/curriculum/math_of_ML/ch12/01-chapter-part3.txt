is obtained by the linearity of the inner product (Section 3.2). Since we have chosen x and x to be on the hyperplane, a b this implies that f(x ) = 0 and f(x ) = 0 and hence w,x x = 0. a b a b (cid:104) − (cid:105) Recall that two vectors are orthogonal when their inner product is zero. wisorthogonalto Therefore,weobtainthatwisorthogonaltoanyvectoronthehyperplane. anyvectoronthe hyperplane. Remark. Recall from Chapter 2 that we can think of vectors in different ways. In this chapter, we think of the parameter vector w as an arrow indicating a direction, i.e., we consider w to be a geometric vector. In contrast, we think of the example vector x as a data point (as indicated by its coordinates), i.e., we consider x to be the coordinates of a vector withrespecttothestandardbasis. ♦ When presented with a test example, we classify the example as positive or negative depending on the side of the hyperplane on which it occurs.Notethat(12.3)notonlydefinesahyperplane;itadditionallydefines a direction. In other words, it defines the positive and negative side of the hyperplane. Therefore, to classify a test example x , we calcutest late the value of the function f(x ) and classify the example as +1 if test f(x ) (cid:62) 0 and 1 otherwise. Thinking geometrically, the positive extest − ampleslie“above”thehyperplaneandthenegativeexamples“below”the hyperplane. When training the classifier, we want to ensure that the examples with positivelabelsareonthepositivesideofthehyperplane,i.e., w,x +b (cid:62) 0 when y = +1 (12.5) n n (cid:104) (cid:105) andtheexampleswithnegativelabelsareonthenegativeside,i.e., w,x +b < 0 when y = 1. (12.6) n n (cid:104) (cid:105) − Refer to Figure 12.2 for a geometric intuition of positive and negative examples.Thesetwoconditionsareoftenpresentedinasingleequation y ( w,x +b) (cid:62) 0. (12.7) n n (cid:104) (cid:105) Equation(12.7)isequivalentto(12.5)and(12.6)whenwemultiplyboth sidesof(12.5)and(12.6)withy = 1andy = 1,respectively. n n − (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 374 ClassificationwithSupportVectorMachines Figure12.3 Possibleseparating hyperplanes.There aremanylinear classifiers(green lines)thatseparate orangecrossesfrom bluediscs. x(1) )2(x 12.2 Primal Support Vector Machine Based on the concept of distances from points to a hyperplane, we now are in a position to discuss the support vector machine. For a dataset (x ,y ),...,(x ,y ) thatislinearlyseparable,wehaveinfinitelymany 1 1 N N { } candidate hyperplanes (refer to Figure 12.3), and therefore classifiers, thatsolveourclassificationproblemwithoutany(training)errors.Tofind a unique solution, one idea is to choose the separating hyperplane that maximizes the margin between the positive and negative examples. In otherwords,wewantthepositiveandnegativeexamplestobeseparated Aclassifierwith byalargemargin(Section12.2.1).Inthefollowing,wecomputethedislargemarginturns tance between an example and a hyperplane to derive the margin. Recall outtogeneralize that the closest point on the hyperplane to a given point (example x ) is n well(Steinwartand obtainedbytheorthogonalprojection(Section3.8). Christmann,2008). 12.2.1 Concept of the Margin margin The concept of the margin is intuitively simple: It is the distance of the Therecouldbetwo separating hyperplane to the closest examples in the dataset, assuming ormoreclosest that the dataset is linearly separable. However, when trying to formalize examplestoa thisdistance,thereisatechnicalwrinklethatmaybeconfusing.Thetechhyperplane. nical wrinkle is that we need to define a scale at which to measure the distance.Apotentialscaleistoconsiderthescaleofthedata,i.e.,theraw values of x . There are problems with this, as we could change the units n of measurement of x and change the values in