as the kernel matrix. Kernels must be symmetric and positive kernelmatrix semidefinite functions so that every kernel matrix K is symmetric and positivesemidefinite(Section3.2.3): z RN : z(cid:62)Kz (cid:62) 0. (12.53) ∀ ∈ Some popular examples of kernels for multivariate real-valued data x i RD are the polynomial kernel, the Gaussian radial basis function kerne ∈ l, andtherationalquadratickernel(Scho¨lkopfandSmola,2002;Rasmussen (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 390 ClassificationwithSupportVectorMachines andWilliams,2006).Figure12.10illustratestheeffectofdifferentkernels on separating hyperplanes on an example dataset. Note that we are still solving for hyperplanes, that is, the hypothesis class of functions are still linear.Thenon-linearsurfacesareduetothekernelfunction. Remark. Unfortunately for the fledgling machine learner, there are multiple meanings of the word “kernel.” In this chapter, the word “kernel” comesfromtheideaofthereproducingkernelHilbertspace(RKHS)(Aronszajn,1950;Saitoh,1988).Wehavediscussedtheideaofthekernelinlinearalgebra(Section2.7.3),wherethekernelisanotherwordforthenull space. The third common use of theword “kernel” in machine learning is thesmoothingkernelinkerneldensityestimation(Section11.5). ♦ Since the explicit representation φ(x) is mathematically equivalent to the kernel representation k(x ,x ), a practitioner will often design the i j kernel function such that it can be computed more efficiently than the inner product between explicit feature maps. For example, consider the polynomial kernel (Scho¨lkopf and Smola, 2002), where the number of terms in the explicit expansion grows very quickly (even for polynomials of low degree) when the input dimension is large. The kernel function only requires one multiplication per input dimension, which can provide significant computational savings. Another example is the Gaussian radial basis function kernel (Scho¨lkopf and Smola, 2002; Rasmussen and Williams,2006),wherethecorrespondingfeaturespaceisinfinitedimensional. In this case, we cannot explicitly represent the feature space but Thechoiceof canstillcomputesimilaritiesbetweenapairofexamplesusingthekernel. kernel,aswellas Another useful aspect of the kernel trick is that there is no need for theparametersof the original data to be already represented as multivariate real-valued thekernel,isoften data. Note that theinner product is defined on theoutput of the function chosenusingnested cross-validation φ( ), but does not restrict the input to real numbers. Hence, the function · (Section8.6.1). φ( ) and the kernel function k( , ) can be defined on any object, e.g., · · · sets, sequences, strings, graphs, and distributions (Ben-Hur et al., 2008; Ga¨rtner,2008;Shietal.,2009;Sriperumbuduretal.,2010;Vishwanathan etal.,2010). 12.5 Numerical Solution We conclude our discussion of SVMs by looking at how to express the problems derived in this chapter in terms of the concepts presented in Chapter 7. We consider two different approaches for finding the optimal solutionfortheSVM.FirstweconsiderthelossviewofSVM8.2.2andexpressthisasanunconstrainedoptimizationproblem.Thenweexpressthe constrained versions of the primal and dual SVMs as quadratic programs instandardform7.3.2. Consider the loss function view of the SVM (12.31). This is a convex unconstrainedoptimizationproblem,butthehingeloss(12.28)isnotdifDraft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 12.5 NumericalSolution 391 ferentiable. Therefore, we apply a subgradient approach for solving it. However, the hinge loss is differentiable almost everywhere, except for one single point at the hinge t = 1. At this point, the gradient is a set of possiblevaluesthatliebetween0and 1.Therefore,thesubgradientg of − thehingelossisgivenby  1 t < 1  − g(t) = [ 1,0] t = 1 . (12.54)  − 0 t > 1 Usingthissubgradient,wecanapplytheoptimizationmethodspresented inSection7.1. Both the primal and the dual SVM result in a convex quadratic programmingproblem(constrainedoptimization).NotethattheprimalSVM in (12.26a) has optimization variables that have the size of the dimension D of the input examples. The dual SVM in