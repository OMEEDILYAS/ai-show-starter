1 problemforthe min w 2 (12.18) SVM(Section12.5). w,b 2(cid:107) (cid:107) subjecttoy ( w,x +b) (cid:62) 1 forall n = 1,...,N . (12.19) n n (cid:104) (cid:105) Equation (12.18) is known as the hard margin SVM. The reason for the hardmarginSVM expression “hard” is because the formulation does not allow for any violations of the margin condition. We will see in Section 12.2.4 that this (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 378 ClassificationwithSupportVectorMachines “hard” condition can be relaxed to accommodate violations if the data is notlinearlyseparable. 12.2.3 Why We Can Set the Margin to 1 In Section 12.2.1, we argued that we would like to maximize some value r,whichrepresentsthedistanceoftheclosestexampletothehyperplane. In Section 12.2.2, we scaled the data such that the closest example is of distance1tothehyperplane.Inthissection,werelatethetwoderivations, andshowthattheyareequivalent. Theorem 12.1. Maximizing the margin r, where we consider normalized weightsasin(12.10), max r w,b,r (cid:124)(cid:123)(cid:122)(cid:125) margin (12.20) subjectto y ( w,x +b) (cid:62) r, w = 1, r > 0, n n (cid:104) (cid:105) (cid:107) (cid:107) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) datafitting normalization isequivalenttoscalingthedata,suchthatthemarginisunity: 1 2 min w w,b 2 (cid:107) (cid:107) (cid:124) (cid:123)(cid:122) (cid:125) margin (12.21) subjectto y ( w,x +b) (cid:62) 1 . n n (cid:104) (cid:105) (cid:124) (cid:123)(cid:122) (cid:125) datafitting Proof Consider (12.20). Since the square is a strictly monotonic transformationfornon-negativearguments,themaximumstaysthesameifwe consider r2 in the objective. Since w = 1 we can reparametrize the (cid:107) (cid:107) equationwithanewweightvectorw(cid:48) thatisnotnormalizedbyexplicitly using w(cid:48) .Weobtain (cid:107)w(cid:48)(cid:107) max r2 w(cid:48),b,r (cid:18)(cid:28) w(cid:48) (cid:29) (cid:19) (12.22) subjectto y ,x +b (cid:62) r, r > 0. n w(cid:48) n (cid:107) (cid:107) Equation(12.22)explicitlystatesthatthedistancerispositive.Therefore, Notethatr>0 wecandividethefirstconstraintbyr,whichyields becausewe assumedlinear max r2 w(cid:48),b,r separability,and   hencethereisno (cid:42) (cid:43) issuetodividebyr.  w(cid:48) b  (12.23) subjectto y  ,x +  (cid:62) 1, r > 0 n w(cid:48) r n r    (cid:124) (cid:107) (cid:123)(cid:122) (cid:107) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) w(cid:48)(cid:48) b(cid:48)(cid:48) Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 12.2 PrimalSupportVectorMachine 379 Figure12.6 (a)Linearly separableand (b)non-linearly separabledata. x(1) )2(x x(1) (a)Linearlyseparabledata,withalarge margin )2(x (b)Non-linearlyseparabledata renamingtheparameterstow(cid:48)(cid:48) andb(cid:48)(cid:48).Sincew(cid:48)(cid:48) = w(cid:48) ,rearrangingfor (cid:107)w(cid:48)(cid:107)r r gives (cid:13) (cid:13) w(cid:48) (cid:13) (cid:13) 1 (cid:13) (cid:13) w(cid:48) (cid:13) (cid:13) 1 w(cid:48)(cid:48) = (cid:13) (cid:13) = (cid:13) (cid:13) = . (12.24) (cid:107) (cid:107) (cid:13) w(cid:48) r(cid:13) r ·(cid:13) w(cid:48) (cid:13) r (cid:107) (cid:107) (cid:107) (cid:107) Bysubstitutingthisresultinto(12.23),weobtain 1 max w(cid:48)(cid:48),b(cid:48)(cid:48) w(cid:48)(cid:48) 2 (12.25) (cid:107) (cid:107) subjectto y ( w(cid:48)(cid:48),x +b(cid:48)(cid:48)) (cid:62) 1. n n (cid:104) (cid:105) Thefinalstepistoobservethatmaximizing 1 yieldsthesamesolution (cid:107)w(cid:48)(cid:48)(cid:107)2 asminimizing 1 w(cid:48)(cid:48) 2 ,whichconcludestheproofofTheorem12.1. 2 (cid:107) (cid:107) 12.2.4 Soft Margin SVM: Geometric View In the case where data is not linearly separable, we may wish to allow some examples to fall within the margin region, or even to be on the wrongsideofthehyperplaneasillustratedinFigure12.6. The model that allows for some classification errors is called the soft softmarginSVM marginSVM.Inthissection,wederivetheresultingoptimizationproblem using geometric arguments. In Section 12.2.5, we will derive an equivalent optimization problem using the idea of a loss function. Using Lagrange multipliers (Section 7.2), we will derive the dual optimization problem of the SVM in Section 12.3. This dual optimization problem allowsustoobserveathirdinterpretationoftheSVM:asahyperplanethat bisects the line between convex hulls corresponding to the positive and negativedataexamples(Section12.3.2). Thekeygeometricideaistointroduceaslackvariableξ n corresponding slackvariable toeachexample–labelpair(x ,y )thatallowsaparticularexampletobe n n within