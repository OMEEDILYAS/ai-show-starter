some sortof uncertainty, e.g., to quantify theconfidence we have about the value of the prediction for a particular test data point. As we have seen in Chapter 6, probability theory provides a language for quantifying uncertainty. Figure 8.3 illustrates the predictive uncertainty of the functionasaGaussiandistribution. Instead of considering a predictor as a single function, we could considerpredictorstobeprobabilisticmodels,i.e.,modelsdescribingthedistribution of possible functions. We limit ourselves in this book to the specialcaseofdistributionswithfinite-dimensionalparameters,whichallows us to describe probabilistic models without needing stochastic processes and random measures. For this special case, we can think about probabilistic models as multivariate probability distributions, which already allowforarichclassofmodels. We will introduce how to use concepts from probability (Chapter 6) to definemachinelearningmodelsinSection8.4,andintroduceagraphical language for describing probabilistic models in a compact way in Section8.5. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.1 Data,Models,andLearning 257 8.1.4 Learning is Finding Parameters The goal of learning is to find a model and its corresponding parameters such that the resulting predictor will perform well on unseen data. There are conceptually three distinct algorithmic phases when discussing machinelearningalgorithms: 1. Predictionorinference 2. Trainingorparameterestimation 3. Hyperparametertuningormodelselection Thepredictionphaseiswhenweuseatrainedpredictoronpreviouslyunseentestdata.Inotherwords,theparametersandmodelchoiceisalready fixed and the predictor is applied to new vectors representing new input datapoints.AsoutlinedinChapter1andtheprevioussubsection,wewill consider two schools of machine learning in this book, corresponding to whether the predictor is a function or a probabilistic model. When we have a probabilistic model (discussed further in Section 8.4) the predictionphaseiscalledinference. Remark. Unfortunately, there is no agreed upon naming for the different algorithmicphases.Theword“inference”issometimesalsousedtomean parameterestimationofaprobabilisticmodel,andlessoftenmaybealso usedtomeanpredictionfornon-probabilisticmodels. ♦ The training or parameter estimation phase is when weadjust our predictive model based on training data. We would like to find good predictors given training data, and there are two main strategies for doing so: finding the best predictor based on some measure of quality (sometimes called finding a point estimate), or using Bayesian inference. Finding a point estimate can be applied to both types of predictors, but Bayesian inferencerequiresprobabilisticmodels. Forthenon-probabilisticmodel,wefollowtheprincipleofempiricalrisk empiricalrisk minimization, which we describe in Section 8.2. Empirical risk minimiza- minimization tion directly provides an optimization problem for finding good parameters. With a statistical model, the principle of maximum likelihood is used maximumlikelihood tofindagoodsetofparameters(Section8.3).Wecanadditionallymodel the uncertainty of parameters using a probabilistic model, which we will lookatinmoredetailinSection8.4. We use numerical methods to find good parameters that “fit” the data, and most training methods can be thought of as hill-climbing approaches tofindthemaximumofanobjective,forexamplethemaximumofalikelihood.Toapplyhill-climbingapproachesweusethegradientsdescribedin Theconventionin Chapter5andimplementnumericaloptimizationapproachesfromChap- optimizationisto minimizeobjectives. ter7. Hence,thereisoften AsmentionedinChapter1,weareinterestedinlearningamodelbased anextraminussign on data such that it performs well on future data. It is not enough for inmachinelearning objectives. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 258 WhenModelsMeetData the model to only fit the training data well, the predictor needs to perform well on unseen data. We simulate the behavior of our predictor on cross-validation future unseen data using cross-validation (Section 8.2.4). As we will see in this chapter, to achieve the goal of performing well on unseen data, we will need to balance between fitting well on training data and finding “simple” explanations of the phenomenon. This trade-off is achieved using regularization (Section 8.2.3) or by adding a prior (Section 8.3.2). In philosophy, this is considered to be neither induction nor deduction, but