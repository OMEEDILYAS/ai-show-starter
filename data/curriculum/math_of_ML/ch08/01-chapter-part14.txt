AsdiscussedinSection6.3,focusingsolelyonsomestatisticoftheposterior distribution (such as the parameter θ∗ that maximizes the posterior) leads to loss of information, which can be critical in a system that (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 274 WhenModelsMeetData uses the prediction p(x θ∗) to make decisions. These decision-making | Bayesianinference systems typically have different objective functions than the likelihood, a isaboutlearningthe squared-error loss or a mis-classification error. Therefore, having the full distributionof posterior distribution around can be extremely useful and leads to more randomvariables. robust decisions. Bayesian inference is about finding this posterior distriBayesianinference bution(Gelmanetal.,2004).Foradataset ,aparameterpriorp(θ),and X alikelihoodfunction,theposterior p( θ)p(θ) (cid:90) p(θ ) = X | , p( ) = p( θ)p(θ)dθ, (8.22) |X p( ) X X | X Bayesianinference is obtained by applying Bayes’ theorem. The key idea is to exploit Bayes’ invertsthe theoremtoinverttherelationshipbetweentheparametersθ andthedata relationship (givenbythelikelihood)toobtaintheposteriordistributionp(θ ). betweenparameters X |X The implication of having a posterior distribution on the parameters is andthedata. that it can be used to propagate uncertainty from the parameters to the data. More specifically, with a distribution p(θ) on the parameters our predictionswillbe (cid:90) p(x) = p(x θ)p(θ)dθ = E [p(x θ)], (8.23) θ | | and they no longer depend on the model parameters θ, which have been marginalized/integrated out. Equation (8.23) reveals that the prediction isanaverageoverallplausibleparametervaluesθ,wheretheplausibility isencapsulatedbytheparameterdistributionp(θ). HavingdiscussedparameterestimationinSection8.3andBayesianinferencehere,letuscomparethesetwoapproachestolearning.Parameter estimationviamaximumlikelihoodorMAPestimationyieldsaconsistent point estimate θ∗ of the parameters, and the key computational problem tobesolvedisoptimization.Incontrast,Bayesianinferenceyieldsa(posterior) distribution, and the key computational problem to be solved is integration.Predictionswithpointestimatesarestraightforward,whereas predictionsintheBayesianframeworkrequiresolvinganotherintegration problem; see (8.23). However, Bayesian inference gives us a principled way to incorporate prior knowledge, account for side information, and incorporate structural knowledge, all of which is not easily done in the contextofparameterestimation.Moreover,thepropagationofparameter uncertainty to the prediction can be valuable in decision-making systems for risk assessment and exploration in the context of data-efficient learning(Deisenrothetal.,2015;KamtheandDeisenroth,2018). WhileBayesianinferenceisamathematicallyprincipledframeworkfor learning about parameters and making predictions, there are some practical challenges that come with it because of the integration problems we needtosolve;see(8.22)and(8.23).Morespecifically,ifwedonotchoose aconjugatepriorontheparameters(Section6.6.1),theintegralsin(8.22) and(8.23)arenotanalyticallytractable,andwecannotcomputetheposDraft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.4 ProbabilisticModelingandInference 275 terior, the predictions, or the marginal likelihood in closed form. In these cases, we need to resort to approximations. Here, we can use stochastic approximations, such as Markov chain Monte Carlo (MCMC) (Gilks et al., 1996), or deterministic approximations, such as the Laplace approximation (Bishop, 2006; Barber, 2012; Murphy, 2012), variational inference (Jordan et al., 1999; Blei et al., 2017), or expectation propagation(Minka,2001a). Despite these challenges, Bayesian inference has been successfully appliedtoavarietyofproblems,includinglarge-scaletopicmodeling(Hoffman et al., 2013), click-through-rate prediction (Graepel et al., 2010), data-efficientreinforcementlearningincontrolsystems(Deisenrothetal., 2015),onlinerankingsystems(Herbrichetal.,2007),andlarge-scalerecommender systems. There are generic tools, such as Bayesian optimization (Brochu et al., 2009; Snoek et al., 2012; Shahriari et al., 2016), that are very useful ingredients for an efficient search of meta parameters of modelsoralgorithms. Remark. In the machine learning literature, there can be a somewhat arbitraryseparationbetween(random)“variables”and“parameters”.While parameters are estimated (e.g., via maximum likelihood), variables are usually marginalized out. In this book, we are not so strict with this separation because, in principle, we can place a prior on any parameter and integrateitout,whichwouldthenturntheparameterintoarandomvariableaccordingtotheaforementionedseparation. ♦ 8.4.3 Latent-Variable Models In practice, it is sometimes useful to have additional latent variables z latentvariable (besides the model parameters