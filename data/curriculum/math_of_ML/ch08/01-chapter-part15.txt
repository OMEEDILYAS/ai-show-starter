θ) as part of the model (Moustaki et al., 2015). These latent variables are different from the model parameters θ as they do not parametrize the model explicitly. Latent variables may describe the data-generating process, thereby contributing to the interpretability of the model. They also often simplify the structure of the model and allow us to define simpler and richer model structures. Simplification of the model structure often goes hand in hand with a smaller number of model parameters (Paquet, 2008; Murphy, 2012). Learning in latent-variablemodels(atleastviamaximumlikelihood)canbedoneina principledwayusingtheexpectationmaximization(EM)algorithm(Dempster et al., 1977; Bishop, 2006). Examples, where such latent variables are helpful, are principal component analysis for dimensionality reduction(Chapter10),Gaussianmixturemodelsfordensityestimation(Chapter 11), hidden Markov models (Maybeck, 1979) or dynamical systems (Ghahramani and Roweis, 1999; Ljung, 1999) for time-series modeling, and meta learning and task generalization (Hausman et al., 2018; Sæmundssonetal.,2018).Althoughtheintroductionoftheselatentvariables (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 276 WhenModelsMeetData maymakethemodelstructureandthegenerativeprocesseasier,learning inlatent-variablemodelsisgenerallyhard,aswewillseeinChapter11. Since latent-variable models also allow us to define the process that generatesdatafromparameters,letushavealookatthisgenerativeprocess. Denoting data by x, the model parameters by θ and the latent variablesbyz,weobtaintheconditionaldistribution p(x θ,z) (8.24) | thatallowsustogeneratedataforanymodelparametersandlatentvariables.Giventhatz arelatentvariables,weplaceapriorp(z)onthem. As the models we discussed previously, models with latent variables can be used for parameter learning and inference within the frameworks we discussed in Sections 8.3 and 8.4.2. To facilitate learning (e.g., by means of maximum likelihood estimation or Bayesian inference), we followatwo-stepprocedure.First,wecomputethelikelihoodp(x θ)ofthe | model,whichdoesnotdependonthelatentvariables.Second,weusethis likelihood for parameter estimation or Bayesian inference, where we use exactlythesameexpressionsasinSections8.3and8.4.2,respectively. Sincethelikelihoodfunctionp(x θ)isthepredictivedistributionofthe | data given the model parameters, we need to marginalize out the latent variablessothat (cid:90) p(x θ) = p(x θ,z)p(z)dz, (8.25) | | where p(x z,θ) is given in (8.24) and p(z) is the prior on the latent | Thelikelihoodisa variables.Notethatthelikelihoodmustnotdependonthelatentvariables functionofthedata z,butitisonlyafunctionofthedataxandthemodelparametersθ. andthemodel The likelihood in (8.25) directly allows for parameter estimation via parameters,butis maximum likelihood. MAP estimation is also straightforward with an adindependentofthe latentvariables. ditional prior on the model parameters θ as discussed in Section 8.3.2. Moreover, with the likelihood (8.25) Bayesian inference (Section 8.4.2) in a latent-variable model works in the usual way: We place a prior p(θ) on the model parameters and use Bayes’ theorem to obtain a posterior distribution p( θ)p(θ) p(θ ) = X | (8.26) |X p( ) X overthemodelparametersgivenadataset .Theposteriorin(8.26)can X beusedforpredictionswithinaBayesianinferenceframework;see(8.23). One challenge we have in this latent-variable model is that the likelihood p( θ) requires the marginalization of the latent variables acX | cording to (8.25). Except when we choose a conjugate prior p(z) for p(x z,θ), the marginalization in (8.25) is not analytically tractable, and | we need to resort to approximations (Bishop, 2006; Paquet, 2008; Murphy,2012;Moustakietal.,2015). Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.4 ProbabilisticModelingandInference 277 Similar to the parameter posterior (8.26) we can compute a posterior onthelatentvariablesaccordingto p( z)p(z) (cid:90) p(z ) = X | , p( z) = p( z,θ)p(θ)dθ, (8.27) |X p( ) X | X | X where p(z) is the prior on the latent variables and p( z) requires us to X | integrateoutthemodelparametersθ. Giventhedifficultyofsolvingintegralsanalytically,itisclearthatmarginalizing out both the latent variables and the model parameters at the same time is not possible in general (Bishop, 2006; Murphy, 2012). A quantitythatiseasiertocomputeistheposteriordistributiononthelatent