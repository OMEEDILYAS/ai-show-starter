variables,butconditionedonthemodelparameters,i.e., p( z,θ)p(z) p(z ,θ) = X | , (8.28) |X p( θ) X | where p(z) is the prior on the latent variables and p( z,θ) is given X | in(8.24). In Chapters 10 and 11, we derive the likelihood functions for PCA and Gaussian mixture models, respectively. Moreover, we compute the posteriordistributions(8.28)onthelatentvariablesforbothPCAandGaussian mixturemodels. Remark. In the following chapters, we may not be drawing such a clear distinction between latent variables z and uncertain model parameters θ and call the model parameters “latent” or “hidden” as well because they areunobserved.InChapters10and11,whereweusethelatentvariables z, we will pay attention to the difference as we will have two different typesofhiddenvariables:modelparametersθ andlatentvariablesz. ♦ Wecanexploitthefactthatalltheelementsofaprobabilisticmodelare random variables to define a unified language for representing them. In Section8.5,wewillseeaconcisegraphicallanguageforrepresentingthe structure of probabilistic models. We will use this graphical language to describetheprobabilisticmodelsinthesubsequentchapters. 8.4.4 Further Reading Probabilistic models in machine learning (Bishop, 2006; Barber, 2012; Murphy,2012)provideawayforuserstocaptureuncertaintyaboutdata andpredictivemodelsinaprincipledfashion.Ghahramani(2015)presents ashortreviewofprobabilisticmodelsinmachinelearning.Givenaprobabilisticmodel,wemaybeluckyenoughtobeabletocomputeparameters of interest analytically. However, in general, analytic solutions are rare, and computational methods such as sampling (Gilks et al., 1996; Brooks et al., 2011) and variational inference (Jordan et al., 1999; Blei et al., (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 278 WhenModelsMeetData 2017)areused.Moustakietal.(2015)andPaquet(2008)provideagood overviewofBayesianinferenceinlatent-variablemodels. In recent years, several programming languages have been proposed that aim to treat the variables defined in software as random variables corresponding to probability distributions. The objective is to be able to writecomplexfunctionsofprobabilitydistributions,whileunderthehood the compiler automatically takes care of the rules of Bayesian inference. probabilistic Thisrapidlychangingfieldiscalledprobabilisticprogramming. programming 8.5 Directed Graphical Models In this section, we introduce a graphical language for specifying a probdirectedgraphical abilistic model, called the directed graphical model. It provides a compact model andsuccinctwaytospecifyprobabilisticmodels,andallowsthereaderto visuallyparsedependenciesbetweenrandomvariables.Agraphicalmodel visually captures the way in which the joint distribution over all random variablescanbedecomposedintoaproductoffactorsdependingonlyon a subset of these variables. In Section 8.4, we identified the joint distribution of a probabilistic model as the key quantity of interest because it comprises information about the prior, the likelihood, and the posterior. Directedgraphical However, the joint distribution by itself can be quite complicated, and modelsarealso it does not tell us anything about structural properties of the probabilisknownasBayesian tic model. For example, the joint distribution p(a,b,c) does not tell us networks. anything about independence relations. This is the point where graphical modelscomeintoplay.Thissectionreliesontheconceptsofindependence andconditionalindependence,asdescribedinSection6.4.5. graphicalmodel In a graphical model, nodes are random variables. In Figure 8.9(a), the nodesrepresenttherandomvariablesa,b,c.Edgesrepresentprobabilistic relationsbetweenvariables,e.g.,conditionalprobabilities. Remark. Noteverydistributioncanberepresentedinaparticularchoiceof graphicalmodel.AdiscussionofthiscanbefoundinBishop(2006). ♦ Probabilisticgraphicalmodelshavesomeconvenientproperties: Theyareasimplewaytovisualizethestructureofaprobabilisticmodel. Theycanbeusedtodesignormotivatenewkindsofstatisticalmodels. Inspectionofthegraphalonegivesusinsightintoproperties,e.g.,conditionalindependence. Complex computations for inference and learning in statistical models canbeexpressedintermsofgraphicalmanipulations. 8.5.1 Graph Semantics directedgraphical Directedgraphicalmodels/Bayesiannetworksareamethodforrepresenting model/Bayesian conditional dependencies in a probabilistic model. They provide a visual network Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.5 DirectedGraphicalModels 279 descriptionoftheconditionalprobabilities,hence,providingasimplelanguage for describing complex interdependence. The modular description Withadditional alsoentailscomputationalsimplification.Directedlinks(arrows)between assumptions,the arrowscanbeused two nodes (random variables) indicate conditional probabilities. For extoindicatecausal ample, the arrow between a and b in Figure 8.9(a) gives the conditional relationships(Pearl, probabilityp(b a)ofbgivena. 2009). | Figure8.9 a b x 1 x 2 x 5 Examplesof directedgraphical models. c x x 3 4 (a)Fullyconnected. (b)Notfullyconnected. Directed graphical models can be derived from joint distributions if we knowsomethingabouttheirfactorization. Example 8.7 Considerthejointdistribution p(a,b,c) = p(c a,b)p(b a)p(a) (8.29) | | ofthreerandomvariablesa,b,c.Thefactorizationofthejointdistribution in (8.29)