Petersetal.,2017;Rosenbaum,2017). 8.6 Model Selection Inmachinelearning,weoftenneedtomakehigh-levelmodelingdecisions that critically influence the performance of the model. The choices we make (e.g., the functional form of the likelihood) influence the number and type of free parameters in the model and thereby also the flexibility and expressivity of the model. More complex models are more flexible in Apolynomial the sense that they can be used to describe more datasets. For instance, a y=a0+a1x+a2x2 polynomialofdegree1(aliney = a +a x)canonlybeusedtodescribe canalsodescribe 0 1 linearfunctionsby linear relations between inputs x and observations y. A polynomial of settinga2=0,i.e., degree2canadditionallydescribequadraticrelationshipsbetweeninputs itisstrictlymore andobservations. expressivethana Onewouldnowthinkthatveryflexiblemodelsaregenerallypreferable first-order polynomial. to simple models because they are more expressive. A general problem (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 284 WhenModelsMeetData Figure8.13 Nested Alllabeleddata cross-validation.We performtwolevels ofK-fold Alltrainingdata Testdata cross-validation. Totrainmodel Validation is that at training time we can only use the training set to evaluate the performance of the model and learn its parameters. However, the performance on the training set is not really what we are interested in. In Section 8.3, we have seen that maximum likelihood estimation can lead to overfitting, especially when the training dataset is small. Ideally, our model(also)workswellonthetestset(whichisnotavailableattraining time). Therefore, we need some mechanisms for assessing how a model generalizes to unseen test data. Model selection is concerned with exactly thisproblem. 8.6.1 Nested Cross-Validation Wehavealreadyseenanapproach(cross-validationinSection8.2.4)that can be used for model selection. Recall that cross-validation provides an estimateofthegeneralizationerrorbyrepeatedlysplittingthedatasetinto training and validation sets. We can apply this idea one more time, i.e., for each split, we can perform another round of cross-validation. This is nested sometimesreferredtoasnestedcross-validation;seeFigure8.13.Theinner cross-validation level is used to estimate the performance of a particular choice of model or hyperparameter on a internal validation set. The outer level is used to estimate generalization performance for the best choice of model chosen bytheinnerloop.Wecantestdifferentmodelandhyperparameterchoices in the inner loop. To distinguish the two levels, the set used to estimate testset thegeneralizationperformanceisoftencalledthetestsetandthesetused validationset for choosing the best model is called the validation set. The inner loop estimatestheexpectedvalueofthegeneralizationerrorforagivenmodel (8.39),byapproximatingitusingtheempiricalerroronthevalidationset, Thestandarderror i.e., isdefinedas √σ , whereKisthe K E [R( M)] 1 (cid:88) K R( (k) M), (8.39) numberof V V| ≈ K V | experimentsandσ k=1 isthestandard whereR( M)istheempiricalrisk(e.g.,rootmeansquareerror)onthe deviationoftherisk V| validationset formodelM.Werepeatthisprocedureforallmodelsand ofeachexperiment. V choose the model that performs best. Note that cross-validation not only gives us the expected generalization error, but we can also obtain highorder statistics, e.g., the standard error, an estimate of how uncertain the Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.6 ModelSelection 285 Figure8.14 Evidence Bayesianinference embodiesOccam’s razor.The horizontalaxis describesthespace p( M ) D| 1 ofallpossible datasetsD.The evidence(vertical axis)evaluateshow wellamodel p( D| M 2 ) predictsavailable data.Since p(D|Mi)needsto integrateto1,we shouldchoosethe D modelwiththe C greatestevidence. Adapted mean estimate is. Once the model is chosen, we can evaluate the final fromMacKay performanceonthetestset. (2003). 8.6.2 Bayesian Model Selection Therearemanyapproachestomodelselection,someofwhicharecovered in this section. Generally, they all attempt to trade off model complexity and data fit. We assume that simpler models are less prone to overfitting thancomplexmodels,andhencetheobjectiveofmodelselectionistofind thesimplestmodelthatexplainsthedatareasonablywell.Thisconceptis alsoknownasOccam’srazor. Occam’srazor Remark. If we treat model selection as a hypothesis testing problem, we arelookingforthesimplesthypothesisthatisconsistentwiththedata(Murphy,2012). ♦ Onemayconsiderplacingaprioronmodelsthatfavorssimplermodels. However, it is not necessary to do this: An “automatic Occam’s