In many machine learning algorithms, weneedtoadditionallybeabletocomparetwovectors.Aswewillseein Chapters 9 and 12, computing the similarity or distance between two examplesallowsustoformalizetheintuitionthatexampleswithsimilarfeatures should have similar labels. The comparison of two vectors requires that we construct a geometry (explained in Chapter 3) and allows us to optimizetheresultinglearningproblemusingtechniquesfromChapter7. Sincewehavevectorrepresentationsofdata,wecanmanipulatedatato find potentially better representations of it. We will discuss finding good representations in two ways: finding lower-dimensional approximations of the original feature vector, and using nonlinear higher-dimensional combinations of the original feature vector. In Chapter 10, we will see an example of finding a low-dimensional approximation of the original data spacebyfindingtheprincipalcomponents.Findingprincipalcomponents iscloselyrelatedtoconceptsofeigenvalueandsingularvaluedecompositionasintroducedinChapter4.Forthehigh-dimensionalrepresentation, featuremap we will see an explicit feature map φ( ) that allows us to represent in- · puts x using a higher-dimensional representation φ(x ). The main mon n tivation for higher-dimensional representations is that we can construct newfeaturesasnon-linearcombinationsoftheoriginalfeatures,whichin turn may make the learning problem easier. We will discuss the feature kernel map in Section 9.2 and show how this feature map leads to a kernel in Section 12.4. In recent years, deep learning methods (Goodfellow et al., 2016)haveshownpromiseinusingthedataitselftolearnnewgoodfeatures and have been very successful in areas, such as computer vision, speech recognition, and natural language processing. We will not cover neural networks in this part of the book, but the reader is referred to Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.1 Data,Models,andLearning 255 Figure8.2 Example 150 function(blacksolid diagonalline)and 125 itspredictionat x=60,i.e., f(60)=100. 100 75 50 25 0 0 10 20 30 40 50 60 70 80 x y Section 5.6 for the mathematical description of backpropagation, a key conceptfortrainingneuralnetworks. 8.1.2 Models as Functions Oncewehavedatainanappropriatevectorrepresentation,wecangetto the business of constructing a predictive function (known as a predictor). predictor InChapter1,wedidnotyethavethelanguagetobepreciseaboutmodels. Using the concepts from the first part of the book, we can now introduce what “model” means. We present two major approaches in this book: a predictor as a function, and a predictor as a probabilistic model. We describetheformerhereandthelatterinthenextsubsection. A predictor is a function that, when given a particular input example (in our case, a vector of features), produces an output. For now, consider theoutputtobeasinglenumber,i.e.,areal-valuedscalaroutput.Thiscan bewrittenas f : RD R, (8.1) → wheretheinputvectorxisD-dimensional(hasDfeatures),andthefunction f then applied to it (written as f(x)) returns a real number. Figure 8.2 illustrates a possible function that can be used to compute the valueofthepredictionforinputvaluesx. Inthisbook,wedonotconsiderthegeneralcaseofallfunctions,which would involve the need for functional analysis. Instead, we consider the specialcaseoflinearfunctions f(x) = θ(cid:62)x+θ (8.2) 0 for unknown θ and θ . This restriction means that the contents of Chap0 ters 2 and 3 suffice for precisely stating the notion of a predictor for thenon-probabilistic(incontrasttotheprobabilisticviewdescribednext) (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 256 WhenModelsMeetData Figure8.3 Example function(blacksolid 150 diagonalline)and itspredictive 125 uncertaintyat x=60(drawnasa 100 Gaussian). 75 50 25 0 0 10 20 30 40 50 60 70 80 x y viewofmachinelearning.Linearfunctionsstrikeagoodbalancebetween thegeneralityoftheproblemsthatcanbesolvedandtheamountofbackgroundmathematicsthatisneeded. 8.1.3 Models as Probability Distributions We often consider data to be noisy observations of some true underlying effect, and hope that by applying machine learning we can identify the signal from the noise. This requires us to have a language for quantifying the effect of noise. We often would also like to have predictors that express