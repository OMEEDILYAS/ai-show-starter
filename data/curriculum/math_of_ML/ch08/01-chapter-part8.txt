− (cid:107) (cid:107) (cid:107) 2 The additional term θ is called the regularizer, and the parameter regularizer (cid:107) (cid:107) λ is the regularization parameter. The regularization parameter trades regularization off minimizing the loss on the training set and the magnitude of the pa- parameter rameters θ. It often happens that the magnitude of the parameter values becomesrelativelylargeifwerunintooverfitting(Bishop,2006). Theregularizationtermissometimescalledthepenaltyterm,whichbi- penaltyterm asesthevectorθ tobeclosertotheorigin.Theideaofregularizationalso appearsinprobabilisticmodelsasthepriorprobabilityoftheparameters. RecallfromSection6.6thatfortheposteriordistributiontobeofthesame formasthepriordistribution,thepriorandthelikelihoodneedtobeconjugate.WewillrevisitthisideainSection8.3.2.WewillseeinChapter12 thattheideaoftheregularizerisequivalenttotheideaofalargemargin. 8.2.4 Cross-Validation to Assess the Generalization Performance Wementionedintheprevioussectionthatwemeasurethegeneralization error by estimating it by applying the predictor on test data. This data is alsosometimesreferredtoasthevalidationset.Thevalidationsetisasub- validationset setoftheavailabletrainingdatathatwekeepaside.Apracticalissuewith this approach is that the amount of data is limited, and ideally we would use as much of the data available to train the model. This would require us to keep our validation set small, which then would lead to a noisy V estimate (with high variance) of the predictive performance. One solution to these contradictory objectives (large training set, large validation set)istousecross-validation.K-foldcross-validationeffectivelypartitions cross-validation the data into K chunks, K 1 of which form the training set , and − R the last chunk serves as the validation set (similar to the idea outlined V previously). Cross-validation iterates through (ideally) all combinations of assignments of chunks to and ; see Figure 8.4. This procedure is R V repeated for all K choices for the validation set, and the performance of themodelfromtheK runsisaveraged. Wepartitionourdatasetintotwosets = ,suchthattheydonot D R∪V overlap ( = ), where is the validation set, and train our model R∩V ∅ V on . After training, we assess the performance of the predictor f on the R (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 264 WhenModelsMeetData Figure8.4 K-fold cross-validation. Training Thedatasetis dividedintoK=5 chunks,K−1of whichserveasthe trainingset(blue) andoneasthe Validation validationset (orangehatch). validation set (e.g., by computing root mean square error (RMSE) of V thetrainedmodelonthevalidationset).Moreprecisely,foreachpartition k the training data (k) produces a predictor f(k), which is then applied R tovalidationset (k) tocomputetheempiricalriskR(f(k), (k)).Wecycle V V throughallpossiblepartitioningsofvalidationandtrainingsetsandcompute the average generalization error of the predictor. Cross-validation approximatestheexpectedgeneralizationerror K 1 (cid:88) E [R(f, )] R(f(k), (k)), (8.13) V V ≈ K V k=1 where R(f(k), (k)) is the risk (e.g., RMSE) on the validation set (k) for V V predictor f(k). The approximation has two sources: first, due to the finite trainingset,whichresultsinnotthebestpossiblef(k);andsecond,dueto the finite validation set, which results in an inaccurate estimation of the risk R(f(k), (k)). A potential disadvantage of K-fold cross-validation is V the computational cost of training the model K times, which can be burdensome if the training cost is computationally expensive. In practice, it isoftennotsufficienttolookatthedirectparametersalone.Forexample, we need to explore multiple complexity parameters (e.g., multiple regularizationparameters),whichmaynotbedirectparametersofthemodel. Evaluatingthequalityofthemodel,dependingonthesehyperparameters, mayresultinanumberoftrainingrunsthatisexponentialinthenumber ofmodelparameters.Onecanusenestedcross-validation(Section8.6.1) tosearchforgoodhyperparameters. embarrassingly However,cross-validationisanembarrassinglyparallelproblem,i.e.,litparallel tle effort is needed to separate the problem into a number of parallel tasks.Givensufficientcomputingresources(e.g.,cloudcomputing,server farms),cross-validationdoesnotrequirelongerthanasingleperformance assessment. Inthissection,wesawthatempiricalriskminimizationisbasedonthe followingconcepts:thehypothesisclassoffunctions,thelossfunctionand regularization.InSection8.3,wewillseetheeffectofusingaprobability distributiontoreplacetheideaoflossfunctionsandregularization. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.3 ParameterEstimation 265 8.2.5 Further Reading Due to the fact that the original development of empirical risk minimization (Vapnik, 1998) was couched in heavily theoretical language, many of the subsequent developments have been theoretical. The area of study is called statistical learning theory (Vapnik, 1999; Evgeniou et al., 2000; statisticallearning Hastieetal.,2001;vonLuxburgandScho¨lkopf,2011).Arecentmachine theory learningtextbookthatbuildsonthetheoreticalfoundationsanddevelops efficientlearningalgorithmsisShalev-ShwartzandBen-David(2014). Theconceptofregularizationhasitsrootsinthesolutionofill-posedinverse