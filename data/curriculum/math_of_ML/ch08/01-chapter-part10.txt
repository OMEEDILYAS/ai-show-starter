n N n | n An illustration of a Gaussian likelihood for a given parameter θ is shown in Figure 8.3. We will see in Section 9.2 how to explicitly expand the precedingexpressionoutintermsoftheGaussiandistribution. independentand Weassumethatthesetofexamples(x 1 ,y 1 ),...,(x N ,y N )areindependent identically andidenticallydistributed(i.i.d.).Theword“independent”(Section6.4.5) distributed implies that the likelihood of the whole dataset ( = y ,...,y and 1 N Y { } = x ,...,x factorizes into a product of the likelihoods of each 1 N X { } Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.3 ParameterEstimation 267 individualexample N (cid:89) p( ,θ) = p(y x ,θ), (8.16) n n Y|X | n=1 wherep(y x ,θ)isaparticulardistribution(whichwasGaussianinExn n | ample8.4).Theexpression“identicallydistributed”meansthateachterm in the product (8.16) is of the same distribution, and all of them share the same parameters. It is often easier from an optimization viewpoint to computefunctionsthatcanbedecomposedintosumsofsimplerfunctions. Hence,inmachinelearningweoftenconsiderthenegativelog-likelihood Recalllog(ab)= log(a)+log(b) N (cid:88) (θ) = logp( ,θ) = logp(y x ,θ). (8.17) n n L − Y|X − | n=1 Whileitistempingtointerpretthefactthatθ isontherightoftheconditioninginp(y x ,θ)(8.15),andhenceshouldbeinterpretedasobserved n n | andfixed,thisinterpretationisincorrect.Thenegativelog-likelihood (θ) L is a function of θ. Therefore, to find a good parameter vector θ that explains the data (x ,y ),...,(x ,y ) well, minimize the negative log1 1 N N likelihood (θ)withrespecttoθ. L Remark. The negative sign in (8.17) is a historical artifact that is due to the convention that we want to maximize likelihood, but numerical optimizationliteraturetendstostudyminimizationoffunctions. ♦ Example 8.5 Continuing on our example of Gaussian likelihoods (8.15), the negative log-likelihoodcanberewrittenas N N (θ) = (cid:88) logp(y x ,θ) = (cid:88) log (cid:0) y x(cid:62)θ, σ2(cid:1) (8.18a) L − n | n − N n | n n=1 n=1 = (cid:88) N log 1 exp (cid:18) (y n − x(cid:62) n θ)2(cid:19) (8.18b) − √2πσ2 − 2σ2 n=1 = (cid:88) N logexp (cid:18) (y n − x(cid:62) n θ)2(cid:19) (cid:88) N log 1 (8.18c) − − 2σ2 − √2πσ2 n=1 n=1 N N 1 (cid:88) (cid:88) 1 = (y x(cid:62)θ)2 log . (8.18d) 2σ2 n − n − √2πσ2 n=1 n=1 Asσisgiven,thesecondtermin(8.18d)isconstant,andminimizing (θ) L corresponds to solving the least-squares problem (compare with (8.8)) expressedinthefirstterm. It turns out that for Gaussian likelihoods the resulting optimization (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 268 WhenModelsMeetData Figure8.5 Forthe givendata,the 150 maximumlikelihood estimateofthe 125 parametersresults intheblack 100 diagonalline.The orangesquare 75 showsthevalueof themaximum 50 likelihood predictionat 25 x=60. 0 0 10 20 30 40 50 60 70 80 x y Figure8.6 Comparingthe 150 predictionswiththe maximumlikelihood 125 estimateandthe MAPestimateat 100 x=60.Theprior biasestheslopeto 75 belesssteepandthe intercepttobe 50 closertozero.In thisexample,the 25 biasthatmovesthe interceptcloserto 0 zeroactually 0 10 20 30 40 50 60 70 80 increasestheslope. x y MLE MAP problem corresponding to maximum likelihood estimation has a closedform solution. We will see more details on this in Chapter 9. Figure 8.5 shows a regression dataset and the function that is induced by the maximum-likelihood parameters. Maximum likelihood estimation may suffer fromoverfitting(Section8.3.3),analogoustounregularizedempiricalrisk minimization (Section 9.2.3). For other likelihood functions, i.e., if we modelournoisewithnon-Gaussiandistributions,maximumlikelihoodestimation may not have a closed-form analytic solution. In this case, we resorttonumericaloptimizationmethodsdiscussedinChapter7. 8.3.2 Maximum A Posteriori Estimation Ifwehavepriorknowledgeaboutthedistributionoftheparametersθ,we can multiply an additional term to the likelihood.