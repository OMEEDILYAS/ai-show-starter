= θ(cid:62)x . (8.4) n n Thislinearpredictorisequivalenttotheaffinemodel D (cid:88) f(x ,θ) = θ + θ x(d). (8.5) n 0 d n d=1 The predictor takes the vector of features representing a single example x as input and produces a real-valued output, i.e., f : RD+1 R. The n → (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 260 WhenModelsMeetData previous figures in this chapter had a straight line as a predictor, which meansthatwehaveassumedanaffinefunction. Instead of a linear function, we may wish to consider non-linear functionsaspredictors.Recentadvancesinneuralnetworksallowforefficient computationofmorecomplexnon-linearfunctionclasses. Given the class of functions, we want to search for a good predictor. Wenowmoveontothesecondingredientofempiricalriskminimization: howtomeasurehowwellthepredictorfitsthetrainingdata. 8.2.2 Loss Function for Training Considerthelabely foraparticularexample;andthecorrespondingpren diction yˆ that we make based on x . To define what it means to fit the n n lossfunction datawell,weneedtospecifyalossfunction(cid:96)(y n ,yˆ n )thattakestheground truthlabelandthepredictionasinputandproducesanon-negativenumber (referred to as the loss) representing how much error we have made Theexpression onthisparticularprediction.Ourgoalforfindingagoodparametervector “error”isoftenused θ∗ istominimizetheaveragelossonthesetofN trainingexamples. tomeanloss. One assumption that is commonly made in machine learning is that independentand the set of examples (x 1 ,y 1 ),...,(x N ,y N ) is independent and identically identically distributed. The word independent (Section 6.4.5) means that two data distributed points(x ,y )and(x ,y )donotstatisticallydependoneachother,meani i j j ing that the empirical mean is a good estimate of the population mean (Section 6.4.1). This implies that we can use the empirical mean of the trainingset lossonthetrainingdata.Foragiventrainingset (x 1 ,y 1 ),...,(x N ,y N ) , { } we introduce the notation of an example matrix X := [x ,...,x ](cid:62) 1 N RN×D and a label vector y := [y ,...,y ](cid:62) RN. Using this matri ∈ x 1 N ∈ notationtheaveragelossisgivenby N 1 (cid:88) R (f,X,y) = (cid:96)(y ,yˆ ), (8.6) emp N n n n=1 empiricalrisk where yˆ n = f(x n ,θ). Equation (8.6) is called the empirical risk and dependsonthreearguments,thepredictorf andthedataX,y.Thisgeneral empiricalrisk strategyforlearningiscalledempiricalriskminimization. minimization Example 8.2 (Least-Squares Loss) Continuing the example of least-squares regression, we specify that we measure the cost of making an error during training using the squared loss(cid:96)(y ,yˆ ) = (y yˆ )2.Wewishtominimizetheempiricalrisk(8.6), n n n n − Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.2 EmpiricalRiskMinimization 261 whichistheaverageofthelossesoverthedata N 1 (cid:88) min (y f(x ,θ))2, (8.7) θ∈RD N n − n n=1 wherewesubstitutedthe predictoryˆ = f(x ,θ).Byusingour choiceof n n alinearpredictorf(x ,θ) = θ(cid:62)x ,weobtaintheoptimizationproblem n n N 1 (cid:88) min (y θ(cid:62)x )2. (8.8) θ∈RD N n − n n=1 Thisequationcanbeequivalentlyexpressedinmatrixform 1 min y Xθ 2 . (8.9) θ∈RD N (cid:107) − (cid:107) Thisisknownastheleast-squaresproblem.Thereexistsaclosed-forman- least-squares alytic solution for this by solving the normal equations, which we will problem discussinSection9.2. We are not interested in a predictor that only performs well on the training data. Instead, we seek a predictor that performs well (has low risk) on unseen test data. More formally, we are interested in finding a predictorf (withparametersfixed)thatminimizestheexpectedrisk expectedrisk R (f) = E [(cid:96)(y,f(x))], (8.10) true x,y where y is the label and f(x) is the prediction based on the example x. ThenotationR (f)indicatesthatthisisthetrueriskifwehadaccessto true aninfiniteamountofdata.Theexpectationisoverthe(infinite)setofall Anotherphrase possibledataandlabels.Therearetwopracticalquestionsthatarisefrom commonlyusedfor expectedriskis our desire to minimize expected risk, which