Reading When considering probabilistic models, the principle of maximum likelihoodestimationgeneralizestheideaofleast-squaresregressionforlinear models, which we will discuss in detail in Chapter 9. When restricting the predictor to have linear form with an additional nonlinear function ϕ appliedtotheoutput,i.e., p(y x ,θ) = ϕ(θ(cid:62)x ), (8.21) n n n | we can consider other models for other prediction tasks, such as binary classification or modeling count data (McCullagh and Nelder, 1989). An alternative view of this is to consider likelihoods that are from the exponential family (Section 6.6). The class of models, which have linear dependence between parameters and data, and have potentially nonlinlinkfunction ear transformation ϕ (called a link function), is referred to as generalized generalizedlinear linearmodels(Agresti,2002,chapter4). model Maximum likelihood estimation has a rich history, and was originally proposedbySirRonaldFisherinthe1930s.Wewillexpandupontheidea of a probabilistic model in Section 8.4. One debate among researchers whouseprobabilisticmodels,isthediscussionbetweenBayesianandfrequentist statistics. As mentioned in Section 6.1.1, it boils down to the definition of probability. Recall from Section 6.1 that one can consider probabilitytobeageneralization(byallowinguncertainty)oflogicalreasoning (Cheeseman, 1985; Jaynes, 2003). The method of maximum likelihood estimation is frequentist in nature, and the interested reader is pointed to Efron and Hastie (2016) for a balanced view of both Bayesian andfrequentiststatistics. There are some probabilistic models where maximum likelihood estimationmaynotbepossible.Thereaderisreferredtomoreadvancedstatisticaltextbooks,e.g.,CasellaandBerger(2002),forapproaches,suchas methodofmoments,M-estimation,andestimatingequations. 8.4 Probabilistic Modeling and Inference Inmachinelearning,wearefrequentlyconcernedwiththeinterpretation and analysis of data, e.g., for prediction of future events and decision making. To make this task more tractable, we often build models that generativeprocess describethegenerativeprocessthatgeneratestheobserveddata. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.4 ProbabilisticModelingandInference 273 For example, we can describe the outcome of a coin-flip experiment (“heads” or “tails”) in two steps. First, we define a parameter µ, which describestheprobabilityof“heads”astheparameterofaBernoullidistribution (Chapter 6); second, we can sample an outcome x head,tail ∈ { } from the Bernoulli distribution p(x µ) = Ber(µ). The parameter µ gives | rise to a specific dataset and depends on the coin used. Since µ is unX known in advance and can never be observed directly, we need mechanisms to learn something about µ given observed outcomes of coin-flip experiments.Inthefollowing,wewilldiscusshowprobabilisticmodeling canbeusedforthispurpose. 8.4.1 Probabilistic Models Aprobabilistic Probabilistic models represent the uncertain aspects of an experiment as modelisspecified probability distributions. The benefit of using probabilistic models is that bythejoint distributionofall they offer a unified and consistent set of tools from probability theory randomvariables. (Chapter6)formodeling,inference,prediction,andmodelselection. In probabilistic modeling, the joint distribution p(x,θ) of the observed variables x and the hidden parameters θ is of central importance: It encapsulatesinformationfromthefollowing: Thepriorandthelikelihood(productrule,Section6.3). The marginal likelihood p(x), which will play an important role in modelselection(Section8.6),canbecomputedbytakingthejointdistributionandintegratingouttheparameters(sumrule,Section6.3). Theposterior,whichcanbeobtainedbydividingthejointbythemarginal likelihood. Only the joint distribution has this property. Therefore, a probabilistic modelisspecifiedbythejointdistributionofallitsrandomvariables. 8.4.2 Bayesian Inference Parameter Akeytaskinmachinelearningistotakeamodelandthedatatouncover estimationcanbe the values of the model’s hidden variables θ given the observed variables phrasedasan optimization x. In Section 8.3.1, we already discussed two ways for estimating model problem. parameters θ using maximum likelihood or maximum a posteriori estimation. In both cases, we obtain a single-best value for θ so that the key algorithmic problem of parameter estimation is solving an optimization problem.Oncethesepointestimatesθ∗ areknown,weusethemtomake predictions. More specifically, the predictive distribution will bep(x θ∗), whereweuseθ∗ inthelikelihoodfunction. |