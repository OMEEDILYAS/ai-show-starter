problems (Neumaier, 1998). The approach presented here is called Tikhonovregularization,andthereisacloselyrelatedconstrainedversion Tikhonov called Ivanov regularization. Tikhonov regularization has deep relation- regularization ships to the bias-variance trade-off and feature selection (Bu¨hlmann and Van De Geer, 2011). An alternative to cross-validation is bootstrap and jackknife(EfronandTibshirani,1993;DavidsonandHinkley,1997;Hall, 1992). Thinking about empirical risk minimization (Section 8.2) as “probability free” is incorrect. There is an underlying unknown probability distribution p(x,y) that governs the data generation. However, the approach of empirical risk minimization is agnostic to that choice of distribution. This is in contrast to standard statistical approaches that explicitly require the knowledge of p(x,y). Furthermore, since the distribution is a jointdistributiononbothexamplesxandlabelsy,thelabelscanbenondeterministic. In contrast to standard statistics we do not need to specify thenoisedistributionforthelabelsy. 8.3 Parameter Estimation In Section 8.2, we did not explicitly model our problem using probability distributions. In this section, we will see how to use probability distributions to model our uncertainty due to the observation process and our uncertainty in the parameters of our predictors. In Section 8.3.1, we introducethelikelihood,whichisanalogoustotheconceptoflossfunctions (Section8.2.2)inempiricalriskminimization.Theconceptofpriors(Section8.3.2)isanalogoustotheconceptofregularization(Section8.2.3). 8.3.1 Maximum Likelihood Estimation Theideabehindmaximumlikelihoodestimation(MLE)istodefineafunc- maximumlikelihood tion of the parameters that enables us to find a model that fits the data estimation well. The estimation problem is focused on the likelihood function, or likelihood more precisely its negative logarithm. For data represented by a random variable x and for a family of probability densities p(x θ) parametrized | byθ,thenegativelog-likelihoodisgivenby negative log-likelihood (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 266 WhenModelsMeetData (θ) = logp(x θ). (8.14) x L − | The notation (θ) emphasizes the fact that the parameter θ is varying x L andthedataxisfixed.Weveryoftendropthereferencetoxwhenwriting the negative log-likelihood, as it is really a function of θ, and write it as (θ) when the random variable representing the uncertainty in the data L isclearfromthecontext. Let us interpret what the probability density p(x θ) is modeling for a | fixedvalueofθ.Itisadistributionthatmodelstheuncertaintyofthedata. In other words, once we have chosen the type of function we want as a predictor,thelikelihoodprovidestheprobabilityofobservingdatax. In a complementary view, if we consider the data to be fixed (because ithasbeenobserved),andwevarytheparametersθ,whatdoes (θ)tell L us?Ittellsushowlikelyaparticularsettingofθ isfortheobservationsx. Basedonthissecondview,themaximumlikelihoodestimatorgivesusthe mostlikelyparameterθ forthesetofdata. We consider the supervised learning setting, where we obtain pairs (x ,y ),...,(x ,y ) with x RD and labels y R. We are inter1 1 N N n n ∈ ∈ ested in constructing a predictor that takes a feature vector x as input n and produces a prediction y (or something close to it), i.e., given a vecn torx wewanttheprobabilitydistributionofthelabely .Inotherwords, n n we specify the conditional probability distribution of the labels given the examplesfortheparticularparametersettingθ. Example 8.4 The first example that is often used is to specify that the conditional probability of the labels given the examples is a Gaussian distribution. In other words, we assume that we can explain our observation uncertainty by independent Gaussian noise (refer to Section 6.5) with zero mean, (cid:0) (cid:1) ε 0, σ2 . We further assume that the linear model x(cid:62)θ is used for n ∼ N n prediction.ThismeanswespecifyaGaussianlikelihoodforeachexample labelpair(x ,y ), n n p(y x ,θ) = (cid:0) y x(cid:62)θ, σ2(cid:1) . (8.15) n |