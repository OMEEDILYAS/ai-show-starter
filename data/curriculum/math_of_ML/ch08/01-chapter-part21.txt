we no longer have to fit the parameters). Furthermore, the marginal likelihood automatically embodies a trade-off between model complexity and datafit(Occam’srazor). ♦ Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.6 ModelSelection 287 8.6.3 Bayes Factors for Model Comparison Consider the problem of comparing two probabilistic models M ,M , 1 2 givenadataset .Ifwecomputetheposteriorsp(M )andp(M ), 1 2 D |D |D wecancomputetheratiooftheposteriors p(M ) p(D|M1)p(M1) p(M ) p( M ) 1 |D = p(D) = 1 D| 1 . (8.46) p(M 2 ) p(D|M2)p(M2) p(M 2 ) p( M 2 ) |D p(D) D| (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) posteriorodds priorodds Bayesfactor The ratio of the posteriors is also called the posterior odds. The first frac- posteriorodds tion on the right-hand side of (8.46), the prior odds, measures how much priorodds ourprior(initial)beliefsfavorM overM .Theratioofthemarginallike1 2 lihoods (second fraction on the right-hand-side) is called the Bayes factor Bayesfactor andmeasureshowwellthedata ispredictedbyM comparedtoM . 1 2 D Remark. The Jeffreys-Lindley paradox states that the “Bayes factor always Jeffreys-Lindley favorsthesimplermodelsincetheprobabilityofthedataunderacomplex paradox model with a diffuse prior will be very small” (Murphy, 2012). Here, a diffuse prior refers to a prior that does not favor specific models, i.e., manymodelsareaprioriplausibleunderthisprior. ♦ Ifwechooseauniformpriorovermodels,theprioroddstermin(8.46) is1,i.e.,theposterioroddsistheratioofthemarginallikelihoods(Bayes factor) p( M ) D| 1 . (8.47) p( M ) 2 D| If the Bayes factor is greater than 1, we choose model M , otherwise 1 model M . In a similar way to frequentist statistics, there are guidelines 2 on the size of the ratio that one should consider before ”significance” of theresult(Jeffreys,1961). Remark (Computing the Marginal Likelihood). The marginal likelihood plays an important role in model selection: We need to compute Bayes factors(8.46)andposteriordistributionsovermodels(8.43). Unfortunately, computing the marginal likelihood requires us to solve an integral (8.44). This integration is generally analytically intractable, and we will have to resort to approximation techniques, e.g., numerical integration (Stoer and Burlirsch, 2002), stochastic approximations using MonteCarlo(Murphy,2012),orBayesianMonteCarlotechniques(O’Hagan, 1991;RasmussenandGhahramani,2003). However,therearespecialcasesinwhichwecansolveit.InSection6.6.1, wediscussedconjugatemodels.Ifwechooseaconjugateparameterprior p(θ), we can compute the marginal likelihood in closed form. In Chapter9,wewilldoexactlythisinthecontextoflinearregression. ♦ We have seen a brief introduction to the basic concepts of machine learning in this chapter. For the rest of this part of the book we will see (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 288 WhenModelsMeetData howthethreedifferentflavorsoflearninginSections8.2,8.3,and8.4are appliedtothefourpillarsofmachinelearning(regression,dimensionality reduction,densityestimation,andclassification). 8.6.4 Further Reading Wementionedatthestartofthesectionthattherearehigh-levelmodeling choicesthatinfluencetheperformanceofthemodel.Examplesincludethe following: Thedegreeofapolynomialinaregressionsetting Thenumberofcomponentsinamixturemodel Thenetworkarchitectureofa(deep)neuralnetwork Thetypeofkernelinasupportvectormachine ThedimensionalityofthelatentspaceinPCA Thelearningrate(schedule)inanoptimizationalgorithm Inparametric models,thenumber Rasmussen and Ghahramani (2001) showed that the automatic Occam’s ofparametersis razordoesnotnecessarilypenalizethenumberofparametersinamodel, oftenrelatedtothe but it is active in terms of the complexity of functions. They also showed complexityofthe that the automatic Occam’s razor also holds for Bayesian nonparametric modelclass. modelswithmanyparameters,e.g.,Gaussianprocesses. Ifwefocusonthemaximumlikelihoodestimate,thereexistanumberof heuristics for model selection that discourage overfitting. They are called informationcriteria,andwechoosethemodelwiththelargestvalue.The Akaikeinformation Akaikeinformationcriterion(AIC)(Akaike,1974) criterion logp(x θ) M (8.48) | − corrects for the bias of the maximum likelihood estimator by addition of apenaltytermtocompensatefortheoverfittingofmorecomplexmodels withlotsofparameters.Here,M isthenumberofmodelparameters.The AICestimatestherelativeinformationlostbyagivenmodel. Bayesian TheBayesianinformationcriterion(BIC)(Schwarz,1978) information (cid:90) 1 criterion logp(x) = log p(x θ)p(θ)dθ logp(x θ) M logN (8.49) | ≈ | − 2 can be used for exponential family distributions. Here, N is the number of data points and M is the number of parameters. BIC penalizes model complexitymoreheavilythanAIC. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.