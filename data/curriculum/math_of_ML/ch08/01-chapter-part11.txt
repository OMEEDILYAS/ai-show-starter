This additional term is apriorprobabilitydistributiononparametersp(θ).Foragivenprior,after Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.3 ParameterEstimation 269 observing some data x, how should we update the distribution of θ? In otherwords,howshouldwerepresentthefactthatwehavemorespecific knowledge of θ after observing data x? Bayes’ theorem, as discussed in Section 6.3, gives us a principled tool to update our probability distributionsofrandomvariables.Itallowsustocomputeaposteriordistribution posterior p(θ x) (the more specific knowledge) on the parameters θ from general | prior statements (prior distribution) p(θ) and the function p(x θ) that prior | linkstheparametersθ andtheobserveddatax(calledthelikelihood): likelihood p(x θ)p(θ) p(θ x) = | . (8.19) | p(x) Recall that we are interested in finding the parameter θ that maximizes the posterior. Since the distribution p(x) does not depend on θ, we can ignorethevalueofthedenominatorfortheoptimizationandobtain p(θ x) p(x θ)p(θ). (8.20) | ∝ | The preceding proportion relation hides the density of the data p(x), which may be difficult to estimate. Instead of estimating the minimum of the negative log-likelihood, we now estimate the minimum of the negative log-posterior, which is referred to as maximum a posteriori estima- maximuma tion (MAP estimation). An illustration of the effect of adding a zero-mean posteriori estimation GaussianpriorisshowninFigure8.6. MAPestimation Example 8.6 InadditiontotheassumptionofGaussianlikelihoodinthepreviousexample, we assume that the parameter vector is distributed as a multivariate (cid:0) (cid:1) Gaussian with zero mean, i.e., p(θ) = 0, Σ , where Σ is the covariN ance matrix (Section 6.5). Note that the conjugate prior of a Gaussian is also a Gaussian (Section 6.6.1), and therefore we expect the posterior distribution to also be a Gaussian. We will see the details of maximum a posterioriestimationinChapter9. The idea of including prior knowledge about where good parameters lie is widespread in machine learning. An alternative view, which we saw in Section 8.2.3, is the idea of regularization, which introduces an additional term that biases the resulting parameters to be close to the origin. Maximum a posteriori estimation can be considered to bridge the nonprobabilistic and probabilistic worlds as it explicitly acknowledges the need for a prior distribution but it still only produces a point estimate oftheparameters. Remark. The maximum likelihood estimate θ possesses the following ML properties(LehmannandCasella,1998;EfronandHastie,2016): Asymptotic consistency: The MLE converges to the true value in the (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 270 WhenModelsMeetData Figure8.7 Model fitting.Ina M θ parametrizedclass M θofmodels,we M optimizethemodel θ∗ parametersθto M∗ M minimizethe θ0 distancetothetrue (unknown)model M∗. limit of infinitely many observations, plus a random error that is approximatelynormal. The size of the samples necessary to achieve these properties can be quitelarge. The error’s variance decays in 1/N, where N is the number of data points. Especially, in the “small” data regime, maximum likelihood estimation canleadtooverfitting. ♦ Theprincipleofmaximumlikelihoodestimation(andmaximumaposteriori estimation) uses probabilistic modeling to reason about the uncertaintyinthedataandmodelparameters.However,wehavenotyettaken probabilisticmodelingtoitsfullextent.Inthissection,theresultingtrainingprocedurestillproducesapointestimateofthepredictor,i.e.,training returns one single set of parameter values that represent the best predictor.InSection8.4,wewilltaketheviewthattheparametervaluesshould also be treated as random variables, and instead of estimating “best” values of that distribution, we will use the full parameter distribution when makingpredictions. 8.3.3 Model Fitting Consider the setting where we are given a dataset, and we are interested in fitting a parametrized model to the data. When we talk about