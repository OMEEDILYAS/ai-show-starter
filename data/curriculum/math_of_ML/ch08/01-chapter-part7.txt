we address in the following “populationrisk”. twosubsections: Howshouldwechangeourtrainingproceduretogeneralizewell? Howdoweestimateexpectedriskfrom(finite)data? Remark. Many machine learning tasks are specified with an associated performance measure, e.g., accuracy of prediction or root mean squared error.Theperformancemeasurecouldbemorecomplex,becostsensitive, and capture details about the particular application. In principle, the designofthelossfunctionforempiricalriskminimizationshouldcorrespond directly to the performance measure specified by the machine learning task.Inpractice,thereisoftenamismatchbetweenthedesignoftheloss function and the performance measure. This could be due to issues such aseaseofimplementationorefficiencyofoptimization. ♦ (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 262 WhenModelsMeetData 8.2.3 Regularization to Reduce Overfitting This section describes an addition to empirical risk minimization that allows it to generalize well (approximately minimizing expected risk). Recallthattheaimoftrainingamachinelearningpredictorissothatwecan performwell onunseen data,i.e., thepredictor generalizeswell. Wesimulate this unseen data by holding out a proportion of the whole dataset. testset Thisholdoutsetisreferredtoasthetestset.Givenasufficientlyrichclass Evenknowingonly offunctionsforthepredictorf,wecanessentiallymemorizethetraining theperformanceof datatoobtainzeroempiricalrisk.Whilethisisgreattominimizetheloss thepredictoronthe (and therefore the risk) on the training data, we would not expect the testsetleaks information(Blum predictor to generalize well to unseen data. In practice, we have only a andHardt,2015). finite set of data, and hence we split our data into a training and a test set. The training set is used to fit the model, and the test set (not seen by the machine learning algorithm during training) is used to evaluate generalization performance. It is important for the user to not cycle back to a new round of training after having observed the test set. We use the subscripts and to denote the training and test sets, respectively. train test We will revisit this idea of using a finite dataset to evaluate expected risk inSection8.2.4. overfitting Itturnsoutthatempiricalriskminimizationcanleadtooverfitting,i.e., the predictor fits too closely to the training data and does not generalize well to new data (Mitchell, 1997). This general phenomenon of having very small average loss on the training set but large average loss on the test set tends to occur when we have little data and a complex hypothesis class. For a particular predictor f (with parameters fixed), the phenomenon of overfitting occurs when the risk estimate from the trainingdataR (f,X ,y )underestimatestheexpectedriskR (f). emp train train true Since we estimate the expected risk R (f) by using the empirical risk true on the test set R (f,X ,y ) if the test risk is much larger than emp test test the training risk, this is an indication of overfitting. We revisit the idea of overfittinginSection8.3.3. Therefore, we need to somehow bias the search for the minimizer of empirical risk by introducing a penalty term, which makes it harder for the optimizer to return an overly flexible predictor. In machine learning, regularization the penalty term is referred to as regularization. Regularization is a way to compromise between accurate solution of empirical risk minimization andthesizeorcomplexityofthesolution. Example 8.3 (Regularized Least Squares) Regularization is an approach that discourages complex or extreme solutions to an optimization problem. The simplest regularization strategy is Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.2 EmpiricalRiskMinimization 263 toreplacetheleast-squaresproblem 1 min y Xθ 2 . (8.11) θ N (cid:107) − (cid:107) in the previous example with the “regularized” problem by adding a penaltyterminvolvingonlyθ: 1 min y Xθ 2 +λ θ 2 . (8.12) θ N (cid:107)