Razor” is quantitativelyembodiedintheapplicationofBayesianprobability(Smith and Spiegelhalter, 1980; Jefferys and Berger, 1992; MacKay, 1992). Figure 8.14, adapted from MacKay (2003), gives us the basic intuition why complex and very expressive models may turn out to be a less probable choice for modeling a given dataset . Let us think of the horizontal axis Thesepredictions D representing the space of all possible datasets . If we are interested in arequantifiedbya theposteriorprobabilityp(M )ofmodelM D giventhedata ,wecan normalized i i |D D probability employ Bayes’ theorem. Assuming a uniform prior p(M) over all moddistributiononD, els, Bayes’ theorem rewards models in proportion to how much they pre- i.e.,itneedsto dicted the data that occurred. This prediction of the data given model integrate/sumto1. M i , p( M i ), is called the evidence for M i . A simple model M 1 can only evidence D| predict a small number of datasets, which is shown by p( M ); a more 1 D| powerful model M that has, e.g., more free parameters than M , is able 2 1 (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 286 WhenModelsMeetData to predict a greater variety of datasets. This means, however, that M 2 does not predict the datasets in region C as well as M . Suppose that 1 equal prior probabilities have been assigned to the two models. Then, if the dataset falls into region C, the less powerful model M is the more 1 probablemodel. Earlierinthischapter,wearguedthatmodelsneedtobeabletoexplain thedata,i.e.,thereshouldbeawaytogeneratedatafromagivenmodel. Furthermore, if the model has been appropriately learned from the data, thenweexpectthatthegenerateddatashouldbesimilartotheempirical data. For this, it is helpful to phrase model selection as a hierarchical inference problem, which allows us to compute the posterior distribution overmodels. Let us consider a finite number of models M = M ,...,M , where 1 K { } Bayesianmodel each model M k possesses parameters θ k . In Bayesian model selection, we selection place a prior p(M) on the set of models. The corresponding generative generativeprocess processthatallowsustogeneratedatafromthismodelis Figure8.15 Illustrationofthe M p(M) (8.40) k ∼ hierarchical θ p(θ M ) (8.41) generativeprocess k ∼ | k inBayesianmodel p( θ ) (8.42) k selection.Weplace D ∼ D| apriorp(M)onthe and illustrated in Figure 8.15. Given a training set , we apply Bayes’ D setofmodels.For theoremandcomputetheposteriordistributionovermodelsas eachmodel,thereis adistribution p(M ) p(M )p( M ). (8.43) k k k p(θ|M)onthe |D ∝ D| corresponding Note that this posterior no longer depends on the model parameters θ k modelparameters, becausetheyhavebeenintegratedoutintheBayesiansettingsince whichisusedto (cid:90) generatethedataD. p( M ) = p( θ )p(θ M )dθ , (8.44) k k k k k D| D| | M where p(θ M ) is the prior distribution of the model parameters θ of k k k | modelM .Theterm(8.44)isreferredtoasthemodelevidenceormarginal k likelihood.Fromtheposteriorin(8.43),wedeterminetheMAPestimate θ M∗ = argmaxp(M ). (8.45) k Mk |D With a uniform prior p(M ) = 1, which gives every model equal (prior) k K D probability, determining the MAP estimate over models amounts to pickmodelevidence ingthemodelthatmaximizesthemodelevidence(8.44). marginallikelihood Remark(LikelihoodandMarginalLikelihood). Therearesomeimportant differences between a likelihood and a marginal likelihood (evidence): Whilethelikelihoodispronetooverfitting,themarginallikelihoodistypically not as the model parameters have been marginalized out (i.e.,