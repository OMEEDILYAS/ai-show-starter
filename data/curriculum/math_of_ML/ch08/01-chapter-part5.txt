abduction is called abduction. According to the Stanford Encyclopedia of Philosophy, abduction is the process of inference to the best explanation (Douven, Agoodmovietitleis 2017). “AIabduction”. We often need to make high-level modeling decisions about the structure of the predictor, such as the number of components to use or the class of probability distributions to consider. The choice of the number of hyperparameter components is an example of a hyperparameter, and this choice can affect the performance of the model significantly. The problem of choosing modelselection among different models is called model selection, which we describe in Section 8.6. For non-probabilistic models, model selection is often done nested using nested cross-validation, which is described in Section 8.6.1. We also cross-validation usemodelselectiontochoosehyperparametersofourmodel. Remark. Thedistinctionbetweenparametersandhyperparametersissomewhat arbitrary, and is mostly driven by the distinction between what can be numerically optimized versus what needs to use search techniques. Another way to consider the distinction is to consider parameters as the explicitparametersofaprobabilisticmodel,andtoconsiderhyperparameters(higher-levelparameters)asparametersthatcontrolthedistribution oftheseexplicitparameters. ♦ Inthefollowingsections,wewilllookatthreeflavorsofmachinelearning: empirical risk minimization (Section 8.2), the principle of maximum likelihood(Section8.3),andprobabilisticmodeling(Section8.4). 8.2 Empirical Risk Minimization After having all the mathematics under our belt, we are now in a position to introduce what it means to learn. The “learning” part of machine learningboilsdowntoestimatingparametersbasedontrainingdata. In this section, we consider the case of a predictor that is a function, and consider the case of probabilistic models in Section 8.3. We describe theideaofempiricalriskminimization,whichwasoriginallypopularized by the proposal of the support vector machine (described in Chapter 12). However, its general principles are widely applicable and allow us to ask thequestionofwhatislearningwithoutexplicitlyconstructingprobabilistic models. There are four main design choices, which we will cover in detailinthefollowingsubsections: Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 8.2 EmpiricalRiskMinimization 259 Section 8.2.1 Whatisthesetoffunctionsweallowthepredictortotake? Section 8.2.2 How do we measure how well the predictor performs on thetrainingdata? Section 8.2.3 How do we construct predictors from only training data thatperformswellonunseentestdata? Section 8.2.4 Whatistheprocedureforsearchingoverthespaceofmodels? 8.2.1 Hypothesis Class of Functions Assume we are given N examples x RD and corresponding scalar lan belsy R.Weconsiderthesupervised ∈ learningsetting,whereweobtain n ∈ pairs (x ,y ),...,(x ,y ). Given this data, we would like to estimate a 1 1 N N predictorf( ,θ) : RD R,parametrizedbyθ.Wehopetobeabletofind agoodparam · eterθ∗ s → uchthatwefitthedatawell,thatis, f(x ,θ∗) y forall n = 1,...,N . (8.3) n n ≈ Inthissection,weusethenotationyˆ = f(x ,θ∗)torepresenttheoutput n n ofthepredictor. Remark. For ease of presentation, we will describe empirical risk minimization in terms of supervised learning (where we have labels). This simplifies the definition of the hypothesis class and the loss function. It is also common in machine learning to choose a parametrized class of functions,forexampleaffinefunctions. ♦ Example 8.1 Weintroducetheproblemofordinaryleast-squaresregressiontoillustrate empiricalriskminimization.Amorecomprehensiveaccountofregression is given in Chapter 9. When the label y is real-valued, a popular choice n of function class for predictors is the set of affine functions. We choose a Affinefunctionsare more compact notation for an affine function by concatenating an addi- oftenreferredtoas linearfunctionsin tional unit feature x(0) = 1 to x , i.e., x = [1,x(1),x(2),...,x(D)](cid:62). The n n n n n machinelearning. parametervectoriscorrespondinglyθ = [θ ,θ ,θ ,...,θ ](cid:62),allowingus 0 1 2 D towritethepredictorasalinearfunction f(x ,θ)