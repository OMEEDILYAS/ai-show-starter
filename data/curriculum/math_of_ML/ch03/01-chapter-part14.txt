··· ···  0 cosθ 0 sinθ 0  R ij (θ) :=   0 0 I j−i−1 − 0 0   Rn×n, (3.80)   ∈  0 sinθ 0 cosθ 0  0 0 I n−j ··· ··· Givensrotation for 1 (cid:54) i < j (cid:54) n and θ R. Then R ij (θ) is called a Givens rotation. ∈ Essentially,R (θ)istheidentitymatrixI with ij n r = cosθ, r = sinθ, r = sinθ, r = cosθ. (3.81) ii ij ji jj − Intwodimensions(i.e.,n = 2),weobtain(3.76)asaspecialcase. 3.9.4 Properties of Rotations Rotations exhibit a number of useful properties, which can be derived by consideringthemasorthogonalmatrices(Definition3.8): Rotationspreservedistances,i.e., x y = R (x) R (y) .Inother θ θ (cid:107) − (cid:107) (cid:107) − (cid:107) words,rotationsleavethedistancebetweenanytwopointsunchanged afterthetransformation. Rotationspreserveangles,i.e.,theanglebetweenR xandR y equals θ θ theanglebetweenxandy. Rotations in three (or more) dimensions are generally not commutative. Therefore, the order in which rotations are applied is important, eveniftheyrotateaboutthesamepoint.Onlyintwodimensionsvector rotations are commutative, such that R(φ)R(θ) = R(θ)R(φ) for all φ,θ [0,2π).TheyformanAbeliangroup(withmultiplication)onlyif ∈ theyrotateaboutthesamepoint(e.g.,theorigin). 3.10 Further Reading Inthischapter,wegaveabriefoverviewofsomeoftheimportantconcepts ofanalyticgeometry,whichwewilluseinlaterchaptersofthebook.Fora broader and more in-depth overview of some the concepts we presented, we refer to the following excellent books: Axler (2015) and Boyd and Vandenberghe(2018). Innerproductsallowustodeterminespecificbasesofvector(sub)spaces, whereeachvectorisorthogonaltoallothers(orthogonalbases)usingthe Gram-Schmidt method. These bases are important in optimization and numerical algorithms for solving linear equation systems. For instance, Krylov subspace methods, such as conjugate gradients or the generalized minimal residual method (GMRES), minimize residual errors that are orthogonaltoeachother(StoerandBurlirsch,2002). In machine learning, inner products are important in the context of Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 3.10 FurtherReading 95 kernelmethods(Scho¨lkopfandSmola,2002).Kernelmethodsexploitthe fact that many linear algorithms can be expressed purely by inner product computations. Then, the “kernel trick” allows us to compute these inner products implicitly in a (potentially infinite-dimensional) feature space,withoutevenknowingthisfeaturespaceexplicitly.Thisallowedthe “non-linearization”ofmanyalgorithmsusedinmachinelearning,suchas kernel-PCA (Scho¨lkopf et al., 1997) for dimensionality reduction. Gaussianprocesses(RasmussenandWilliams,2006)alsofallintothecategory of kernel methods and are the current state of the art in probabilistic regression (fitting curves to data points). The idea of kernels is explored furtherinChapter12. Projectionsareoftenusedincomputergraphics,e.g.,togenerateshadows.Inoptimization,orthogonalprojectionsareoftenusedto(iteratively) minimize residual errors. This also has applications in machine learning, e.g., in linear regression where we want to find a (linear) function that minimizes the residual errors, i.e., the lengths of the orthogonal projections of the data onto the linear function (Bishop, 2006). We will investigate this further in Chapter 9. PCA (Pearson, 1901; Hotelling, 1933) also uses projections to reduce the dimensionality of high-dimensional data. WewilldiscussthisinmoredetailinChapter10. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 96 AnalyticGeometry Exercises 3.1 Showthat(cid:104)·,·(cid:105)definedforallx=[x ,x ](cid:62) ∈R2 andy=[y ,y ](cid:62) ∈R2 by 1 2 1 2 (cid:104)x,y(cid:105):=x y −(x y +x y )+2(x y ) 1 1 1 2 2 1 2 2 isaninnerproduct. 3.2 ConsiderR2 with(cid:104)·,·(cid:105)definedforallxandyinR2 as (cid:20) (cid:21) 2 0 (cid:104)x,y(cid:105):=x(cid:62) y. 1 2 (cid:124) (cid:123)(cid:122) (cid:125) =:A Is(cid:104)·,·(cid:105)aninnerproduct? 3.3 Computethedistancebetween     1 −1 x=2 , y=−1 3 0 using a. (cid:104)x,y(cid:105):=x(cid:62)y   2 1 0 b. (cid:104)x,y(cid:105):=x(cid:62)Ay, A:=1 3 −1 0 −1 2 3.4 Computetheanglebetween (cid:20) (cid:21) (cid:20) (cid:21) 1 −1 x= , y= 2 −1 using a. (cid:104)x,y(cid:105):=x(cid:62)y (cid:20) (cid:21) 2 1 b. (cid:104)x,y(cid:105):=x(cid:62)By, B:= 1 3 3.5 Consider the Euclidean vector