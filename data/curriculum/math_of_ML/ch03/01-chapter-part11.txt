6 1 2 5 − Toverifytheresults,wecan(a)checkwhetherthedisplacementvector π (x) x is orthogonal to all basis vectors of U, and (b) verify that U P = P −2 (seeDefinition3.10). π π Remark. Theprojectionsπ (x)arestillvectorsinRn althoughtheyliein U an m-dimensional subspace U Rn. However, to represent a projected ⊆ vector we only need the m coordinates λ ,...,λ with respect to the 1 m basisvectorsb ,...,b ofU. 1 m ♦ Remark. In vector spaces with general inner products, we have to pay attention when computing angles and distances, which are defined by meansoftheinnerproduct. Wecanfind ♦ approximate Projectionsallowustolookatsituationswherewehavealinearsystem solutionsto Ax = b without a solution. Recall that this means that b does not lie in unsolvablelinear the span of A, i.e., the vector b does not lie in the subspace spanned by equationsystems thecolumnsofA.Giventhatthelinearequationcannotbesolvedexactly, usingprojections. we can find an approximate solution. The idea is to find the vector in the subspacespannedbythecolumnsofAthatisclosesttob,i.e.,wecompute theorthogonalprojectionofbontothesubspacespannedbythecolumns of A. This problem arises often in practice, and the solution is called the least-squares least-squares solution (assuming the dot product as the inner product) of solution an overdetermined system. This is discussed further in Section 9.4. Using reconstruction errors (3.63) is one possible approach to derive principal componentanalysis(Section10.3). Remark. WejustlookedatprojectionsofvectorsxontoasubspaceU with basis vectors b ,...,b . If this basis is an ONB, i.e., (3.33) and (3.34) 1 k { } aresatisfied,theprojectionequation(3.58)simplifiesgreatlyto π (x) = BB(cid:62)x (3.65) U sinceB(cid:62)B = I withcoordinates λ = B(cid:62)x. (3.66) This means that we no longer have to compute the inverse from (3.58), whichsavescomputationtime. ♦ Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 3.8 OrthogonalProjections 89 3.8.3 Gram-Schmidt Orthogonalization Projections are at the core of the Gram-Schmidt method that allows us to constructivelytransformanybasis(b ,...,b )ofann-dimensionalvector 1 n space V into an orthogonal/orthonormal basis (u ,...,u ) of V. This 1 n basisalwaysexists(LiesenandMehrmann,2015)andspan[b ,...,b ] = 1 n span[u 1 ,...,u n ].TheGram-Schmidtorthogonalizationmethoditeratively Gram-Schmidt constructsanorthogonalbasis(u ,...,u )fromanybasis(b ,...,b )of orthogonalization 1 n 1 n V asfollows: u := b (3.67) 1 1 u := b π (b ), k = 2,...,n. (3.68) k k − span[u1,...,uk−1] k In (3.68), the kth basis vector b is projected onto the subspace spanned k by the first k 1 constructed orthogonal vectors u ,...,u ; see Sec1 k−1 − tion 3.8.2. This projection is then subtracted from b and yields a vector k u that is orthogonal to the (k 1)-dimensional subspace spanned by k − u ,...,u . Repeating this procedure for all n basis vectors b ,...,b 1 k−1 1 n yields an orthogonal basis (u ,...,u ) of V. If we normalize the u , we 1 n k obtainanONBwhere u = 1fork = 1,...,n. k (cid:107) (cid:107) Example 3.12 (Gram-Schmidt Orthogonalization) Figure3.12 b2 b2 u2 b2 Gram-Schmidt orthogonalization. (a)non-orthogonal basis(b1,b2)ofR2; 0 b1 0 π span[u1](b2) u1 0 π span[u1](b2) u1 (b)firstconstructed (a) Original non-orthogonal (b) First new basis vector (c) Orthogonal basis vectors u1 basisvectoru1and orthogonal basisvectorsb1,b2. u on 1 to = th b e 1 s a u n b d sp p a r c o e je s c p t