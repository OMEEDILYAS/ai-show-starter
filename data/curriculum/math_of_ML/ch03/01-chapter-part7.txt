product.If(3.37)evaluatesto0,thefunctionsuandv areorthogonal.To maketheprecedinginnerproductmathematicallyprecise,weneedtotake careofmeasuresandthedefinitionofintegrals,leadingtothedefinitionof aHilbertspace.Furthermore,unlikeinnerproductsonfinite-dimensional vectors,innerproductsonfunctionsmaydiverge(haveinfinitevalue).All thisrequiresdivingintosomemoreintricatedetailsofrealandfunctional analysis,whichwedonotcoverinthisbook. Example 3.9 (Inner Product of Functions) If we choose u = sin(x) and v = cos(x), the integrand f(x) = u(x)v(x) Figure3.8 f(x)= sin(x)cos(x). 0.5 0.0 0.5 − 2.5 0.0 2.5 − x )x(soc)x(nis of (3.37), is shown in Figure 3.8. We see that this function is odd, i.e., f( x) = f(x).Therefore,theintegralwithlimitsa = π,b = π ofthis − − − productevaluatesto0.Therefore,sinandcosareorthogonalfunctions. Remark. Italsoholdsthatthecollectionoffunctions 1,cos(x),cos(2x),cos(3x),... (3.38) { } is orthogonal if we integrate from π to π, i.e., any pair of functions are − orthogonal to each other. The collection of functions in (3.38) spans a largesubspaceofthefunctionsthatareevenandperiodicon[ π,π),and − projecting functions onto this subspace is the fundamental idea behind Fourierseries. ♦ InSection6.4.6,wewillhavealookatasecondtypeofunconventional innerproducts:theinnerproductofrandomvariables. 3.8 Orthogonal Projections Projectionsareanimportantclassoflineartransformations(besidesrotations and reflections) and play an important role in graphics, coding theory, statistics and machine learning. In machine learning, we often deal with data that is high-dimensional. High-dimensional data is often hard to analyze or visualize. However, high-dimensional data quite often possessesthepropertythatonlyafewdimensionscontainmostinformation, and most other dimensions are not essential to describe key properties of the data. When we compress or visualize high-dimensional data, we will lose information. To minimize this compression loss, we ideally find the most informative dimensions in the data. As discussed in Chapter 1, “Feature”isa data can be represented as vectors, and in this chapter, we will discuss commonexpression fordata someofthefundamentaltoolsfordatacompression.Morespecifically,we representation. can project the original high-dimensional data onto a lower-dimensional feature space and work in this lower-dimensional space to learn more about the dataset and extract relevant patterns. For example, machine (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 82 AnalyticGeometry Figure3.9 Orthogonal 2 projection(orange dots)ofa 1 two-dimensional dataset(bluedots) 0 ontoa one-dimensional 1 subspace(straight − line). 2 − 4 2 0 2 4 − − x 1 x 2 learningalgorithms,suchasprincipalcomponentanalysis(PCA)byPearson (1901) and Hotelling (1933) and deep neural networks (e.g., deep auto-encoders(Dengetal.,2010)),heavilyexploittheideaofdimensionality reduction. In the following, we will focus on orthogonal projections, which we will use in Chapter 10 for linear dimensionality reduction and in Chapter 12 for classification. Even linear regression, which we discuss inChapter9,canbeinterpretedusingorthogonalprojections.Foragiven lower-dimensional subspace, orthogonal projections of high-dimensional dataretainasmuchinformationaspossibleandminimizethedifference/ error between the original data and the corresponding projection. An illustration of such an orthogonal projection is given in Figure 3.9. Before we detail how to obtain these projections, let us define what a projection actuallyis. Definition 3.10 (Projection). Let V be a vector space and U V a ⊆ projection subspace of V. A linear mapping π : V U is called a projection if → π2 = π π = π. ◦ Sincelinearmappingscanbeexpressedbytransformationmatrices(see Section 2.7), the preceding definition applies equally to a special kind projectionmatrix of transformation matrices, the projection matrices P π , which exhibit the propertythatP2 = P . π π Inthefollowing,wewillderiveorthogonalprojectionsofvectorsinthe inner product space (Rn, , ) onto subspaces. We will start with one- (cid:104)· ·(cid:105) line dimensional subspaces, which are also called lines. If not mentioned otherwise,weassumethedotproduct x,y = x(cid:62)y astheinnerproduct. (cid:104) (cid:105) 3.8.1 Projection onto One-Dimensional Subspaces (Lines) Assume we are given a line (one-dimensional subspace) through the origin with