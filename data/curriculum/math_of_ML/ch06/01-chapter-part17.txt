the mean and vari2 − ancerespectively.Numerically,thisis µ = 0+( 1) 0.2 ( 1 2) = 0.6 (6.70) x1|x2=−1 − · · − − and σ2 = 0.3 ( 1) 0.2 ( 1) = 0.1. (6.71) x1|x2=−1 − − · · − Therefore,theconditionalGaussianisgivenby (cid:0) (cid:1) p(x x = 1) = 0.6, 0.1 . (6.72) 1 2 | − N The marginal distribution p(x ), in contrast, can be obtained by apply1 ing(6.68),whichisessentiallyusingthemeanandvarianceoftherandom variablex ,givingus 1 (cid:0) (cid:1) p(x ) = 0, 0.3 . (6.73) 1 N Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.5 GaussianDistribution 201 6.5.2 Product of Gaussian Densities For linear regression (Chapter 9), we need to compute a Gaussian likelihood.Furthermore,wemaywishtoassumeaGaussianprior(Section9.3). WeapplyBayes’Theoremtocomputetheposterior,whichresultsinamultiplicationofthelikelihoodandtheprior,thatis,themultiplicationoftwo (cid:0) (cid:1) (cid:0) (cid:1) Gaussiandensities.TheproductoftwoGaussians x a, A x b, B Thederivationisan isaGaussiandistributionscaledbyac R,give N nbyc | (cid:0) x N c, C (cid:1)| with exerciseattheend ∈ N | ofthischapter. C = (A−1+B−1)−1 (6.74) c = C(A−1a+B−1b) (6.75) c = (2π)− D 2 A+B − 1 2 exp (cid:0) 1(a b)(cid:62)(A+B)−1(a b) (cid:1) . (6.76) | | − 2 − − The scaling constant c itself can be written in the form of a Gaussian density either in a or in b with an “inflated” covariance matrix A + B, (cid:0) (cid:1) (cid:0) (cid:1) i.e.,c = a b, A+B = b a, A+B . N | N | (cid:0) (cid:1) Remark. For notation convenience, we will sometimes use x m, S N | to describe the functional form of a Gaussian density even if x is not a random variable. We have just done this in the preceding demonstration whenwewrote (cid:0) (cid:1) (cid:0) (cid:1) c = a b, A+B = b a, A+B . (6.77) N | N | Here,neitheranorbarerandomvariables.However,writingcinthisway ismorecompactthan(6.76). ♦ 6.5.3 Sums and Linear Transformations If X,Y are independent Gaussian random variables (i.e., the joint distri- (cid:0) (cid:1) bution is given as p(x,y) = p(x)p(y)) with p(x) = x µ , Σ and (cid:0) (cid:1) N | x x p(y) = y µ , Σ ,thenx+y isalsoGaussiandistributedandgiven N | y y by (cid:0) (cid:1) p(x+y) = µ +µ , Σ +Σ . (6.78) N x y x y Knowing that p(x+y) is Gaussian, the mean and covariance matrix can bedeterminedimmediatelyusingtheresultsfrom(6.46)through(6.49). This property will be important when we consider i.i.d. Gaussian noise acting on random variables, as is the case for linear regression (Chapter9). Example 6.7 Sinceexpectationsarelinearoperations,wecanobtaintheweightedsum ofindependentGaussianrandomvariables p(ax+by) = (cid:0) aµ +bµ , a2Σ +b2Σ (cid:1) . (6.79) N x y x y (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 202 ProbabilityandDistributions Remark. A case that will be useful in Chapter 11 is the weighted sum of Gaussian densities. This is different from the weighted sum of Gaussian randomvariables. ♦ In Theorem 6.12, the random variable x is from a density that is a mixtureoftwodensitiesp (x)andp (x),weightedbyα.Thetheoremcan 1 2 begeneralizedtothemultivariaterandomvariablecase,sincelinearityof expectations holds also for multivariate random variables. However, the ideaofasquaredrandomvariableneedstobereplacedbyxx(cid:62). Theorem 6.12. ConsideramixtureoftwounivariateGaussiandensities p(x) = αp (x)+(1 α)p (x), (6.80) 1 2 − where the scalar 0 < α < 1 is the mixture weight, and p (x) and p (x) are 1 2 univariate Gaussian densities (Equation