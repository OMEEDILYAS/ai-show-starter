prior for the multinomial likelihood function. For further details, we refertoBishop(2006). 6.6.2 Sufficient Statistics Recall that a statistic of a random variable is a deterministic function of that random variable. For example, if x = [x ,...,x ](cid:62) is a vector of 1 N (cid:0) (cid:1) univariate Gaussian random variables, that is, x µ, σ2 , then the n ∼ N sample mean µˆ = 1(x + +x ) is a statistic. Sir Ronald Fisher disN 1 ··· N sufficientstatistics covered the notion of sufficient statistics: the idea that there are statistics that will contain all available information that can be inferred from data correspondingtothedistributionunderconsideration.Inotherwords,sufficientstatisticscarryalltheinformationneededtomakeinferenceabout the population, that is, they are the statistics that are sufficient to representthedistribution. Forasetofdistributionsparametrizedbyθ,letX bearandomvariable withdistributionp(x θ )givenanunknownθ .Avectorφ(x)ofstatistics 0 0 | is called sufficient statistics for θ if they contain all possible informa0 tionaboutθ .Tobemoreformalabout“containallpossibleinformation”, 0 this means that the probability of x given θ can be factored into a part that does not depend on θ, and a part that depends on θ only via φ(x). The Fisher-Neyman factorization theorem formalizes this notion, which westateinTheorem6.14withoutproof. Theorem 6.14 (Fisher-Neyman). [Theorem 6.5 in Lehmann and Casella Fisher-Neyman (1998)] Let X have probability density function p(x θ). Then the statistics | theorem φ(x)aresufficientforθ ifandonlyifp(x θ)canbewrittenintheform | p(x θ) = h(x)g (φ(x)), (6.106) θ | where h(x) is a distribution independent of θ and g captures all the depenθ denceonθ viasufficientstatisticsφ(x). Ifp(x θ)doesnotdependonθ,thenφ(x)istriviallyasufficientstatistic | for any function φ. The more interesting case is that p(x θ) is dependent | only on φ(x) and not x itself. In this case, φ(x) is a sufficient statistic for θ. In machine learning, we consider a finite number of samples from a distribution. One could imagine that for simple distributions (such as the Bernoulli in Example 6.8) we only need a small number of samples to estimate the parameters of the distributions. We could also consider the opposite problem: If we have a set of data (a sample from an unknown distribution), which distribution gives the best fit? A natural question to ask is, as we observe more data, do we need more parameters θ to describe the distribution? It turns out that the answer is yes in general, and thisisstudiedinnon-parametricstatistics(Wasserman,2007).Aconverse questionistoconsiderwhichclassofdistributionshavefinite-dimensional Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.6 ConjugacyandtheExponentialFamily 211 sufficient statistics, that is the number of parameters needed to describe them does not increase arbitrarily. The answer is exponential family distributions,describedinthefollowingsection. 6.6.3 Exponential Family There are three possible levels of abstraction we can have when considering distributions (of discrete or continuous random variables). At level one (the most concrete end of the spectrum), we have a particular named distribution with fixed parameters, for example a univariate (cid:0) (cid:1) Gaussian 0, 1 withzeromeanandunitvariance.Inmachinelearning, N we often use the second level of abstraction, that is, we fix the parametricform(theunivariateGaussian)andinfertheparametersfromdata.For (cid:0) (cid:1) example,weassumeaunivariateGaussian µ, σ2 withunknownmean N µ and unknown variance σ2, and use a maximum likelihood fit to determine the best parameters (µ,σ2). We will see an example of this when considering linear regression in Chapter