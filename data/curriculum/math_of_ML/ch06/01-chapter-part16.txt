. Fig- ∈ N | ∼ N multivariatenormal ure 6.7 shows a bivariate Gaussian (mesh), with the corresponding condistribution. tourplot.Figure6.8showsaunivariateGaussianandabivariateGaussian with corresponding samples. The special case of the Gaussian with zero meanandidentitycovariance,thatis,µ = 0andΣ = I,isreferredtoas standardnormal thestandardnormaldistribution. distribution Gaussians are widely used in statistical estimation and machine learningastheyhaveclosed-formexpressionsformarginalandconditionaldistributions.InChapter9,weusetheseclosed-formexpressionsextensively for linear regression. A major advantage of modeling with Gaussian randomvariablesisthatvariabletransformations(Section6.7)areoftennot needed. Since the Gaussian distribution is fully specified by its mean and covariance, we often can obtain the transformed distribution by applying thetransformationtothemeanandcovarianceoftherandomvariable. 6.5.1 Marginals and Conditionals of Gaussians are Gaussians Inthefollowing,wepresentmarginalizationandconditioninginthegeneralcaseofmultivariaterandomvariables.Ifthisisconfusingatfirstreading,thereaderisadvisedtoconsidertwounivariaterandomvariablesinstead. Let X and Y be two multivariate random variables, that may have Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.5 GaussianDistribution 199 different dimensions. To consider the effect of applying the sum rule of probability and the effect of conditioning, we explicitly write the Gaussiandistributionintermsoftheconcatenatedstates[x,y](cid:62), (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) µ Σ Σ p(x,y) = x , xx xy . (6.64) N µ y Σ yx Σ yy where Σ = Cov[x,x] and Σ = Cov[y,y] are the marginal covarixx yy ance matrices of x and y, respectively, and Σ = Cov[x,y] is the crossxy covariancematrixbetweenxandy. Theconditionaldistributionp(x y)isalsoGaussian(illustratedinFig- | ure6.9(c))andgivenby(derivedinSection2.3ofBishop,2006) (cid:0) (cid:1) p(x y) = µ , Σ (6.65) | N x|y x|y µ = µ +Σ Σ−1(y µ ) (6.66) x|y x xy yy − y Σ = Σ Σ Σ−1Σ . (6.67) x|y xx − xy yy yx Note that in the computation of the mean in (6.66), the y-value is an observationandnolongerrandom. Remark. The conditional Gaussian distribution shows up in many places, whereweareinterestedinposteriordistributions: The Kalman filter (Kalman, 1960), one of the most central algorithms for state estimation in signal processing, does nothing but computing Gaussian conditionals of joint distributions (Deisenroth and Ohlsson, 2011;Sa¨rkka¨,2013). Gaussianprocesses(RasmussenandWilliams,2006),whichareapracticalimplementationofadistributionoverfunctions.InaGaussianprocess,wemakeassumptionsofjointGaussianityofrandomvariables.By (Gaussian) conditioning on observed data, we can determine a posteriordistributionoverfunctions. Latent linear Gaussian models (Roweis and Ghahramani, 1999; Murphy, 2012), which include probabilistic principal component analysis (PPCA) (Tipping and Bishop, 1999). We will look at PPCA in more detailinSection10.7. ♦ The marginal distribution p(x) of a joint Gaussian distribution p(x,y) (see (6.64)) is itself Gaussian and computed by applying the sum rule (6.20)andgivenby (cid:90) (cid:0) (cid:1) p(x) = p(x,y)dy = x µ , Σ . (6.68) N | x xx Thecorrespondingresultholdsforp(y),whichisobtainedbymarginalizingwithrespecttox.Intuitively,lookingatthejointdistributionin(6.64), we ignore (i.e., integrate out) everything we are not interested in. This is illustratedinFigure6.9(b). (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 200 ProbabilityandDistributions Example 6.6 Figure6.9 (a)Bivariate 8 Gaussian; 6 (b)marginalofa jointGaussian 4 distributionis 2 Gaussian;(c)the conditional 0 distributionofa 2 Gaussianisalso − Gaussian. 4 − 1 0 1 − x1 2x x2= − 1 (a)BivariateGaussian. p(x1) 1.2 p(x1| x2= − 1) 0.6 Mean 1.0 Mean 2σ 2σ 0.8 0.4 0.6 0.4 0.2 0.2 0.0 0.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.5 1.0 0.5 0.0 0.5 1.0 1.5 − − − x1 − − − x1 (b)Marginaldistribution. (c)Conditionaldistribution. ConsiderthebivariateGaussiandistribution(illustratedinFigure6.9): (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) 0 0.3 1 p(x ,x ) = , − . (6.69) 1 2 N 2 1 5 − We can compute the parameters of the univariate Gaussian, conditioned on x = 1, by applying (6.66) and (6.67) to obtain