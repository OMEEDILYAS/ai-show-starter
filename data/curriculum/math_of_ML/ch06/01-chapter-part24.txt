particular distribution (for example, those in Table 6.2). Exponential families provide a convenient way to find conjugate pairs of distributions. Consider the random variable X is a member of the exponentialfamily(6.107): p(x θ) = h(x)exp( θ,φ(x) A(θ)) . (6.119) | (cid:104) (cid:105)− Every member of the exponential family has a conjugate prior (Brown, 1986) (cid:18)(cid:28)(cid:20) (cid:21) (cid:20) (cid:21)(cid:29) (cid:19) γ θ p(θ γ) = h (θ)exp 1 , A (γ) , (6.120) | c γ 2 A(θ) − c − (cid:20) (cid:21) γ where γ = 1 has dimension dim(θ) + 1. The sufficient statistics of γ 2 (cid:20) (cid:21) θ the conjugate prior are . By using the knowledge of the general A(θ) − formofconjugatepriorsforexponentialfamilies,wecanderivefunctional formsofconjugatepriorscorrespondingtoparticulardistributions. Example 6.15 RecalltheexponentialfamilyformoftheBernoullidistribution(6.113d) (cid:20) µ (cid:21) p(x µ) = exp xlog +log(1 µ) . (6.121) | 1 µ − − (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 214 ProbabilityandDistributions Thecanonicalconjugatepriorhastheform µ (cid:20) µ (cid:21) p(µ α,β) = exp αlog +(β+α)log(1 µ) A (γ) , | 1 µ 1 µ − − c − − (6.122) where we defined γ := [α,β + α](cid:62) and h (µ) := µ/(1 µ). Equac − tion(6.122)thensimplifiesto p(µ α,β) = exp[(α 1)logµ+(β 1)log(1 µ) A (α,β)] . c | − − − − (6.123) Puttingthisinnon-exponentialfamilyformyields p(µ α,β) µα−1(1 µ)β−1, (6.124) | ∝ − which we identify as the Beta distribution (6.98). In example 6.12, we assumed that the Beta distribution is the conjugate prior of the Bernoulli distribution and showed that it was indeed the conjugate prior. In this example, we derived the form of the Beta distribution by looking at the canonicalconjugateprioroftheBernoullidistributioninexponentialfamilyform. As mentioned in the previous section, the main motivation for exponential families is that they have finite-dimensional sufficient statistics. Additionally,conjugatedistributionsareeasytowritedown,andtheconjugate distributions also come from an exponential family. From an inference perspective, maximum likelihood estimation behaves nicely because empiricalestimatesofsufficientstatisticsareoptimalestimatesofthepopulation values of sufficient statistics (recall the mean and covariance of a Gaussian). From an optimization perspective, the log-likelihood function is concave, allowing for efficient optimization approaches to be applied (Chapter7). 6.7 Change of Variables/Inverse Transform It may seem that there are very many known distributions, but in reality the set of distributions for which we have names is quite limited. Therefore, it is often useful to understand how transformed random variables are distributed. For example, assuming that X is a random variable dis- (cid:0) (cid:1) tributed according to the univariate normal distribution 0, 1 , what is N the distribution of X2? Another example, which is quite common in machine learning, is, given that X and X are univariate standard normal, 1 2 whatisthedistributionof 1(X +X )? 2 1 2 One option to work out the distribution of 1(X +X ) is to calculate 2 1 2 the meanand varianceof X andX and thencombine them.As we saw 1 2 inSection6.4.4,wecancalculatethemeanandvarianceofresultingrandom variables when we consider affine transformations of random variDraft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.7 ChangeofVariables/InverseTransform 215 ables. However, we may not be able to obtain the functional form of the distribution under transformations. Furthermore, we may be interested in nonlinear transformations of random variables for which closed-form