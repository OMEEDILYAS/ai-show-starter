(6.62)) with different parameters, i.e.,(µ ,σ2) = (µ ,σ2). 1 1 (cid:54) 2 2 Then the mean of the mixture density p(x) is given by the weighted sum ofthemeansofeachrandomvariable: E[x] = αµ +(1 α)µ . (6.81) 1 2 − Thevarianceofthemixturedensityp(x)isgivenby (cid:16) (cid:17) V[x] = (cid:2) ασ2+(1 α)σ2(cid:3) + (cid:2) αµ2+(1 α)µ2(cid:3) [αµ +(1 α)µ ] 2 . 1 − 2 1 − 2 − 1 − 2 (6.82) Proof The mean of the mixture density p(x) is given by the weighted sumofthemeansofeachrandomvariable.Weapplythedefinitionofthe mean(Definition6.4),andpluginourmixture(6.80),whichyields (cid:90) ∞ E[x] = xp(x)dx (6.83a) −∞ (cid:90) ∞ = αxp (x)+(1 α)xp (x)dx (6.83b) 1 2 − −∞ (cid:90) ∞ (cid:90) ∞ = α xp (x)dx+(1 α) xp (x)dx (6.83c) 1 2 − −∞ −∞ = αµ +(1 α)µ . (6.83d) 1 2 − To compute the variance, we can use the raw-score version of the variance from (6.44), which requires an expression of the expectation of the squaredrandomvariable.Hereweusethedefinitionofanexpectationof afunction(thesquare)ofarandomvariable(Definition6.3), (cid:90) ∞ E[x2] = x2p(x)dx (6.84a) −∞ (cid:90) ∞ = αx2p (x)+(1 α)x2p (x)dx (6.84b) 1 2 − −∞ Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.5 GaussianDistribution 203 (cid:90) ∞ (cid:90) ∞ = α x2p (x)dx+(1 α) x2p (x)dx (6.84c) 1 2 − −∞ −∞ = α(µ2+σ2)+(1 α)(µ2+σ2), (6.84d) 1 1 − 2 2 where in the last equality, we again used the raw-score version of the variance (6.44) giving σ2 = E[x2] µ2. This is rearranged such that the − expectationofasquaredrandomvariableisthesumofthesquaredmean andthevariance. Therefore,thevarianceisgivenbysubtracting(6.83d)from(6.84d), V[x] = E[x2] (E[x])2 (6.85a) − = α(µ2+σ2)+(1 α)(µ2+σ2) (αµ +(1 α)µ )2 (6.85b) 1 1 − 2 2 − 1 − 2 = (cid:2) ασ2+(1 α)σ2(cid:3) 1 − 2 (cid:16) (cid:17) + (cid:2) αµ2+(1 α)µ2(cid:3) [αµ +(1 α)µ ] 2 . (6.85c) 1 − 2 − 1 − 2 Remark. The preceding derivation holds for any density, but since the Gaussian is fully determined by the mean and variance, the mixture densitycanbedeterminedinclosedform. ♦ For a mixture density, the individual components can be considered to be conditional distributions (conditioned on the component identity). Equation (6.85c) is an example of the conditional variance formula, also knownasthelawoftotalvariance,whichgenerallystatesthatfortworan- lawoftotalvariance domvariablesX andY itholdsthatV [x] = E [V [x y]]+V [E [x y]], X Y X Y X | | i.e.,the(total)varianceofX istheexpectedconditionalvarianceplusthe varianceofaconditionalmean. We consider in Example 6.17 a bivariate standard Gaussian random variableX andperformedalineartransformationAxonit.Theoutcome isaGaussianrandomvariablewithmeanzeroandcovarianceAA(cid:62).Observe that adding a constant vector will change the mean of the distribution, without affecting its variance, that is, the random variable x+µ is Gaussian with mean µ and identity covariance. Hence, any linear/affine transformationofaGaussianrandomvariableisGaussiandistributed. Anylinear/affine (cid:0) (cid:1) Consider a Gaussian distributed random variable X µ, Σ . For transformationofa a given matrix A of appropriate shape, let Y be a rando ∼ m N variable such Gaussianrandom variableisalso that y = Ax is a transformed version of x. We can compute the mean of Gaussian y byexploitingthattheexpectationisalinearoperator(6.50)asfollows: distributed. E[y] = E[Ax] = AE[x] = Aµ. (6.86) Similarlythevarianceofy canbefoundbyusing(6.51): V[y] = V[Ax] = AV[x]A(cid:62) = AΣA(cid:62). (6.87) Thismeansthattherandomvariabley isdistributedaccordingto p(y) = (cid:0) y Aµ, AΣA(cid:62)(cid:1) . (6.88) N | (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 204 ProbabilityandDistributions Let us now consider the