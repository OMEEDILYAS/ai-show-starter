geometric properties of random variables(Eaton,2007).Ifwedefine X,Y := Cov[x,y] (6.59) (cid:104) (cid:105) forzeromeanrandomvariablesX andY,weobtainaninnerproduct.We Cov[x,x]=0 ⇐⇒ seethatthecovarianceissymmetric,positivedefinite,andlinearineither x=0 argument.Thelengthofarandomvariableis Cov[αx+z,y]= (cid:113) (cid:113) αCov[x,y]+ X = Cov[x,x] = V[x] = σ[x], (6.60) Cov[z,y]forα∈R. (cid:107) (cid:107) i.e., its standard deviation. The “longer” the random variable, the more uncertainitis;andarandomvariablewithlength0isdeterministic. Ifwelookattheangleθ betweentworandomvariablesX,Y,weget X,Y Cov[x,y] cosθ = (cid:104) (cid:105) = , (6.61) X Y (cid:112)V[x]V[y] (cid:107) (cid:107) (cid:107) (cid:107) which is the correlation (Definition 6.8) between the two random variables. This means that we can think of correlation as the cosine of the angle between two random variables when we consider them geometrically. We know from Definition 3.7 that X Y X,Y = 0. In our ⊥ ⇐⇒ (cid:104) (cid:105) case,thismeansthatX andY areorthogonalifandonlyifCov[x,y] = 0, i.e.,theyareuncorrelated.Figure6.6illustratesthisrelationship. Remark. While it is tempting to use the Euclidean distance (constructed Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.5 GaussianDistribution 197 Figure6.7 Gaussian distributionoftwo randomvariablesx1 0.20 andx2. 0.15) 2 x 0.10, 1 x ( 0.05p 0.00 7.5 5.0 2.5 − 1 0 2. 0 5 .0 x2 x 1 1 5−.0 − from the preceding definition of inner products) to compare probability distributions, it is unfortunately not the best way to obtain distances between distributions. Recall that the probability mass (or density) is positive and needs to add up to 1. These constraints mean that distributions live on something called a statistical manifold. The study of this space of probability distributions is called information geometry. Computing distances between distributions are often done using Kullback-Leibler divergence,whichisageneralizationofdistancesthataccountforpropertiesof thestatisticalmanifold.JustliketheEuclideandistanceisaspecialcaseof ametric(Section3.3),theKullback-Leiblerdivergenceisaspecialcaseof two more general classes of divergences called Bregman divergences and f-divergences.Thestudyofdivergencesisbeyondthescopeofthisbook, and we refer for more details to the recent book by Amari (2016), one of thefoundersofthefieldofinformationgeometry. ♦ 6.5 Gaussian Distribution TheGaussiandistributionisthemostwell-studiedprobabilitydistribution forcontinuous-valuedrandomvariables.Itisalsoreferredtoasthenormal normaldistribution distribution.Itsimportanceoriginatesfromthefactthatithasmanycom- TheGaussian putationallyconvenientproperties,whichwewillbediscussinginthefol- distributionarises naturallywhenwe lowing. In particular, we will use it to define the likelihood and prior for considersumsof linear regression (Chapter 9), and consider a mixture of Gaussians for independentand densityestimation(Chapter11). identically There are many other areas of machine learning that also benefit from distributedrandom usingaGaussiandistribution,forexampleGaussianprocesses,variational variables.Thisis knownasthe inference, and reinforcement learning. It is also widely used in other apcentrallimit plicationareassuchassignalprocessing(e.g.,Kalmanfilter),control(e.g., theorem(Grinstead linearquadraticregulator),andstatistics(e.g.,hypothesistesting). andSnell,1997). (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 198 ProbabilityandDistributions Figure6.8 8 Gaussian p(x) 0.20 distributions Mean 6 overlaidwith100 0.15 Sample 4 samples.(a)One- 2σ 2 dimensionalcase; 0.10 (b)two-dimensional 0 0.05 case. 2 − 0.00 4 − 5.0 2.5 0.0 2.5 5.0 7.5 1 0 1 − − x − x1 (a) Univariate (one-dimensional) Gaussian; The red cross shows the mean and the red lineshowstheextentofthevariance. 2x Mean Sample (b) Multivariate (two-dimensional) Gaussian, viewed from top. The red cross shows themeanandthecoloredlinesshowthecontourlinesofthedensity. For a univariate random variable, the Gaussian distribution has a densitythatisgivenby 1 (cid:18) (x µ)2(cid:19) p(x µ,σ2) = exp − . (6.62) | √2πσ2 − 2σ2 multivariate The multivariate Gaussian distribution is fully characterized by a mean Gaussian vectorµandacovariancematrix Σanddefinedas distribution meanvector p(x µ,Σ) = (2π)− D 2 Σ − 1 2 exp (cid:0) 1(x µ)(cid:62)Σ−1(x µ) (cid:1) , (6.63) covariancematrix | | | − 2 − − Alsoknownasa where x RD. We write p(x) = (cid:0) x µ, Σ (cid:1) or X (cid:0) µ, Σ (cid:1)