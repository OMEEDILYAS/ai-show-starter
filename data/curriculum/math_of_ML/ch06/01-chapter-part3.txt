P (S) [0,1] (the sourceof X ⊆ T ∈ misunderstanding probability) to a particular event occurring corresponding to the random asitisneither variable X. Example 6.1 provides a concrete illustration of the terminol- randomnorisita ogy. variable.Itisa function. Remark. The aforementioned sample space Ω unfortunately is referred to by different names in different books. Another common name for Ω is “state space” (Jacod and Protter, 2004), but state space is sometimes reserved for referring to states in a dynamical system (Hasselblatt and (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 176 ProbabilityandDistributions Katok, 2003). Other names sometimes used to describe Ω are: “sample descriptionspace”,“possibilityspace,”and“eventspace”. ♦ Example 6.1 Thistoyexampleis We assume that the reader is already familiar with computing probabilessentiallyabiased ities of intersections and unions of sets of events. A gentler introduction coinflipexample. to probability with many examples can be found in chapter 2 of Walpole etal.(2011). Consider a statistical experiment where we model a funfair game consisting of drawing two coins from a bag (with replacement). There are coins from USA (denoted as $) and UK (denoted as £) in the bag, and since we draw two coins from the bag, there are four outcomes in total. The state space or sample space Ω of this experiment is then ($, $), ($, £),(£,$),(£,£).Letusassumethatthecompositionofthebagofcoinsis suchthatadrawreturnsatrandoma$withprobability0.3. Theeventweareinterestedinisthetotalnumberoftimestherepeated draw returns $. Let us define a random variable X that maps the sample space Ω to , which denotes the number of times we draw $ out of the T bag.Wecanseefromtheprecedingsamplespacewecangetzero$,one$, ortwo$s,andtherefore = 0,1,2 .TherandomvariableX (afunction T { } orlookuptable)canberepresentedasatablelikethefollowing: X(($,$)) = 2 (6.1) X(($,£)) = 1 (6.2) X((£,$)) = 1 (6.3) X((£,£)) = 0. (6.4) Since we return the first coin we draw before drawing the second, this implies that the two draws are independent of each other, which we will discuss in Section 6.4.5. Note that there are two experimental outcomes, which map to the same event, where only one of the draws returns $. Therefore,theprobabilitymassfunction(Section6.2.1)ofX isgivenby P(X = 2) = P(($,$)) = P($) P($) · = 0.3 0.3 = 0.09 (6.5) · P(X = 1) = P(($,£) (£,$)) ∪ = P(($,£))+P((£,$)) = 0.3 (1 0.3)+(1 0.3) 0.3 = 0.42 (6.6) · − − · P(X = 0) = P((£,£)) = P(£) P(£) · = (1 0.3) (1 0.3) = 0.49. (6.7) − · − Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.1 ConstructionofaProbabilitySpace 177 In the calculation, we equated two different concepts, the probability of the output of X and the probability of the samples in Ω. For example, in (6.7) we say P(X = 0) = P((£,£)). Consider the random variable X : Ω and a subset S (for example, a single element of , → T ⊆ T T such as the outcome that one head is obtained when tossing two coins). LetX−1(S)bethepre-imageofS byX,i.e.,thesetofelementsofΩthat map to S under X; ω Ω : X(ω) S . One way to understand the { ∈ ∈ } transformation of probability from events in Ω via the random variable X is to associate it with the probability of the pre-image of S (Jacod and Protter,2004).ForS ,wehavethenotation ⊆ T