can observe. If we observe y, we can use Bayes’ theorem to draw some conclusions about x given the observed values of y. Bayes’ theorem (also Bayes’theorem Bayes’ruleorBayes’law) Bayes’rule Bayes’law likelihood prior (cid:122) (cid:125)(cid:124) (cid:123)(cid:122)(cid:125)(cid:124)(cid:123) p(y x)p(x) p(x y) = | (6.23) | p(y) (cid:124) (cid:123)(cid:122) (cid:125) posterior (cid:124)(cid:123)(cid:122)(cid:125) evidence isadirectconsequenceoftheproductrulein(6.22)since p(x,y) = p(x y)p(y) (6.24) | and p(x,y) = p(y x)p(x) (6.25) | sothat p(y x)p(x) p(x y)p(y) = p(y x)p(x) p(x y) = | . (6.26) | | ⇐⇒ | p(y) In (6.23), p(x) is the prior, which encapsulates our subjective prior prior knowledge of the unobserved (latent) variable x before observing any data. We can choose any prior that makes sense to us, but it is critical to ensure that the prior has a nonzero pdf (or pmf) on all plausible x, even iftheyareveryrare. The likelihood p(y x) describes how x and y are related, and in the likelihood | caseofdiscreteprobabilitydistributions,itistheprobabilityofthedatay Thelikelihoodis ifweweretoknowthelatentvariablex.Notethatthelikelihoodisnota sometimesalso calledthe distribution in x, but only in y. We call p(y x) either the “likelihood of | “measurement x (given y)” or the “probability of y given x” but never the likelihood of model”. y (MacKay,2003). The posterior p(x y) is the quantity of interest in Bayesian statistics posterior | becauseitexpressesexactlywhatweareinterestedin,i.e.,whatweknow aboutxafterhavingobservedy. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 186 ProbabilityandDistributions Thequantity (cid:90) p(y) := p(y x)p(x)dx = E [p(y x)] (6.27) X | | marginallikelihood isthemarginallikelihood/evidence.Theright-handsideof(6.27)usesthe evidence expectation operator which we define in Section 6.4.1. By definition, the marginallikelihoodintegratesthenumeratorof(6.23)withrespecttothe latent variable x. Therefore, the marginal likelihood is independent of x, and it ensures that the posterior p(x y) is normalized. The marginal | likelihood can also be interpreted as the expected likelihood where we taketheexpectationwithrespecttothepriorp(x).Beyondnormalization of the posterior, the marginal likelihood also plays an important role in Bayesian model selection, as we will discuss in Section 8.6. Due to the Bayes’theoremis integrationin(8.44),theevidenceisoftenhardtocompute. alsocalledthe Bayes’ theorem (6.23) allows us to invert the relationship between x “probabilistic and y given by the likelihood. Therefore, Bayes’ theorem is sometimes inverse.” called the probabilistic inverse. We will discuss Bayes’ theorem further in probabilisticinverse Section8.4. Remark. In Bayesian statistics, the posterior distribution is the quantity of interest as it encapsulates all available information from the prior and the data. Instead of carrying the posterior around, it is possible to focus on some statistic of the posterior, such as the maximum of the posterior, which we will discuss in Section 8.3. However, focusing on some statistic of the posterior leads to loss of information. If we think in a bigger context,thentheposteriorcanbeusedwithinadecision-makingsystem,and havingthefullposteriorcanbeextremelyusefulandleadtodecisionsthat arerobusttodisturbances.Forexample,inthecontextofmodel-basedreinforcement learning, Deisenroth et al. (2015) show that using the full posterior distribution of plausible transition functions leads to very fast (data/sample efficient) learning, whereas focusing on the maximum of the posterior leads to consistent failures. Therefore, having the full posterior can be very useful for a downstream task. In Chapter 9, we will continuethisdiscussioninthecontextoflinearregression. ♦ 6.4 Summary Statistics and Independence Weareofteninterestedinsummarizingsetsofrandomvariablesandcomparing pairs of random variables. A statistic of a random variable is a deterministic function of that random variable. The summary