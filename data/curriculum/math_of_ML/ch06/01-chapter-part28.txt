more about how to use probability distributions to model machine learning tasks in Chapter 8. Ironically, the recent surge in interest in neural networks has resulted in a broader appreciation of probabilisticmodels.Forexample,theideaofnormalizingflows(Jimenez RezendeandMohamed,2015)reliesonchangeofvariablesfortransformingrandomvariables.Anoverviewofmethodsforvariationalinferenceas applied to neural networks is described in chapters 16 to 20 of the book byGoodfellowetal.(2016). Wesidesteppedalargepartofthedifficultyincontinuousrandomvariablesbyavoidingmeasuretheoreticquestions(Billingsley,1995;Pollard, 2002),andbyassumingwithoutconstructionthatwehaverealnumbers, andwaysofdefiningsetsonrealnumbersaswellastheirappropriatefrequencyofoccurrence.Thesedetailsdomatter,forexample,inthespecification of conditional probability p(y x) for continuous random variables | x,y (Proschan and Presnell, 1998). The lazy notation hides the fact that we want to specify that X = x (which is a set of measure zero). Furthermore, we are interested in the probability density function of y. A more precise notation would have to say E [f(y) σ(x)], where we take y | theexpectationoveryofatestfunctionf conditionedontheσ-algebraof x. A more technical audience interested in the details of probability theory have many options (Jaynes, 2003; MacKay, 2003; Jacod and Protter, 2004; Grimmett and Welsh, 2014), including some very technical discussions(Shiryayev,1984;LehmannandCasella,1998;Dudley,2002;Bickel andDoksum,2006;C¸inlar,2011).Analternativewaytoapproachprobability is to start with the concept of expectation, and “work backward” to derive the necessary properties of a probability space (Whittle, 2000). As machine learning allows us to model more intricate distributions on ever more complex types of data, a developer of probabilistic machine learning models would have to understand these more technical aspects. Machinelearningtextswithaprobabilisticmodelingfocusincludethebooks byMacKay(2003);Bishop(2006);RasmussenandWilliams(2006);Barber(2012);Murphy(2012). (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 222 ProbabilityandDistributions Exercises 6.1 Considerthefollowingbivariatedistributionp(x,y)oftwodiscreterandom variablesX andY. y 1 0.01 0.02 0.03 0.1 0.1 Y y 2 0.05 0.1 0.05 0.07 0.2 y 3 0.1 0.05 0.03 0.05 0.04 x x x x x 1 2 3 4 5 X Compute: a. Themarginaldistributionsp(x)andp(y). b. Theconditionaldistributionsp(x|Y =y )andp(y|X =x ). 1 3 6.2 ConsideramixtureoftwoGaussiandistributions(illustratedinFigure6.4), (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) 10 1 0 0 8.4 2.0 0.4N , +0.6N , . 2 0 1 0 2.0 1.7 a. Computethemarginaldistributionsforeachdimension. b. Computethemean,modeandmedianforeachmarginaldistribution. c. Computethemeanandmodeforthetwo-dimensionaldistribution. 6.3 You have written a computer program that sometimes compiles and sometimesnot(codedoesnotchange).Youdecidetomodeltheapparentstochasticity(successvs.nosuccess)xofthecompilerusingaBernoullidistribution withparameterµ: p(x|µ)=µx(1−µ)1−x, x∈{0,1}. ChooseaconjugatepriorfortheBernoullilikelihoodandcomputetheposteriordistributionp(µ|x ,...,x ). 1 N 6.4 Therearetwobags.Thefirstbagcontainsfourmangosandtwoapples;the secondbagcontainsfourmangosandfourapples. We also have a biased coin, which shows “heads” with probability 0.6 and “tails” with probability 0.4. If the coin shows “heads”. we pick a fruit at randomfrombag1;otherwisewepickafruitatrandomfrombag2. Yourfriendflipsthecoin(youcannotseetheresult),picksafruitatrandom fromthecorrespondingbag,andpresentsyouamango. Whatistheprobabilitythatthemangowaspickedfrombag2? Hint:UseBayes’theorem. 6.5 Considerthetime-seriesmodel (cid:0) (cid:1) x t+1 =Axt+w, w∼N 0, Q (cid:0) (cid:1) y t =Cxt+v, v∼N 0, R , wherew,varei.i.d.Gaussiannoisevariables.Further,assumethatp(x )= 0 (cid:0) (cid:1) N µ , Σ . 0 0 Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. Exercises 223 a. Whatistheformofp(x ,x ,...,x )?Justifyyouranswer(youdonot 0 1 T havetoexplicitlycomputethejointdistribution). (cid:0) (cid:1) b. Assumethatp(xt|y 1 ,...,y t )=N µ t , Σt . 1. Computep(x |y ,...,y ). t+1 1 t 2. Computep(x ,y |y ,...,y ). t+1 t+1 1 t 3. Attimet+1,weobservethevaluey =yˆ.Computetheconditional t+1 distributionp(x |y ,...,y ). t+1 1 t+1 6.6 Provetherelationshipin(6.44),whichrelatesthestandarddefinitionofthe variancetotheraw-scoreexpressionforthevariance. 6.7 Prove the relationship in (6.45), which relates the pairwise difference betweenexamplesinadatasetwiththeraw-scoreexpressionforthevariance. 6.8 Express the Bernoulli distribution in the natural parameter form of the exponentialfamily,see(6.107). 6.9 ExpresstheBinomialdistributionasanexponentialfamilydistribution.Also expresstheBetadistributionisanexponentialfamilydistribution.Showthat the product of the Beta and the Binomial distribution is also a member of theexponentialfamily. 6.10 DerivetherelationshipinSection6.5.2intwoways: a. Bycompletingthesquare b. ByexpressingtheGaussianinitsexponentialfamilyform (cid:0) (cid:1) (cid:0) (cid:1) The product of two Gaussians N x|a, A N x|b, B is an