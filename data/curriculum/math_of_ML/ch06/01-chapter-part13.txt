(sometimescalled corrected) covariancehasthe 6.4.3 Three Expressions for the Variance factorN−1inthe We now focus on a single random variable X and use the preceding emdenominator insteadofN. pirical formulas toderive three possible expressions forthe variance. The Thederivationsare following derivation is the same for the population variance, except that exercisesattheend weneedtotakecareofintegrals.Thestandarddefinitionofvariance,corofthischapter. responding to the definition of covariance (Definition 6.5), is the expectation of the squared deviation of a random variable X from its expected valueµ,i.e., V [x] := E [(x µ)2]. (6.43) X X − The expectation in (6.43) and the mean µ = E (x) are computed usX ing (6.32), depending on whether X is a discrete or continuous random variable.Thevarianceasexpressedin(6.43)isthemeanofanewrandom variableZ := (X µ)2. − When estimating the variance in (6.43) empirically, we need to resort to a two-pass algorithm: one pass through the data to calculate the mean µusing(6.41),andthenasecondpassusingthisestimateµˆ calculatethe Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.4 SummaryStatisticsandIndependence 193 variance. It turns out that we can avoid two passes by rearranging the terms. The formula in (6.43) can be converted to the so-called raw-score raw-scoreformula formulaforvariance: forvariance V [x] = E [x2] (E [x]) 2 . (6.44) X X X − The expression in (6.44) can be remembered as “the mean of the square minusthesquareofthemean”.Itcanbecalculatedempiricallyinonepass through data since we can accumulate x (to calculate the mean) and x2 i i simultaneously, where x i is the ith observation. Unfortunately, if imple- Ifthetwoterms mentedinthisway,itcanbenumericallyunstable.Theraw-scoreversion in(6.44)arehuge andapproximately ofthevariancecanbeusefulinmachinelearning,e.g.,whenderivingthe equal,wemay bias–variancedecomposition(Bishop,2006). sufferfroman Athirdwaytounderstandthevarianceisthatitisasumofpairwisedif- unnecessarylossof ferences between all pairs of observations. Consider a sample x ,...,x numericalprecision 1 N ofrealizationsofrandomvariableX,andwecomputethesquareddiffer- infloating-point arithmetic. ence between pairs of x and x . By expanding the square, we can show i j that the sum of N2 pairwise differences is the empirical variance of the observations:   (cid:32) (cid:33)2 N N N 1 (cid:88) 1 (cid:88) 1 (cid:88) N2 (x i − x j )2 = 2 N x2 i − N x i  . (6.45) i,j=1 i=1 i=1 We see that (6.45) is twice the raw-score expression (6.44). This means thatwecanexpressthesumofpairwisedistances(ofwhichthereareN2 ofthem)asasumofdeviationsfromthemean(ofwhichthereareN).Geometrically, this means that there is an equivalence between the pairwise distances and the distances from the center of the set of points. From a computational perspective, this means that by computing the mean (N terms in the summation), and then computing the variance (again N terms in the summation), we can obtain an expression (left-hand side of(6.45))thathasN2 terms. 6.4.4 Sums and Transformations of Random Variables We may want to model a phenomenon that cannot be well explained by textbook distributions (we introduce some in Sections 6.5 and 6.6), and hence may perform simple manipulations of random variables (such as addingtworandomvariables). ConsidertworandomvariablesX,Y withstatesx,y RD.Then: ∈ E[x+y] = E[x]+E[y] (6.46) E[x y] = E[x] E[y] (6.47) − − V[x+y] = V[x]+V[y]+Cov[x,y]+Cov[y,x] (6.48) V[x y] = V[x]+V[y] Cov[x,y] Cov[y,x]. (6.49) − − − (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 194 ProbabilityandDistributions Mean and (co)variance exhibit some useful properties when it comes toaffinetransformationofrandomvariables.Considerarandomvariable X with mean µ and covariance matrix Σ and a (deterministic) affine transformation y = Ax + b of x. Then y is