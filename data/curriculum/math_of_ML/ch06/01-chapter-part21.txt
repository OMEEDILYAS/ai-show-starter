a particular reason, but may have other applications. Knowing the reason behind the creation of a particular distribution often allows insight into howtobestuseit.Weintroducedprecedingthreedistributionstobeable (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 208 ProbabilityandDistributions to illustrate the concepts of conjugacy (Section 6.6.1) and exponential families(Section6.6.3). ♦ 6.6.1 Conjugacy According to Bayes’ theorem (6.23), the posterior is proportional to the product of the prior and the likelihood. The specification of the prior can be tricky for two reasons: First, the prior should encapsulate our knowledge about the problem before we see any data. This is often difficult to describe.Second,itisoftennotpossibletocomputetheposteriordistributionanalytically.However,therearesomepriorsthatarecomputationally conjugateprior convenient:conjugatepriors. conjugate Definition 6.13 (Conjugate Prior). A prior is conjugate for the likelihood functioniftheposteriorisofthesameform/typeastheprior. Conjugacy is particularly convenient because we can algebraically calculate our posterior distribution by updating the parameters of the prior distribution. Remark. Whenconsideringthegeometryofprobabilitydistributions,conjugatepriorsretainthesamedistancestructureasthelikelihood(Agarwal andDaum´eIII,2010). ♦ Tointroduceaconcreteexampleofconjugatepriors,wedescribeinExample 6.11 the Binomial distribution (defined on discrete random variables) and the Beta distribution (defined on continuous random variables). Example 6.11 (Beta-Binomial Conjugacy) ConsideraBinomialrandomvariablex Bin(N,µ)where ∼ (cid:32) (cid:33) N p(x N,µ) = µx(1 µ)N−x, x = 0,1,...,N , (6.102) | x − is the probability of finding x times the outcome “heads” in N coin flips, where µ is the probability of a “head”. We place a Beta prior on the parameterµ,thatis,µ Beta(α,β),where ∼ Γ(α+β) p(µ α,β) = µα−1(1 µ)β−1. (6.103) | Γ(α)Γ(β) − Ifwenowobservesomeoutcomex = h,thatis,weseehheadsinN coin flips,wecomputetheposteriordistributiononµas p(µ x = h,N,α,β) p(x N,µ)p(µ α,β) (6.104a) | ∝ | | µh(1 µ)(N−h)µα−1(1 µ)β−1 (6.104b) ∝ − − = µh+α−1(1 µ)(N−h)+β−1 (6.104c) − Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.6 ConjugacyandtheExponentialFamily 209 Likelihood Conjugateprior Posterior Table6.2 Examples Bernoulli Beta Beta ofconjugatepriors Binomial Beta Beta forcommon Gaussian Gaussian/inverseGamma Gaussian/inverseGamma likelihoodfunctions. Gaussian Gaussian/inverseWishart Gaussian/inverseWishart Multinomial Dirichlet Dirichlet Beta(h+α,N h+β), (6.104d) ∝ − i.e., the posterior distribution is a Beta distribution as the prior, i.e., the Beta prior is conjugate for the parameter µ in the Binomial likelihood function. In the following example, we will derive a result that is similar to the Beta-Binomialconjugacyresult.HerewewillshowthattheBetadistributionisaconjugatepriorfortheBernoullidistribution. Example 6.12 (Beta-Bernoulli Conjugacy) Let x 0,1 be distributed according to the Bernoulli distribution with ∈ { } parameter θ [0,1], that is, p(x = 1 θ) = θ. This can also be expressed ∈ | as p(x θ) = θx(1 θ)1−x. Let θ be distributed according to a Beta distri- | − butionwithparametersα,β,thatis,p(θ α,β) θα−1(1 θ)β−1. | ∝ − MultiplyingtheBetaandtheBernoullidistributions,weget p(θ x,α,β) = p(x θ)p(θ α,β) (6.105a) | | | θx(1 θ)1−xθα−1(1 θ)β−1 (6.105b) ∝ − − = θα+x−1(1 θ)β+(1−x)−1 (6.105c) − p(θ α+x,β+(1 x)). (6.105d) ∝ | − ThelastlineistheBetadistributionwithparameters(α+x,β+(1 x)). − Table6.2listsexamplesforconjugatepriorsfortheparametersofsome standard likelihoods used in probabilistic modeling. Distributions such as TheGammaprioris Multinomial,inverseGamma,inverseWishart,andDirichletcanbefound conjugateforthe precision(inverse inanystatisticaltext,andaredescribedinBishop(2006),forexample. variance)inthe TheBetadistributionistheconjugatepriorfortheparameterµinboth univariateGaussian theBinomialandtheBernoullilikelihood.ForaGaussianlikelihoodfunc- likelihood,andthe tion, we can place a conjugate Gaussian prior on the mean. The reason Wishartprioris why the Gaussian likelihood appears twice in the table is that we need conjugateforthe precisionmatrix to distinguish the univariate from the multivariate case. In the univariate (inversecovariance (scalar) case, the inverse Gamma is the conjugate prior for the variance. matrix)inthe In the multivariate case, we use a conjugate inverse Wishart distribution multivariate asaprioronthecovariancematrix.TheDirichletdistributionistheconju- Gaussianlikelihood. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 210 ProbabilityandDistributions gate