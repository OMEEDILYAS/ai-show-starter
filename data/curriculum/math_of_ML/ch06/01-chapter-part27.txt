arises because our differentials (cubes of volume) are transformed into parallelepipeds by the Jacobian. Let us summarize preceding the discussion in thefollowing theorem,whichgives usa recipeformultivariate changeof variables. Theorem6.16. [Theorem17.2inBillingsley(1995)]Letf(x)bethevalue oftheprobabilitydensityofthemultivariatecontinuousrandomvariableX. If the vector-valued function y = U(x) is differentiable and invertible for all values within the domain of x, then for corresponding values of y, the probabilitydensityofY = U(X)isgivenby (cid:12) (cid:12) (cid:18) ∂ (cid:19)(cid:12) (cid:12) f(y) = f x (U−1(y)) · (cid:12) (cid:12) det ∂y U−1(y) (cid:12) (cid:12) . (6.144) Thetheoremlooksintimidatingatfirstglance,butthekeypointisthat a change of variable of a multivariate random variable follows the procedure of the univariate change of variable. First we need to work out the inverse transform, and substitute that into the density of x. Then we calculate the determinant of the Jacobian and multiply the result. The followingexampleillustratesthecaseofabivariaterandomvariable. Example 6.17 (cid:20) (cid:21) x ConsiderabivariaterandomvariableX withstatesx = 1 andprobax 2 bilitydensityfunction (cid:32) (cid:33) (cid:18)(cid:20) (cid:21)(cid:19) (cid:20) (cid:21)(cid:62)(cid:20) (cid:21) x 1 1 x x f 1 = exp 1 1 . (6.145) x 2 2π −2 x 2 x 2 Weusethechange-of-variabletechniquefromTheorem6.16toderivethe (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 220 ProbabilityandDistributions effect of a linear transformation (Section 2.7) of the random variable. ConsideramatrixA R2×2 definedas ∈ (cid:20) (cid:21) a b A = . (6.146) c d We are interested in finding the probability density function of the transformedbivariaterandomvariableY withstatesy = Ax. Recallthatforchangeofvariableswerequiretheinversetransformation of x as a function of y. Since we consider linear transformations, the inverse transformation is given by the matrix inverse (see Section 2.2.2). For2 2matrices,wecanexplicitlywriteouttheformula,givenby × (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21) x y 1 d b y 1 = A−1 1 = − 1 . (6.147) x y ad bc c a y 2 2 2 − − Observe that ad bc is the determinant (Section 4.1) of A. The corre- − spondingprobabilitydensityfunctionisgivenby 1 (cid:16) (cid:17) f(x) = f(A−1y) = exp 1y(cid:62)A−(cid:62)A−1y . (6.148) 2π −2 Thepartialderivativeofamatrixtimesavectorwithrespecttothevector isthematrixitself(Section5.5),andtherefore ∂ A−1y = A−1. (6.149) ∂y Recall from Section 4.1 that the determinant of the inverse is the inverse ofthedeterminantsothatthedeterminantoftheJacobianmatrixis (cid:18) ∂ (cid:19) 1 det A−1y = . (6.150) ∂y ad bc − We are now able to apply the change-of-variable formula from Theorem6.16bymultiplying(6.148)with(6.150),whichyields f(y) = f(x) (cid:12) (cid:12) (cid:12)det (cid:18) ∂ A−1y (cid:19)(cid:12) (cid:12) (cid:12) (6.151a) (cid:12) ∂y (cid:12) 1 (cid:16) (cid:17) = exp 1y(cid:62)A−(cid:62)A−1y ad bc −1. (6.151b) 2π −2 | − | While Example 6.17 is based on a bivariate random variable, which allowsustoeasilycomputethematrixinverse,theprecedingrelationholds forhigherdimensions. Remark. WesawinSection6.5thatthedensityf(x)in(6.148)isactually thestandardGaussiandistribution,andthetransformeddensityf(y)isa bivariateGaussianwithcovarianceΣ = AA(cid:62). ♦ We will use the ideas in this chapter to describe probabilistic modeling Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.8 FurtherReading 221 inSection8.4,aswellasintroduceagraphicallanguageinSection8.5.We will see direct machine learning applications of these ideas in Chapters 9 and11. 6.8 Further Reading This chapter is rather terse at times. Grinstead and Snell (1997) and Walpole et al. (2011) provide more relaxed presentations that are suitable for self-study. Readers interested in more philosophical aspects of probability should consider Hacking (2001), whereas an approach that is more related to software engineering is presented by Downey (2014). An overview of exponential families can be found in Barndorff-Nielsen (2014). We will see