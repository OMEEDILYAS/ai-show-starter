itself a random variable whosemeanvectorandcovariancematrixaregivenby E [y] = E [Ax+b] = AE [x]+b = Aµ+b, (6.50) Y X X V [y] = V [Ax+b] = V [Ax] = AV [x]A(cid:62) = AΣA(cid:62), (6.51) Y X X X Thiscanbeshown respectively.Furthermore, directlybyusingthe Cov[x,y] = E[x(Ax+b)(cid:62)] E[x]E[Ax+b](cid:62) (6.52a) definitionofthe − meanand = E[x]b(cid:62)+E[xx(cid:62)]A(cid:62) µb(cid:62) µµ(cid:62)A(cid:62) (6.52b) covariance. − − = µb(cid:62) µb(cid:62)+ (cid:0)E[xx(cid:62)] µµ(cid:62)(cid:1) A(cid:62) (6.52c) − − (6. = 38b) ΣA(cid:62), (6.52d) whereΣ = E[xx(cid:62)] µµ(cid:62) isthecovarianceofX. − 6.4.5 Statistical Independence statistical Definition 6.10 (Independence). Two random variables X,Y are statisindependence ticallyindependentifandonlyif p(x,y) = p(x)p(y). (6.53) Intuitively,tworandomvariablesX andY areindependentifthevalue ofy (onceknown)doesnotaddanyadditionalinformationaboutx(and viceversa).IfX,Y are(statistically)independent,then p(y x) = p(y) | p(x y) = p(x) V | [x+y] = V [x]+V [y] X,Y X Y Cov [x,y] = 0 X,Y The last point may not hold in converse, i.e., two random variables can havecovariancezerobutarenotstatisticallyindependent.Tounderstand why, recall that covariance measures only linear dependence. Therefore, random variables that are nonlinearly dependent could have covariance zero. Example 6.5 Consider a random variable X with zero mean (E [x] = 0) and also X E [x3] = 0. Let y = x2 (hence, Y is dependent on X) and consider the X covariance(6.36)betweenX andY.Butthisgives Cov[x,y] = E[xy] E[x]E[y] = E[x3] = 0. (6.54) − Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.4 SummaryStatisticsandIndependence 195 In machine learning, we often consider problems that can be modeled as independent and identically distributed (i.i.d.) random variables, independentand X ,...,X . For more than two random variables, the word “indepen- identically 1 N distributed dent” (Definition 6.10) usually refers to mutually independent random i.i.d. variables, where all subsets are independent (see Pollard (2002, chapter 4) and Jacod and Protter (2004, chapter 3)). The phrase “identically distributed”meansthatalltherandomvariablesarefromthesamedistribution. Another concept that is important in machine learning is conditional independence. Definition 6.11 (Conditional Independence). Two random variables X andY areconditionallyindependentgivenZ ifandonlyif conditionally independent p(x,y z) = p(x z)p(y z) forall z , (6.55) | | | ∈ Z where isthesetofstatesofrandomvariableZ.WewriteX Y Z to Z ⊥⊥ | denotethatX isconditionallyindependentofY givenZ. Definition 6.11 requires that the relation in (6.55) must hold true for everyvalueofz.Theinterpretationof(6.55)canbeunderstoodas“given knowledgeaboutz,thedistributionofxandy factorizes”.Independence canbecastasaspecialcaseofconditionalindependenceifwewriteX ⊥⊥ Y . By using the product rule of probability (6.22), we can expand the |∅ left-handsideof(6.55)toobtain p(x,y z) = p(x y,z)p(y z). (6.56) | | | Bycomparingtheright-handsideof(6.55)with(6.56),weseethatp(y z) | appearsinbothofthemsothat p(x y,z) = p(x z). (6.57) | | Equation(6.57)providesanalternativedefinitionofconditionalindependence, i.e., X Y Z. This alternative presentation provides the inter- ⊥⊥ | pretation“giventhatweknowz,knowledgeabouty doesnotchangeour knowledgeofx”. 6.4.6 Inner Products of Random Variables RecallthedefinitionofinnerproductsfromSection3.2.Wecandefinean Innerproducts innerproductbetweenrandomvariables,whichwebrieflydescribeinthis between section.IfwehavetwouncorrelatedrandomvariablesX,Y,then multivariaterandom variablescanbe V[x+y] = V[x]+V[y]. (6.58) treatedinasimilar fashion Since variances are measured in squared units, this looks very much like thePythagoreantheoremforrighttrianglesc2 = a2+b2. In the following, we see whether we can find a geometric interpretation of the variance relation of uncorrelated random variables in (6.58). (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 196 ProbabilityandDistributions Figure6.6 Geometryof randomvariables.If randomvariablesX andY are uncorrelated,they areorthogonal vectorsina corresponding vectorspace,and +var[y] thePythagorean (cid:112)var[x] theoremapplies. +y] = c a (cid:112) var[x] (cid:112)var[x b (cid:112) var[y] Random variables can be considered vectors in a vector space, and we can define inner products to obtain