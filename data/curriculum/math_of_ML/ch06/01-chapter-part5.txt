probability is the Cartesian product of the target spaces of each of jointprobability the random variables. We define the joint probability as the entry of both valuesjointly n P(X = x ,Y = y ) = ij , (6.9) i j N where n is the number of events with state x and y and N the total ij i j number of events. The joint probability is the probability of the intersection of both events, that is, P(X = x ,Y = y ) = P(X = x Y = y ). i j i j ∩ probabilitymass Figure6.2illustratestheprobabilitymassfunction(pmf)ofadiscreteprobfunction ability distribution. For two random variables X and Y, the probability Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.2 DiscreteandContinuousProbabilities 179 c Figure6.2 i (cid:122)(cid:125)(cid:124)(cid:123) Visualizationofa y 1 discretebivariate (cid:111) probabilitymass Y y 2 n ij r j function,with randomvariablesX y 3 andY.This x x x x x diagramisadapted 1 2 3 4 5 fromBishop(2006). X that X = x and Y = y is (lazily) written as p(x,y) and is called the joint probability. One can think of a probability as a function that takes state x and y and returns a real number, which is the reason we write p(x,y). ThemarginalprobabilitythatX takesthevaluexirrespectiveofthevalue marginalprobability of random variable Y is (lazily) written as p(x). We write X p(x) to ∼ denotethattherandomvariableX isdistributedaccordingtop(x).Ifwe consider only the instances where X = x, then the fraction of instances (theconditionalprobability)forwhichY = yiswritten(lazily)asp(y x). conditional | probability Example 6.2 ConsidertworandomvariablesX andY,whereX hasfivepossiblestates andY hasthreepossiblestates,asshowninFigure6.2.Wedenotebyn ij the number of events with state X = x and Y = y , and denote by i j N the total number of events. The value c is the sum of the individual i frequenciesfortheithcolumn,thatis,c = (cid:80)3 n .Similarly,thevalue i j=1 ij r is the row sum, that is, r = (cid:80)5 n . Using these definitions, we can j j i=1 ij compactlyexpressthedistributionofX andY. The probability distribution of each random variable, the marginal probability,canbeseenasthesumoveraroworcolumn c (cid:80)3 n P(X = x ) = i = j=1 ij (6.10) i N N and r (cid:80)5 n P(Y = y ) = j = i=1 ij, (6.11) j N N where c and r are the ith column and jth row of the probability table, i j respectively. By convention, for discrete random variables with a finite numberofevents,weassumethatprobabiltiessumuptoone,thatis, 5 3 (cid:88) (cid:88) P(X = x ) = 1 and P(Y = y ) = 1. (6.12) i j i=1 j=1 The conditional probability is the fraction of a row or column in a par- (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 180 ProbabilityandDistributions ticularcell.Forexample,theconditionalprobabilityofY givenX is n P(Y = y X = x ) = ij , (6.13) j | i c i andtheconditionalprobabilityofX givenY is n P(X = x Y = y ) = ij . (6.14) i | j r j In machine learning, we use discrete probability distributions to model categoricalvariable categorical variables, i.e., variables that take a finite set of unordered values. They could be categorical features, such as the degree taken at university when used for predicting the