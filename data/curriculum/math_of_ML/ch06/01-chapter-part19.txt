reverse transformation: when we know that a random variable has a mean that is a linear transformation of another randomvariable.ForagivenfullrankmatrixA RM×N,whereM (cid:62) N, lety RM beaGaussianrandomvariablewith ∈ meanAx,i.e., ∈ (cid:0) (cid:1) p(y) = y Ax, Σ . (6.89) N | What is the corresponding probability distribution p(x)? If A is invertible, then we can write x = A−1y and apply the transformation in the previous paragraph. However, in general A is not invertible, and we use an approach similar to that of the pseudo-inverse (3.57). That is, we premultiply both sides with A(cid:62) and then invert A(cid:62)A, which is symmetric andpositivedefinite,givingustherelation y = Ax (A(cid:62)A)−1A(cid:62)y = x. (6.90) ⇐⇒ Hence,xisalineartransformationofy,andweobtain p(x) = (cid:0) x (A(cid:62)A)−1A(cid:62)y, (A(cid:62)A)−1A(cid:62)ΣA(A(cid:62)A)−1(cid:1) . (6.91) N | 6.5.4 Sampling from Multivariate Gaussian Distributions Wewillnotexplainthesubtletiesofrandomsamplingonacomputer,and the interested reader is referred to Gentle (2004). In the case of a multivariate Gaussian, this process consists of three stages: first, we need a source of pseudo-random numbers that provide a uniform sample in the interval [0,1]; second, we use a non-linear transformation such as the Box-Mu¨llertransform(Devroye,1986)toobtainasamplefromaunivariate Gaussian; and third, we collate a vector of these samples to obtain a (cid:0) (cid:1) samplefromamultivariatestandardnormal 0, I . N For a general multivariate Gaussian, that is, where the mean is non zero and the covariance is not the identity matrix, we use the properties of linear transformations of a Gaussian random variable. Assume we are interested in generating samples x ,i = 1,...,n, from a multivariate i Tocomputethe Gaussian distribution with mean µ and covariance matrix Σ. We would Cholesky like to construct the sample from a sampler that provides samples from factorizationofa themultivariatestandardnormal (cid:0) 0, I (cid:1) . matrix,itisrequired N (cid:0) (cid:1) To obtain samples from a multivariate normal µ, Σ , we can use thatthematrixis N symmetricand the properties of a linear transformation of a Gaussian random variable: positivedefinite If x (cid:0) 0, I (cid:1) , then y = Ax+µ, where AA(cid:62) = Σ is Gaussian dis- (Section3.2.3). ∼ N tributedwithmeanµandcovariancematrixΣ.Oneconvenientchoiceof Covariancematrices A is to use the Cholesky decomposition (Section 4.3) of the covariance possessthis property. matrix Σ = AA(cid:62). The Cholesky decomposition has the benefit that A is triangular,leadingtoefficientcomputation. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 6.6 ConjugacyandtheExponentialFamily 205 6.6 Conjugacy and the Exponential Family Many of the probability distributions “with names” that we find in statistics textbooks were discovered to model particular types of phenomena. For example, we have seen the Gaussian distribution in Section 6.5. The distributions are also related to each other in complex ways (Leemis and McQueston,2008).Forabeginnerinthefield,itcanbeoverwhelmingto figure out which distribution to use. In addition, many of these distributionswerediscoveredatatimethatstatisticsandcomputationweredone “Computers”usedto by pencil and paper. It is natural to ask what are meaningful concepts beajobdescription. in the computing age (Efron and Hastie, 2016). In the previous section, we saw that many of the operations required for inference can be conveniently calculated when the distribution is Gaussian. It is worth recalling at this point the desiderata for manipulating probability distributions in themachinelearningcontext: 1. Thereissome“closureproperty”whenapplyingtherulesofprobability, e.g., Bayes’ theorem. By closure, we mean that applying a particular