the coordinate transformation is linear (as in our case), determinantgives and (5.66) recovers exactly the basis change matrix in (5.62). If the cothemagnification/ ordinatetransformationisnonlinear,theJacobianapproximatesthisnonscalingfactorwhen wetransforman linear transformation locally with a linear one. The absolute value of the areaorvolume. Jacobiandeterminant det(J) isthefactorbywhichareasorvolumesare | | Jacobian scaledwhencoordinatesaretransformed.Ourcaseyields det(J) = 3. determinant | | The Jacobian determinant and variable transformations will become relevant in Section 6.7 when we transform random variables and probFigure5.6 ability distributions. These transformations are extremely relevant in maDimensionalityof chine learning in the context of training deep neural networks using the (partial)derivatives. reparametrizationtrick,alsocalledinfiniteperturbationanalysis. x Inthischapter,weencounteredderivativesoffunctions.Figure5.6sumf(x) marizes the dimensions of those derivatives. If f : R R the gradient is simply a scalar (top-left entry). For f : RD R the → gradient is a 1 D ∂f ∂x row vector (top-right entry). For f : R R→ E, the gradient is an E × 1 columnvector,andforf : RD RE the → gradientisanE D matrix × . → × Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 5.3 GradientsofVector-ValuedFunctions 153 Example 5.9 (Gradient of a Vector-Valued Function) Wearegiven f(x) = Ax, f(x) RM, A RM×N, x RN . ∈ ∈ ∈ To compute the gradient df/dx we first determine the dimension of df/dx: Since f : RN RM, it follows that df/dx RM×N. Second, → ∈ to compute the gradient we determine the partial derivatives of f with respecttoeveryx : j f (x) = (cid:88) N A x = ∂f i = A (5.67) i ij j ⇒ ∂x ij j=1 j WecollectthepartialderivativesintheJacobianandobtainthegradient ∂f1 ∂f1   A A  d d f x =   ∂x . . . 1 ··· ∂x . . . N   =   . . . 11 ··· 1 . . . N   = A ∈ RM×N . (5.68) ∂fM ∂fM A A ∂x1 ··· ∂xN M1 ··· MN Example 5.10 (Chain Rule) Considerthefunctionh : R R,h(t) = (f g)(t)with → ◦ f : R2 R (5.69) → g : R R2 (5.70) → f(x) = exp(x x2), (5.71) 1 2 (cid:20) (cid:21) (cid:20) (cid:21) x tcost x = 1 = g(t) = (5.72) x tsint 2 and compute the gradient of h with respect to t. Since f : R2 R and g : R R2 wenotethat → → ∂f ∂g R1×2, R2×1. (5.73) ∂x ∈ ∂t ∈ Thedesiredgradientiscomputedbyapplyingthechainrule: ∂x  dh ∂f ∂x (cid:20) ∂f ∂f (cid:21) 1 = =  ∂t  (5.74a) dt ∂x ∂t ∂x 1 ∂x 2 ∂x 2  ∂t (cid:20) (cid:21) (cid:2) (cid:3) cost tsint = exp(x 1 x2 2 )x2 2 2exp(x 1 x2 2 )x 1 x 2 sint+ − tcost (5.74b) = exp(x x2) (cid:0) x2(cost tsint)+2x x (sint+tcost) (cid:1) , (5.74c) 1 2 2 − 1 2 wherex = tcostandx = tsint;see(5.72). 1 2 (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 154 VectorCalculus Example 5.11 (Gradient of a Least-Squares Loss in a Linear Model) Wewilldiscussthis Letusconsiderthelinearmodel modelinmuch moredetailin y = Φθ, (5.75) Chapter9inthe contextoflinear where θ RD is a parameter vector, Φ RN×D are input features and regression,where y RN