5.1 applies to functions f of a scalar variable x R. In the following, we consider the general case where the function ∈ f depends on one or more variables x Rn, e.g., ∈ f(x) = f(x ,x ).Thegeneralizationofthederivativetofunctionsofsev1 2 eralvariablesisthegradient. We find the gradient of the function f with respect to x by varying one variable at a time and keeping the others constant. The gradient is then thecollectionofthesepartialderivatives. Definition 5.5 (Partial Derivative). For a function f : Rn R, x partialderivative f(x), x Rn ofnvariablesx 1 ,...,x n wedefinethepartiald → erivatives (cid:55)→ as ∈ ∂f f(x +h,x ,...,x ) f(x) 1 2 n = lim − ∂x 1 h→0 h . . . (5.39) ∂f f(x ,...,x ,x +h) f(x) 1 n−1 n = lim − ∂x n h→0 h andcollectthemintherowvector df (cid:20) ∂f(x) ∂f(x) ∂f(x) (cid:21) f = gradf = = R1×n, (5.40) x ∇ dx ∂x ∂x ··· ∂x ∈ 1 2 n where n is the number of variables and 1 is the dimension of the image/ range/codomainoff.Here,wedefinedthecolumnvectorx = [x ,...,x ](cid:62) 1 n gradient Rn. The row vector in (5.40) is called the gradient of f or the Jacobian ∈ Jacobian andisthegeneralizationofthederivativefromSection5.1. Remark. This definition of the Jacobian is a special case of the general definition of the Jacobian for vector-valued functions as the collection of partialderivatives.WewillgetbacktothisinSection5.3. Wecanuseresults ♦ fromscalar differentiation:Each Example 5.6 (Partial Derivatives Using the Chain Rule) partialderivativeis Forf(x,y) = (x+2y3)2,weobtainthepartialderivatives aderivativewith respecttoascalar. ∂f(x,y) ∂ = 2(x+2y3) (x+2y3) = 2(x+2y3), (5.41) ∂x ∂x Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 5.2 PartialDifferentiationandGradients 147 ∂f(x,y) ∂ = 2(x+2y3) (x+2y3) = 12(x+2y3)y2. (5.42) ∂y ∂y whereweusedthechainrule(5.32)tocomputethepartialderivatives. Remark (Gradient as a Row Vector). It is not uncommon in the literature to define the gradient vector as a column vector, following the conventionthatvectorsaregenerallycolumnvectors.Thereasonwhywedefine the gradient vector as a row vector is twofold: First, we can consistently generalize the gradient to vector-valued functions f : Rn Rm (then → the gradient becomes a matrix). Second, we can immediately apply the multi-variate chain rule without paying attention to the dimension of the gradient.WewilldiscussbothpointsinSection5.3. ♦ Example 5.7 (Gradient) For f(x ,x ) = x2x +x x3 R, the partial derivatives (i.e., the deriva1 2 1 2 1 2 ∈ tivesoff withrespecttox andx )are 1 2 ∂f(x ,x ) 1 2 = 2x x +x3 (5.43) ∂x 1 2 2 1 ∂f(x ,x ) 1 2 = x2+3x x2 (5.44) ∂x 1 1 2 2 andthegradientisthen df = (cid:20) ∂f(x 1 ,x 2 ) ∂f(x 1 ,x 2 ) (cid:21) = (cid:2) 2x x +x3 x2+3x x2 (cid:3) R1×2. dx ∂x ∂x 1 2 2 1 1 2 ∈ 1 2 (5.45) 5.2.1 Basic Rules of Partial Differentiation Productrule: Inthemultivariatecase,wherex Rn,thebasicdifferentiationrulesthat (fg)(cid:48)=f(cid:48)g+fg(cid:48), ∈ we know from school (e.g., sum rule, product rule, chain rule; see also Sumrule: (f+g)(cid:48)=f(cid:48)+g(cid:48), Section5.1.2)stillapply.However,whenwecomputederivativeswithreChainrule: spect to vectors x Rn we need to pay attention: Our gradients now (g(f))(cid:48)=g(cid:48)(f)f(cid:48) ∈ involve vectors and matrices, and matrix multiplication is not commutative(Section2.2.1),i.e.,theordermatters. Herearethegeneralproductrule,sumrule,andchainrule: ∂ ∂f ∂g (cid:0) (cid:1) Productrule: f(x)g(x) = g(x)+f(x) (5.46) ∂x