a ∈ rethecorrespondingobservatio ∈ ns.Wedefinethefunctions weneedderivatives ∈ oftheleast-squares L(e) := e 2, (5.76) lossLwithrespect (cid:107) (cid:107) totheparametersθ. e(θ) := y Φθ. (5.77) − We seek ∂L, and we will use the chain rule for this purpose. L is called a ∂θ least-squaresloss least-squareslossfunction. Beforewestartourcalculation,wedeterminethedimensionalityofthe gradientas ∂L R1×D. (5.78) ∂θ ∈ Thechainruleallowsustocomputethegradientas ∂L ∂L∂e = , (5.79) ∂θ ∂e ∂θ dLdtheta = wherethedthelementisgivenby np.einsum( ’n,nd’, ∂L (cid:88) N ∂L ∂e dLde,dedtheta) [1,d] = [n] [n,d]. (5.80) ∂θ ∂e ∂θ n=1 Weknowthat e 2 = e(cid:62)e(seeSection3.2)anddetermine (cid:107) (cid:107) ∂L = 2e(cid:62) R1×N . (5.81) ∂e ∈ Furthermore,weobtain ∂e = Φ RN×D, (5.82) ∂θ − ∈ suchthatourdesiredderivativeis ∂L = 2e(cid:62)Φ (5 = .77) 2(y(cid:62) θ(cid:62)Φ(cid:62)) Φ R1×D. (5.83) ∂θ − − (cid:124) − (cid:123)(cid:122) (cid:125)(cid:124)(cid:123)(cid:122)(cid:125) ∈ 1×N N×D Remark. Wewouldhaveobtainedthesameresultwithoutusingthechain rulebyimmediatelylookingatthefunction L (θ) := y Φθ 2 = (y Φθ)(cid:62)(y Φθ). (5.84) 2 (cid:107) − (cid:107) − − This approach is still practical for simple functions like L but becomes 2 impracticalfordeepfunctioncompositions. ♦ Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 5.4 GradientsofMatrices 155 A R4×2 x R3 Figure5.7 ∈ ∈ x Visualizationof 1 x gradient 2 x computationofa 3 matrixwithrespect toavector.Weare interestedin computingthe Partialderivatives: gradientof ∂A R4×2 A∈R4×2with ∂x 3 ∈ dA R4×2×3 respecttoavector ∂A R4×2 dx ∈ x∈R3.Weknow ∂x 2 ∈ collate thatgradient ∂A R4×2 dA ∈R4×2×3.We ∂x 1 ∈ f d o x llowtwo equivalent approachestoarrive 4 there:(a)collating 3 partialderivatives intoaJacobian 2 tensor; (a) Approach 1: We compute the partial derivative (b)flatteningofthe ∂A, ∂A, ∂A,eachofwhichisa4×2matrix,andcol- matrixintoavector, l ∂ a x te 1 th ∂ e x m 2 in ∂x a 3 4×2×3tensor. computingthe Jacobianmatrix, re-shapingintoa Jacobiantensor. A R4×2 x R3 ∈ ∈ x 1 x 2 x 3 dA˜ dA R8×3 R4×2×3 A R4×2 A˜ R8 dx ∈ dx ∈ ∈ ∈ re-shape gradient re-shape (b)Approach2:Were-shape(flatten)A∈R4×2intoavectorA˜ ∈R8.Then,wecomputethegradient dA˜ ∈R8×3. dx Weobtainthegradienttensorbyre-shapingthisgradientas illustratedabove. 5.4 Gradients of Matrices Wecanthinkofa Wewillencountersituationswhereweneedtotakegradientsofmatrices tensorasa multidimensional withrespecttovectors(orothermatrices),whichresultsinamultidimenarray. sionaltensor.Wecanthinkofthistensorasamultidimensionalarraythat (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 156 VectorCalculus collectspartialderivatives.Forexample,ifwecomputethegradientofan m n matrix A with respect to a p q matrix B, the resulting Jacobian × × wouldbe(m n) (p q),i.e.,afour-dimensionaltensorJ,whoseentries × × × aregivenasJ = ∂A /∂B . ijkl ij kl Since matrices represent linear mappings, we can exploit the fact that there is a vector-space isomorphism (linear, invertible mapping) between the space Rm×n of m n matrices and the space Rmn of mn vectors. × Therefore, we can re-shape our matrices into vectors of lengths mn and pq,respectively.ThegradientusingthesemnvectorsresultsinaJacobian Matricescanbe of size mn pq. Figure 5.7 visualizes both approaches. In practical ap- × transformedinto plications, it is often desirable to re-shape the matrix into a vector and vectorsbystacking continue working with this Jacobian matrix: The chain rule (5.48) boils thecolumnsofthe down to simple matrix multiplication, whereas in the case of a Jacobian matrix (“flattening”). tensor, we will need to pay more attention to what dimensions we need tosumout. Example 5.12 (Gradient of Vectors with Respect to Matrices) Letusconsiderthefollowingexample,where f = Ax, f RM, A RM×N, x RN (5.85) ∈ ∈ ∈ andwhereweseekthegradientdf/dA.Letusstartagainbydetermining thedimensionofthegradientas df RM×(M×N). (5.86) dA ∈ Bydefinition,thegradientisthecollectionofthepartialderivatives: ∂f1  d d A f =   ∂