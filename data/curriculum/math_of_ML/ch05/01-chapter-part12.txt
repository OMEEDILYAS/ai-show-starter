θ = A ,b j j j { } ofeachlayerj = 0,...,K 1.Thechainruleallowsustodeterminethe − Amorein-depth partialderivativesas discussionabout ∂L ∂L ∂f gradientsofneural = K (5.115) networkscanbe ∂θ ∂f ∂θ K−1 K K−1 foundinJustin Domke’slecture ∂L ∂L ∂f ∂f = K K−1 (5.116) notes ∂θ ∂f ∂f ∂θ https://tinyurl. K−2 K K−1 K−2 com/yalcxgtv. ∂L ∂L ∂f ∂f ∂f = K K−1 K−2 (5.117) ∂θ ∂f ∂f ∂f ∂θ K−3 K K−1 K−2 K−3 ∂L ∂L ∂f ∂f ∂f = K i+2 i+1 (5.118) ∂θ ∂f ∂f ··· ∂f ∂θ i K K−1 i+1 i The orange terms are partial derivatives of the output of a layer with respect to its inputs, whereas the blue terms are partial derivatives of the output of a layer with respect to its parameters. Assuming, we have alreadycomputedthepartialderivatives∂L/∂θ ,thenmostofthecomi+1 putationcanbereusedtocompute∂L/∂θ .Theadditionaltermsthatwe i Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 5.6 BackpropagationandAutomaticDifferentiation 161 Figure5.9 Backwardpassina x f 1 f K−1 f K L multi-layerneural networktocompute thegradientsofthe lossfunction. A0,b0 A1,b1 AK−2,bK−2 AK−1,bK−1 x a b y Figure5.10 Simple graphillustrating theflowofdata fromxtoyvia needtocomputeareindicatedbytheboxes.Figure5.9visualizesthatthe someintermediate gradientsarepassedbackwardthroughthenetwork. variablesa,b. 5.6.2 Automatic Differentiation It turns out that backpropagation is a special case of a general technique innumericalanalysiscalledautomaticdifferentiation.Wecanthinkofau- automatic tomaticdifferentationasasetoftechniquestonumerically(incontrastto differentiation symbolically) evaluate the exact (up to machine precision) gradient of a function by working with intermediate variables and applying the chain rule. Automatic differentiation applies a series of elementary arithmetic Automatic operations, e.g., addition and multiplication and elementary functions, differentiationis e.g., sin,cos,exp,log. By applying the chain rule to these operations, the differentfrom symbolic gradient of quite complicated functions can be computed automatically. differentiationand Automatic differentiation applies to general computer programs and has numerical forwardandreversemodes.Baydinetal.(2018)giveagreatoverviewof approximationsof automaticdifferentiationinmachinelearning. thegradient,e.g.,by usingfinite Figure 5.10 shows a simple graph representing the data flow from indifferences. puts x to outputs y via some intermediate variables a,b. If we were to computethederivativedy/dx,wewouldapplythechainruleandobtain dy dy db da = . (5.119) dx dbdadx Intuitively, the forward and reverse mode differ in the order of multipli- Inthegeneralcase, cation. Due to the associativity of matrix multiplication, we can choose weworkwith Jacobians,which between canbevectors, dy (cid:18) dy db(cid:19) da matrices,ortensors. = , (5.120) dx dbda dx dy dy (cid:18) db da(cid:19) = . (5.121) dx db dadx Equation (5.120) would be the reverse mode because gradients are prop- reversemode agated backward through the graph, i.e., reverse to the data flow. Equation (5.121) would be the forward mode, where the gradients flow with forwardmode thedatafromlefttorightthroughthegraph. (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 162 VectorCalculus In the following, we will focus on reverse mode automatic differentiation, which is backpropagation. In the context of neural networks, where the input dimensionality is often much higher than the dimensionality of thelabels,thereversemodeiscomputationallysignificantlycheaperthan theforwardmode.Letusstartwithaninstructiveexample. Example 5.14 Considerthefunction (cid:113) f(x) = x2+exp(x2)+cos (cid:0) x2+exp(x2) (cid:1) (5.122) from (5.109). If we were to implement a function f on a computer, we intermediate wouldbeabletosavesomecomputationbyusing intermediatevariables: variables a = x2, (5.123) b = exp(a), (5.124) c = a+b, (5.125) d = √c, (5.126) e = cos(c), (5.127) f = d+e. (5.128) Figure5.11 exp() b √ d · · Computationgraph withinputsx, x ()2 a + c + f functionvaluesf,