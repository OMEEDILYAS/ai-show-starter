D E F tensorwouldbeanE F-dimensional × × × × matrix. This is a special case of a tensor contraction. Similarly, when we Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 5.6 BackpropagationandAutomaticDifferentiation 159 “transpose” a tensor, we mean swapping the first two dimensions. Specifically, in (5.99) through (5.102), we require tensor-related computations when we work with multivariate functions f( ) and compute derivatives · withrespecttomatrices(andchoosenottovectorizethemasdiscussedin Section5.4). ♦ 5.6 Backpropagation and Automatic Differentiation Agooddiscussion In many machine learning applications, we find good model parameters about by performing gradient descent (Section 7.1), which relies on the fact backpropagation andthechainruleis that we can compute the gradient of a learning objective with respect availableatablog to the parameters of the model. For a given objective function, we can byTimVieraat obtain the gradient with respect to the model parameters using calculus https://tinyurl. and applying the chain rule; see Section 5.2.2. We already had a taste in com/ycfm2yrw. Section5.3whenwelookedatthegradientofasquaredlosswithrespect totheparametersofalinearregressionmodel. Considerthefunction (cid:113) f(x) = x2+exp(x2)+cos (cid:0) x2+exp(x2) (cid:1) . (5.109) By application of the chain rule, and noting that differentiation is linear, wecomputethegradient df = 2x+2xexp(x2) sin (cid:0) x2+exp(x2) (cid:1)(cid:0) 2x+2xexp(x2) (cid:1) (cid:112) dx 2 x2+exp(x2) − (cid:32) (cid:33) = 2x 1 sin (cid:0) x2+exp(x2) (cid:1) (cid:0) 1+exp(x2) (cid:1) . (cid:112) 2 x2+exp(x2) − (5.110) Writing out the gradient in this explicit way is often impractical since it often results in a very lengthy expression for a derivative. In practice, it means that, if we are not careful, the implementation of the gradient couldbesignificantlymoreexpensivethancomputingthefunction,which imposes unnecessary overhead. For training deep neural network models, the backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, backpropagation 1962;Rumelhartetal.,1986)isanefficientwaytocomputethegradient ofanerrorfunctionwithrespecttotheparametersofthemodel. 5.6.1 Gradients in a Deep Network Anareawherethechainruleisusedtoanextremeisdeeplearning,where thefunctionvaluey iscomputedasamany-levelfunctioncomposition y = (f f f )(x) = f (f ( (f (x)) )), (5.111) K K−1 1 K K−1 1 ◦ ◦···◦ ··· ··· where x are the inputs (e.g., images), y are the observations (e.g., class labels),andeveryfunctionf ,i = 1,...,K,possessesitsownparameters. i (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 160 VectorCalculus Figure5.8 Forward passinamulti-layer neuralnetworkto x f 1 f K−1 f K L computethelossL asafunctionofthe inputsxandthe parametersAi, bi. A0,b0 A1,b1 AK−2,bK−2 AK−1,bK−1 Wediscussthecase, In neural networks with multiple layers, we have functions f i (x i−1 ) = wheretheactivation σ(A x +b )intheithlayer.Herex istheoutputoflayeri 1 i−1 i−1 i−1 i−1 functionsare andσ anactivationfunction,suchasthelogisticsigmoid 1 ,tanho − ra identicalineach 1+e−x rectifiedlinearunit(ReLU).Inordertotrainthesemodels,werequirethe layertounclutter notation. gradient of a loss function L with respect to all model parameters A j ,b j for j = 1,...,K. This also requires us to compute the gradient of L with respect to the inputs of each layer. For example, if we have inputs x and observationsy andanetworkstructuredefinedby f := x (5.112) 0 f := σ (A f +b ), i = 1,...,K, (5.113) i i i−1 i−1 i−1 see also Figure 5.8 for a visualization, we may be interested in finding A ,b forj = 0,...,K 1,suchthatthesquaredloss j j − L(θ) = y f (θ,x) 2 (5.114) (cid:107) − K (cid:107) isminimized,whereθ = A ,b ,...,A ,b . 0 0 K−1 K−1 { } Toobtainthegradientswithrespecttotheparametersetθ,werequire the partial derivatives of L with respect to the parameters