derivatives, we obtain the Hessian (cid:34) ∂2f ∂2f (cid:35) (cid:20) (cid:21) 2 2 H = ∂x2 ∂x∂y = , (5.171) ∂2f ∂2f 2 6y ∂y∂x ∂y2 suchthat (cid:20) (cid:21) 2 2 H(1,2) = R2×2. (5.172) 2 12 ∈ Therefore,thenexttermoftheTaylor-seriesexpansionisgivenby D2 f(1,2) 1 x,y δ2 = δ(cid:62)H(1,2)δ (5.173a) 2! 2 (cid:20) (cid:21)(cid:20) (cid:21) 1 (cid:2) (cid:3) 2 2 x 1 = x 1 y 2 − (5.173b) 2 − − 2 12 y 2 − = (x 1)2+2(x 1)(y 2)+6(y 2)2. (5.173c) − − − − Here,D2 f(1,2)δ2containsonlyquadraticterms,i.e.,second-orderpolyx,y nomials. Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 5.8 LinearizationandMultivariateTaylorSeries 169 Thethird-orderderivativesareobtainedas (cid:104) (cid:105) D3 f = ∂H ∂H R2×2×2, (5.174) x,y ∂x ∂y ∈ ∂H (cid:34) ∂3f ∂3f (cid:35) D3 f[:,:,1] = = ∂x3 ∂x2∂y , (5.175) x,y ∂x ∂3f ∂3f ∂x∂y∂x ∂x∂y2 ∂H (cid:34) ∂3f ∂3f (cid:35) D3 f[:,:,2] = = ∂y∂x2 ∂y∂x∂y . (5.176) x,y ∂y ∂3f ∂3f ∂y2∂x ∂y3 Since most second-order partial derivatives in the Hessian in (5.171) are constant,theonlynonzerothird-orderpartialderivativeis ∂3f ∂3f = 6 = (1,2) = 6. (5.177) ∂y3 ⇒ ∂y3 Higher-order derivatives and the mixed derivatives of degree 3 (e.g., ∂f3 )vanish,suchthat ∂x2∂y (cid:20) (cid:21) (cid:20) (cid:21) 0 0 0 0 D3 f[:,:,1] = , D3 f[:,:,2] = (5.178) x,y 0 0 x,y 0 6 and D3 f(1,2) x,y δ3 = (y 2)3, (5.179) 3! − which collects all cubic terms of the Taylor series. Overall, the (exact) Taylorseriesexpansionoff at(x ,y ) = (1,2)is 0 0 D2 f(1,2) D3 f(1,2) f(x) =f(1,2)+D1 f(1,2)δ+ x,y δ2+ x,y δ3 x,y 2! 3! (5.180a) ∂f(1,2) ∂f(1,2) =f(1,2)+ (x 1)+ (y 2) ∂x − ∂y − 1 (cid:18)∂2f(1,2) ∂2f(1,2) + (x 1)2+ (y 2)2 2! ∂x2 − ∂y2 − ∂2f(1,2) (cid:19) 1∂3f(1,2) +2 (x 1)(y 2) + (y 2)3 (5.180b) ∂x∂y − − 6 ∂y3 − =13+6(x 1)+14(y 2) − − +(x 1)2+6(y 2)2+2(x 1)(y 2)+(y 2)3. (5.180c) − − − − − In this case, we obtained an exact Taylor series expansion of the polynomialin(5.161),i.e.,thepolynomialin(5.180c)isidenticaltotheoriginal polynomial in (5.161). In this particular example, this result is not surprising since the original function was a third-order polynomial, which we expressed through a linear combination of constant terms, first-order, second-order,andthird-orderpolynomialsin(5.180c). (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 170 VectorCalculus 5.9 Further Reading Further details of matrix differentials, along with a short review of the required linear algebra, can be found in Magnus and Neudecker (2007). Automaticdifferentiationhashadalonghistory,andwerefertoGriewank and Walther (2003), Griewank and Walther (2008), and Elliott (2009) andthereferencestherein. In machine learning (and other disciplines), we often need to compute expectations,i.e.,weneedtosolveintegralsoftheform (cid:90) E [f(x)] = f(x)p(x)dx. (5.181) x Even if p(x) is in a convenient form (e.g., Gaussian), this integral generally cannot be solved analytically. The Taylor series expansion of f is (cid:0) (cid:1) one way of finding an approximate solution: Assuming p(x) = µ, Σ N is Gaussian, then the first-order Taylor series expansion around µ locally linearizes the nonlinear function f. For linear functions, we can compute themean(andthecovariance)exactlyifp(x)isGaussiandistributed(see extendedKalman Section 6.5). This property is heavily exploited by the extended Kalman filter filter (Maybeck, 1979) for online state estimation in nonlinear dynamical systems (also called