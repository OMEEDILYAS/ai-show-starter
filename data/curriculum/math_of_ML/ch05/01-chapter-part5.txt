∂x ∂x ∂ ∂f ∂g (cid:0) (cid:1) Sumrule: f(x)+g(x) = + (5.47) ∂x ∂x ∂x (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 148 VectorCalculus ∂ ∂ ∂g ∂f (cid:0) (cid:1) Chainrule: (g f)(x) = g(f(x)) = (5.48) ∂x ◦ ∂x ∂f ∂x Thisisonlyan Let us have a closer look at the chain rule. The chain rule (5.48) resemintuition,butnot blestosomedegreetherulesformatrixmultiplicationwherewesaidthat mathematically neighboringdimensionshavetomatchformatrixmultiplicationtobedecorrectsincethe fined;seeSection2.2.1.Ifwegofromlefttoright,thechainruleexhibits partialderivativeis notafraction. similar properties: ∂f shows up in the “denominator” of the first factor andinthe“numerator”ofthesecondfactor.Ifwemultiplythefactorstogether,multiplicationisdefined,i.e.,thedimensionsof∂f match,and∂f “cancels”,suchthat∂g/∂xremains. 5.2.2 Chain Rule Consider a function f : R2 R of two variables x ,x . Furthermore, 1 2 → x (t) and x (t) are themselves functions of t. To compute the gradient of 1 2 f withrespecttot,weneedtoapplythechainrule(5.48)formultivariate functionsas (cid:34) (cid:35) df (cid:104) (cid:105) ∂x1(t) ∂f ∂x ∂f ∂x = ∂f ∂f ∂t = 1 + 2 , (5.49) dt ∂x1 ∂x2 ∂x2(t) ∂x ∂t ∂x ∂t ∂t 1 2 whereddenotesthegradientand∂ partialderivatives. Example 5.8 Considerf(x ,x ) = x2+2x ,wherex = sintandx = cost,then 1 2 1 2 1 2 df ∂f ∂x ∂f ∂x 1 2 = + (5.50a) dt ∂x ∂t ∂x ∂t 1 2 ∂sint ∂cost = 2sint +2 (5.50b) ∂t ∂t = 2sintcost 2sint = 2sint(cost 1) (5.50c) − − isthecorrespondingderivativeoff withrespecttot. If f(x ,x ) is a function of x and x , where x (s,t) and x (s,t) are 1 2 1 2 1 2 themselves functions of two variables s and t, the chain rule yields the partialderivatives ∂f ∂f ∂x ∂f ∂x = 1 + 2 , (5.51) ∂s ∂x ∂s ∂x ∂s 1 2 ∂f ∂f ∂x ∂f ∂x = 1 + 2 , (5.52) ∂t ∂x ∂t ∂x ∂t 1 2 Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 5.3 GradientsofVector-ValuedFunctions 149 andthegradientisobtainedbythematrixmultiplication ∂x ∂x  1 1 df = ∂f ∂x = (cid:104) ∂f ∂f (cid:105)  ∂s ∂t  . (5.53) d(s,t) ∂x∂(s,t) ∂x 1 ∂x 2 ∂x 2 ∂x 2  (cid:124) (cid:123)(cid:122) (cid:125) ∂s ∂t ∂f (cid:124) (cid:123)(cid:122) (cid:125) = ∂x ∂x = ∂(s,t) Thiscompactwayofwritingthechainruleasamatrixmultiplicationonly Thechainrulecan makes sense if the gradient is defined as a row vector. Otherwise, we will bewrittenasa matrix need to start transposing gradients for the matrix dimensions to match. multiplication. This may still be straightforward as long as the gradient is a vector or a matrix;however,whenthegradientbecomesatensor(wewilldiscussthis inthefollowing),thetransposeisnolongeratriviality. Remark (Verifying the Correctness of a Gradient Implementation). The definition of the partial derivatives as the limit of the corresponding differencequotient(see(5.39))canbeexploitedwhennumericallychecking the correctness of gradients in computer programs: When we compute Gradientchecking gradients and implement them, we can use finite differences to numerically test our computation and implementation: We choose the value h tobesmall(e.g.,h = 10−4)andcomparethefinite-differenceapproximationfrom(5.39)withour(analytic)implementationofthegradient.Ifthe error is small, our gradient implementation is probably correct. “Small” could mean that (cid:113)(cid:80) i (dhi−dfi)2 < 10−6, where dh is the finite-difference (cid:80) i (dhi+dfi)2 i approximationanddf istheanalyticgradientoff withrespecttotheith i variablex . i ♦ 5.3 Gradients of Vector-Valued Functions Thus far, we discussed partial derivatives and gradients of functions f : Rn Rmappingtotherealnumbers.Inthefollowing,wewillgeneralize → the concept of the gradient to vector-valued functions (vector fields) f : Rn Rm,wheren (cid:62) 1andm >