· andintermediate variablesa,b,c,d,e. cos() e · This is the same kind of thinking process that occurs when applying the chain rule. Note that the preceding set of equations requires fewer operations than a direct implementation of the function f(x) as defined in (5.109). The corresponding computation graph in Figure 5.11 shows the flow of data and computations required to obtain the function value f. Thesetofequationsthatincludeintermediatevariablescanbethought of as a computation graph, a representation that is widely used in implementationsofneuralnetworksoftwarelibraries.Wecandirectlycompute the derivatives of the intermediate variables with respect to their correspondinginputsbyrecallingthedefinitionofthederivativeofelementary functions.Weobtainthefollowing: ∂a = 2x (5.129) ∂x ∂b = exp(a) (5.130) ∂a Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 5.6 BackpropagationandAutomaticDifferentiation 163 ∂c ∂c = 1 = (5.131) ∂a ∂b ∂d 1 = (5.132) ∂c 2√c ∂e = sin(c) (5.133) ∂c − ∂f ∂f = 1 = . (5.134) ∂d ∂e By looking at the computation graph in Figure 5.11, we can compute ∂f/∂xbyworkingbackwardfromtheoutputandobtain ∂f ∂f ∂d ∂f ∂e = + (5.135) ∂c ∂d ∂c ∂e ∂c ∂f ∂f ∂c = (5.136) ∂b ∂c ∂b ∂f ∂f ∂b ∂f ∂c = + (5.137) ∂a ∂b ∂a ∂c ∂a ∂f ∂f ∂a = . (5.138) ∂x ∂a∂x Notethatweimplicitlyappliedthechainruletoobtain∂f/∂x.Bysubstitutingtheresultsofthederivativesoftheelementaryfunctions,weget ∂f 1 = 1 +1 ( sin(c)) (5.139) ∂c · 2√c · − ∂f ∂f = 1 (5.140) ∂b ∂c · ∂f ∂f ∂f = exp(a)+ 1 (5.141) ∂a ∂b ∂c · ∂f ∂f = 2x. (5.142) ∂x ∂a · By thinking of each of the derivatives above as a variable, we observe that the computation required for calculating the derivative is of similar complexityasthecomputationofthefunctionitself.Thisisquitecounterintuitive since the mathematical expression for the derivative ∂f (5.110) ∂x issignificantlymorecomplicatedthanthemathematicalexpressionofthe functionf(x)in(5.109). AutomaticdifferentiationisaformalizationofExample5.14.Letx ,...,x 1 d betheinputvariablestothefunction,x ,...,x betheintermediate d+1 D−1 variables,andx theoutputvariable.Thenthecomputationgraphcanbe D expressedasfollows: Fori = d+1,...,D : x = g (x ), (5.143) i i Pa(xi) (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 164 VectorCalculus wheretheg ( )areelementaryfunctionsandx aretheparentnodes i · Pa(xi) of the variable x in the graph. Given a function defined in this way, we i canusethechainruletocomputethederivativeofthefunctioninastepby-stepfashion.Recallthatbydefinitionf = x andhence D ∂f = 1. (5.144) ∂x D Forothervariablesx ,weapplythechainrule i ∂f = (cid:88) ∂f ∂x j = (cid:88) ∂f ∂g j , (5.145) ∂x ∂x ∂x ∂x ∂x i j i j i xj:xi∈Pa(xj) xj:xi∈Pa(xj) where Pa(x ) is the set of parent nodes of x in the computation graph. j j Auto-differentiation Equation(5.143)istheforwardpropagationofafunction,whereas(5.145) inreversemode is the backpropagation of the gradient through the computation graph. requiresaparse Forneuralnetworktraining,webackpropagatetheerroroftheprediction tree. withrespecttothelabel. Theautomaticdifferentiationapproachaboveworkswheneverwehave a function that can be expressed as a computation graph, where the elementaryfunctionsaredifferentiable.Infact,thefunctionmaynotevenbe a mathematical function but a computer program. However, not all computerprogramscanbeautomaticallydifferentiated,e.g.,ifwecannotfind differential elementary functions. Programming structures, such as for loopsandifstatements,requiremorecareaswell. 5.7 Higher-Order Derivatives So far, we have discussed gradients, i.e., first-order derivatives. Sometimes,weareinterestedinderivativesofhigherorder,e.g.,whenwewant to use Newton’s Method for optimization, which requires second-order derivatives (Nocedal and Wright, 2006). In Section 5.1.1, we discussed the Taylor series to approximate functions using polynomials. In the multivariate case, we can do exactly the same. In the following, we will do exactlythis.Butletusstartwithsomenotation. Consider a function f : R2 R of two variables x,y. We use the → followingnotationforhigher-orderpartialderivatives(andforgradients): ∂2f isthesecondpartialderivativeoff