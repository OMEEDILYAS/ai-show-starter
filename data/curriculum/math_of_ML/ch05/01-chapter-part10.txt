. . . A   , ∂ ∂ A f i ∈ R1×(M×N). (5.87) ∂fM ∂A Tocomputethepartialderivatives,itwillbehelpfultoexplicitlywriteout thematrixvectormultiplication: N (cid:88) f = A x , i = 1,...,M , (5.88) i ij j j=1 andthepartialderivativesarethengivenas ∂f i = x . (5.89) ∂A q iq This allows us to compute the partial derivatives of f with respect to a i rowofA,whichisgivenas ∂f i = x(cid:62) R1×1×N , (5.90) ∂A ∈ i,: Draft(2019-12-11)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com. 5.4 GradientsofMatrices 157 ∂f i = 0(cid:62) R1×1×N (5.91) ∂A ∈ k(cid:54)=i,: where we have to pay attention to the correct dimensionality. Since f i mapsontoRandeachrowofAisofsize1 N,weobtaina1 1 N- × × × sizedtensorasthepartialderivativeoff withrespecttoarowofA. i We stack the partial derivatives (5.91) and get the desired gradient in(5.87)via 0(cid:62) .  .   .   0(cid:62)  ∂f i =  x(cid:62)   R1×(M×N). (5.92) ∂A   ∈ 0(cid:62)    . .   .  0(cid:62) Example 5.13 (Gradient of Matrices with Respect to Matrices) ConsideramatrixR RM×N andf : RM×N RN×N with ∈ → f(R) = R(cid:62)R =: K RN×N , (5.93) ∈ whereweseekthegradientdK/dR. To solve this hard problem, let us first write down what we already know:Thegradienthasthedimensions dK R(N×N)×(M×N), (5.94) dR ∈ whichisatensor.Moreover, dK pq R1×M×N (5.95) dR ∈ for p,q = 1,...,N, where K is the (p,q)th entry of K = f(R). Depq noting the ith column of R by r , every entry of K is given by the dot i productoftwocolumnsofR,i.e., M (cid:88) K = r(cid:62)r = R R . (5.96) pq p q mp mq m=1 Whenwenowcomputethepartialderivative ∂Kpq weobtain ∂Rij ∂K pq = (cid:88) M ∂ R R = ∂ , (5.97) ∂R ∂R mp mq pqij ij m=1 ij (cid:13)c2019M.P.Deisenroth,A.A.Faisal,C.S.Ong.TobepublishedbyCambridgeUniversityPress. 158 VectorCalculus  R ifj = p, p = q   iq (cid:54)  R ifj = q, p = q ∂ = ip (cid:54) . (5.98) pqij 2R ifj = p, p = q  iq   0 otherwise From (5.94), we know that the desired gradient has the dimension (N × N) (M N), and every single entry of this tensor is given by ∂ pqij × × in(5.98),wherep,q,j = 1,...,N andi = 1,...,M. 5.5 Useful Identities for Computing Gradients Inthefollowing,welistsomeusefulgradientsthatarefrequentlyrequired in a machine learning context (Petersen and Pedersen, 2012). Here, we use tr( ) as the trace (see Definition 4.4), det( ) as the determinant (see · · Section4.1)andf(X)−1 astheinverseoff(X),assumingitexists. ∂ (cid:18)∂f(X) (cid:19)(cid:62) f(X)(cid:62) = (5.99) ∂X ∂X ∂ (cid:18)∂f(X) (cid:19) tr(f(X)) = tr (5.100) ∂X ∂X ∂ (cid:18) ∂f(X) (cid:19) det(f(X)) = det(f(X))tr f(X)−1 (5.101) ∂X ∂X ∂ ∂f(X) f(X)−1 = f(X)−1 f(X)−1 (5.102) ∂X − ∂X ∂a(cid:62)X−1b = (X−1)(cid:62)ab(cid:62)(X−1)(cid:62) (5.103) ∂X − ∂x(cid:62)a = a(cid:62) (5.104) ∂x ∂a(cid:62)x = a(cid:62) (5.105) ∂x ∂a(cid:62)Xb = ab(cid:62) (5.106) ∂X ∂x(cid:62)Bx = x(cid:62)(B+B(cid:62)) (5.107) ∂x ∂ (x As)(cid:62)W(x As) = 2(x As)(cid:62)WA forsymmetricW ∂s − − − − (5.108) Remark. In this book, we only cover traces and transposes of matrices. However, we have seen that derivatives can be higher-dimensional tensors,inwhichcasetheusualtraceandtransposearenotdefined.Inthese cases,thetraceofaD